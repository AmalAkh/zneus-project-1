\documentclass{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{float}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{ZNEUS Project 1}
\author{Amal Akhmadinurov, Lukáš Šebök}

\begin{document}
\maketitle

\section{Tools}

\begin{itemize}
	\item Torch
	\item TorchMetrics
	\item Pandas
	\item Other utility libraries
	
\end{itemize}

\section{Dataset}

\textbf{Steel Plates Faults}\\
https://api.openml.org/d/1504\\

The target variable represents the fault type, with seven classes: \textit{Pastry}, \textit{Z\_Scratch}, \textit{K\_Scratch}, \textit{Stains}, \textit{Dirtiness}, \textit{Bumps}, and \textit{Other\_Faults}. This allows both \textbf{multiclass classification} (predicting the specific fault type) and \textbf{binary classification} (e.g., distinguishing one fault type from all others or fault vs. non-fault).

\subsection{Train Test Val Splits}

For binary classification split was defined: 0.9, 0.05, 0.05\\
For multiclass classification split was defined: 0.7, 0.15, 0.15\\
Different splits were define to make test and validation sets more representative considering disbalance of the dataset
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/binary-balance.png}%
    }
    \caption{Class balance for binary classification}
    \label{fig:figure2}
\end{figure}

\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.95\textwidth]{imgs/multiclass-balance.png}%
    }
    \caption{Class balance for binary classification}
    \label{fig:figure2}
\end{figure}

\subsection{Normalization and data tranformation}

\subsubsection{Scaling}
To normalize the data we first apply Z-Score standardization - we get the difference of each value from the mean of its column and divided by the standard deviation.
\[
z = \dfrac{x - \bar{x}}{\sigma}
\]

\begin{itemize}
    \item \textbf{z} - the normalized value
    \item \textbf{x} - value of the row
    \item \textbf{$\bar{x}$} - mean of the column
    \item \textbf{$\sigma$} - standard deviation
\end{itemize}

\subsubsection{Outlier Detection and Clearing}
Next we detect and clear some outliers in the dataset using Z-Score and a modified 5\% 95\% method. With the Z-Score method we're erasing rows containing score above or equal to 4. This is followed by a targetted clearing of outliers that have been discovered during Exploratory Data Analysis. Specifically we target columns Pixels\_Areas, X\_Perimeter, Y\_Perimeter, Sum\_of\_Luminosity, Outside\_X\_Index, LogOfAreas and erasing rows above 98th percentile.

\subsubsection{Feature Selection}
At this point the data is split into Binary and Multiclass dataset. On the binary data we apply the Random Forest feature selection targetting the column Class to discover and rank the best non-linear predictors and discard the rest.

\section{Binary Classification}
Report available here: \href{https://wandb.ai/amal-akhmadinurov-stu/zneus-project-1-binary/reports/Binary-Classification-Steel-Plates-Faults--VmlldzoxNDk4ODI3OQ}{Report}\\
\textbf{Warning: Some models in the report have the same name, it means that architecture is the same and only some hyperparameters are changed}
\subsection{Model Description}

This section describes the best model created.

\subsection{Model Architecture}

The neural network, implemented as the \texttt{MyModel} class, consists of a sequential stack of fully connected layers with the following structure:

\begin{itemize}
    \item \textbf{Input Layer}: Accepts input features of size \texttt{input\_size}.
    \item \textbf{Hidden Layer 1}: Linear layer mapping from \texttt{input\_size} to 512 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 2}: Linear layer mapping from 512 to 256 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 3}: Linear layer mapping from 256 to 128 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Output Layer}: Linear layer mapping from 128 to 1 unit, producing a single logit for binary classification.
\end{itemize}

The model applies a sigmoid activation to the output logit during training and evaluation to produce probabilities in $[0, 1]$.

\subsection{Hyperparameters}

The model uses the following hyperparameters, defined as constants in the implementation:

\begin{table}[h]
\centering
\caption{Hyperparameters of the Model}
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 32 \\
Learning Rate & 0.001 \\
Number of Classes & 2 \\
Maximum Epochs & 50 \\
Positive Class Weight & 2 \\
Dropout Probability & 0.35 \\
Reduce LR Patience & 10 \\
Early Stopping Patience & 10 \\

\bottomrule
\end{tabular}
\end{table}

The loss function is \texttt{BCEWithLogitsLoss} with a positive class weight of 2 to address potential class imbalance. The Adam optimizer is used with an initial learning rate of 0.001. A \texttt{ReduceLROnPlateau} scheduler reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 10 epochs. Early stopping is implemented with a patience of 10 epochs and a minimum delta of 0.01.

\subsection{Training Procedure}

The training procedure is implemented in the \texttt{fit} method, which processes the training data over a specified number of epochs (default 50). For each epoch, the model:

\begin{enumerate}
    \item Iterates over batches of the training data loader.
    \item Computes the forward pass to obtain logits, applies a sigmoid activation to produce probabilities, and calculates the binary cross-entropy loss.
    \item Updates model parameters using backpropagation and the Adam optimizer.
    \item Tracks training metrics: loss, accuracy, and precision for both positive and negative classes.
    \item Evaluates the model on the validation set using the \texttt{evaluate} method.
    \item Saves the model weights if the validation loss improves.
    \item Adjusts the learning rate using the \texttt{ReduceLROnPlateau} scheduler based on validation loss.
    \item Checks for early stopping based on validation loss stagnation.
\end{enumerate}





\subsection{Evaluation Metrics}

The model evaluates performance using the following metrics, computed for both training and validation sets:

\begin{itemize}
    \item \textbf{Loss}: Binary cross-entropy loss with logits, weighted for the positive class.
    \item \textbf{Accuracy}: Fraction of correct predictions, computed using \texttt{torchmetrics.Accuracy} for binary classification.
    \item \textbf{Positive Precision}: Precision for the positive class (label 1), computed using \texttt{torchmetrics.BinaryPrecision}.
    \item \textbf{Negative Precision}: Precision for the negative class (label 0), computed by transforming outputs and labels (\texttt{1 - output}, \texttt{1 - y}).
\end{itemize}

The \texttt{evaluate} method computes these metrics on the validation set without gradient computation, ensuring efficient evaluation.




\section{Multiclass Classification}
Report available here: \href{https://wandb.ai/amal-akhmadinurov-stu/zneus-project-1-multiclass/reports/Multiclass-Classification-Steel-Plates-Faults--VmlldzoxNDk4ODA0OA?accessToken=kqdc0lt40v64m6s2zfgd99f9gbp8e3b1q2rfm1wf8wxcc72thiwmyvqj832vhy24}{Report}\\
\textbf{Warning: Some models in the report have the same name, it means that architecture is the same and only some hyperparameters are changed}

\subsection{Model Description}

This section describes the best model created.

\subsection{Model Architecture}

The neural network, implemented as the \texttt{MyModel} class, consists of a sequential stack of fully connected layers with the following structure:

\begin{itemize}
    \item \textbf{Input Layer}: Accepts input features of size \texttt{input\_size}.
    \item \textbf{Hidden Layer 1}: Linear layer mapping from \texttt{input\_size} to 512 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 2}: Linear layer mapping from 512 to 256 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 3}: Linear layer mapping from 256 to 128 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Output Layer}: Linear layer mapping from 128 to \texttt{num\_classes} (6) units, producing logits for multiclass classification.
\end{itemize}

The model applies a argmax activation to the output logits during training and evaluation to produce probabilities for each class. 

\subsection{Hyperparameters}

The model uses the following hyperparameters, defined as constants in the implementation:

\begin{table}[h]
\centering
\caption{Hyperparameters of the Model}
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 32 \\
Learning Rate & 0.001 \\
Number of Classes & 6 \\
Train/Validation/Test Split & [0.7, 0.15, 0.15] \\
Maximum Epochs & 100 \\
Loss Weights & [4.0,4.0,2.0,3.0,5.0,5.0]\\
Dropout Probability & 0.35 \\
Reduce LR Patience & 10 \\
Early Stopping Patience & 20 \\

\bottomrule
\end{tabular}
\end{table}

The loss function is \texttt{CrossEntropyLoss} with class-specific weights [5.0, 5.0, 2.0, 3.0, 6.0, 6.0] to address class imbalance. The Adam optimizer is used with an initial learning rate of 0.001. A \texttt{ReduceLROnPlateau} scheduler reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 10 epochs. Early stopping is implemented with a patience of 20 epochs and a minimum delta of 0.01.

\subsection{Training Procedure}

The training procedure is implemented in the \texttt{fit} method, which processes the training data over a specified max number of epochs (default 100). For each epoch, the model:

\begin{enumerate}
    \item Iterates over batches of the training data loader.
    \item Computes the forward pass to obtain logits, applies a sigmoid activation to produce probabilities, and calculates the weighted cross-entropy loss.
    \item Updates model parameters using backpropagation and the Adam optimizer.
    \item Tracks training metrics: loss, macro-averaged accuracy, and macro-averaged precision.
    \item Evaluates the model on the validation set using the \texttt{evaluate} method.
    \item Saves the model weights if the validation loss improves.
    \item Adjusts the learning rate using the \texttt{ReduceLROnPlateau} scheduler based on validation loss.
    \item Checks for early stopping based on validation loss stagnation.
\end{enumerate}




\subsection{Evaluation Metrics}

The model evaluates performance using the following metrics, computed for both training and validation sets:

\begin{itemize}
    \item \textbf{Loss}: Weighted cross-entropy loss, accounting for class-specific weights.
    \item \textbf{Accuracy}: Macro-averaged accuracy across all classes, computed using \texttt{torchmetrics.Accuracy} with \texttt{task="multiclass"} and \texttt{average="macro"}.
    \item \textbf{Precision}: Macro-averaged precision across all classes, computed using \texttt{torchmetrics.Precision} with \texttt{task="multiclass"} and \texttt{average="macro"}.
\end{itemize}

The \texttt{evaluate} method computes these metrics on the validation set without gradient computation, ensuring efficient evaluation.

\pagebreak

\section{Binary Classification Results}

\subsection{Validation set results:}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Class Positive & 46 \\
Class Negative & 35 \\

\hline
\end{tabular}
\caption{Class distribution in the validation set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{0.8322}
    \item Test Accuracy: \textbf{0.7778}
    \item Test Negative Precision: \textbf{0.7917}
    \item Test Positive Precision: \textbf{.7576}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/val-binary-conf.png}%
    }
    \caption{Confusion Matrix Validation set}
    \label{fig:figure2}
\end{figure}
\pagebreak

\subsection{Test set results:}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Class Positive & 52 \\
Class Negative & 28 \\

\hline
\end{tabular}
\caption{Class distribution in the test set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{0.8152}
    \item Test Accuracy: \textbf{0.8000}
    \item Test Negative Precision: \textbf{0.8103}
    \item Test Positive Precision: \textbf{0.7727}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/binary-test-conf.png}%
    }
    \caption{Confusion Matrix Test set}
    \label{fig:figure2}
\end{figure}
\pagebreak

\pagebreak
\section{Multiclass Classification results}

\subsection{Validation set results}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Pastry & 110 \\
Z\_Scratch & 28 \\
K\_Scratch & 26 \\
Stains & 11 \\
Dirtiness & 8 \\
Bumps & 59 \\
\hline
\end{tabular}
\caption{Class distribution in the validation set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{1.2534}
    \item Test Accuracy: \textbf{0.7838}
    \item Test Precision: \textbf{0.8431}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/val-test-conf.png}%
    }
    \caption{Confusion Matrix Validation set}
    \label{fig:figure2}
\end{figure}


\pagebreak

\subsection{Test set results}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Pastry & 110 \\
Z\_Scratch & 28 \\
K\_Scratch & 26 \\
Stains & 11 \\
Dirtiness & 8 \\
Bumps & 59 \\
\hline
\end{tabular}
\caption{Class distribution in the test set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{1.2503}
    \item Test Accuracy: \textbf{0.7988}
    \item Test Precision: \textbf{0.7985}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/test-conf.png}%
    }
    \caption{Confusion Matrix Test set}
    \label{fig:figure2}
\end{figure}




\end{document}