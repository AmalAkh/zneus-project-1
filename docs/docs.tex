\documentclass{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{float}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{ZNEUS Project 1}
\author{Amal Akhmadinurov, Lukáš Šebök}

\begin{document}
\maketitle

\section{Tools}

\begin{itemize}
	\item Torch
	\item TorchMetrics
	\item Pandas
	\item Other utility libraries
	
\end{itemize}

\section{Dataset}

\textbf{Steel Plates Faults}\\
https://api.openml.org/d/1504\\
Both binary and multiclass classification \\
1,941 rows

\subsection{Train Test Val Splits}

For binary classification split was defined: 0.9, 0.05, 0.05\\
For multiclass classification split was defined: 0.7, 0.15, 0.15\\
Different splits were define to make test and validation sets more representative considering disbalance of the dataset
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.75\textwidth]{imgs/binary-balance.png}%
    }
    \caption{Class balance for binary classification}
    \label{fig:figure2}
\end{figure}

\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.95\textwidth]{imgs/multiclass-balance.png}%
    }
    \caption{Class balance for binary classification}
    \label{fig:figure2}
\end{figure}

\subsection{Normalization and data tranformation}

\section{Binary Classification}
Report available here: \href{https://wandb.ai/amal-akhmadinurov-stu/zneus-project-1-binary/reports/Binary-Classification-Steel-Plates-Faults--VmlldzoxNDk4ODI3OQ}{Report}\\
\textbf{Warning: Some models in the report have the same name, it means that architecture is the same and only some hyperparameters are changed}
\subsection{Model Description}

This section describes the best model created.

\subsection{Model Architecture}

The neural network, implemented as the \texttt{MyModel} class, consists of a sequential stack of fully connected layers with the following structure:

\begin{itemize}
    \item \textbf{Input Layer}: Accepts input features of size \texttt{input\_size}.
    \item \textbf{Hidden Layer 1}: Linear layer mapping from \texttt{input\_size} to 512 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 2}: Linear layer mapping from 512 to 256 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Hidden Layer 3}: Linear layer mapping from 256 to 128 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.35$.
    \item \textbf{Output Layer}: Linear layer mapping from 128 to 1 unit, producing a single logit for binary classification.
\end{itemize}

The model applies a sigmoid activation to the output logit during training and evaluation to produce probabilities in $[0, 1]$.

\subsection{Hyperparameters}

The model uses the following hyperparameters, defined as constants in the implementation:

\begin{table}[h]
\centering
\caption{Hyperparameters of the Model}
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 32 \\
Learning Rate & 0.001 \\
Number of Classes & 2 \\
Maximum Epochs & 50 \\
Positive Class Weight & 2 \\
Dropout Probability & 0.35 \\
Reduce LR Patience & 10 \\
Early Stopping Patience & 10 \\

\bottomrule
\end{tabular}
\end{table}

The loss function is \texttt{BCEWithLogitsLoss} with a positive class weight of 2 to address potential class imbalance. The Adam optimizer is used with an initial learning rate of 0.001. A \texttt{ReduceLROnPlateau} scheduler reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 10 epochs. Early stopping is implemented with a patience of 10 epochs and a minimum delta of 0.01.

\subsection{Training Procedure}

The training procedure is implemented in the \texttt{fit} method, which processes the training data over a specified number of epochs (default 50). For each epoch, the model:

\begin{enumerate}
    \item Iterates over batches of the training data loader.
    \item Computes the forward pass to obtain logits, applies a sigmoid activation to produce probabilities, and calculates the binary cross-entropy loss.
    \item Updates model parameters using backpropagation and the Adam optimizer.
    \item Tracks training metrics: loss, accuracy, and precision for both positive and negative classes.
    \item Evaluates the model on the validation set using the \texttt{evaluate} method.
    \item Saves the model weights if the validation loss improves.
    \item Adjusts the learning rate using the \texttt{ReduceLROnPlateau} scheduler based on validation loss.
    \item Checks for early stopping based on validation loss stagnation.
\end{enumerate}





\subsection{Evaluation Metrics}

The model evaluates performance using the following metrics, computed for both training and validation sets:

\begin{itemize}
    \item \textbf{Loss}: Binary cross-entropy loss with logits, weighted for the positive class.
    \item \textbf{Accuracy}: Fraction of correct predictions, computed using \texttt{torchmetrics.Accuracy} for binary classification.
    \item \textbf{Positive Precision}: Precision for the positive class (label 1), computed using \texttt{torchmetrics.BinaryPrecision}.
    \item \textbf{Negative Precision}: Precision for the negative class (label 0), computed by transforming outputs and labels (\texttt{1 - output}, \texttt{1 - y}).
\end{itemize}

The \texttt{evaluate} method computes these metrics on the validation set without gradient computation, ensuring efficient evaluation.




\section{Multiclass Classification}
Report available here: \href{https://wandb.ai/amal-akhmadinurov-stu/zneus-project-1-multiclass/reports/Multiclass-Classification-Steel-Plates-Faults--VmlldzoxNDk4ODA0OA?accessToken=kqdc0lt40v64m6s2zfgd99f9gbp8e3b1q2rfm1wf8wxcc72thiwmyvqj832vhy24}{Report}\\
\textbf{Warning: Some models in the report have the same name, it means that architecture is the same and only some hyperparameters are changed}

\subsection{Model Description}

This section describes the best model created.

\subsection{Model Architecture}

The neural network, implemented as the \texttt{MyModel} class, consists of a sequential stack of fully connected layers with the following structure:

\begin{itemize}
    \item \textbf{Input Layer}: Accepts input features of size \texttt{input\_size}.
    \item \textbf{Hidden Layer 1}: Linear layer mapping from \texttt{input\_size} to 512 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.5$.
    \item \textbf{Hidden Layer 2}: Linear layer mapping from 512 to 256 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.5$.
    \item \textbf{Hidden Layer 3}: Linear layer mapping from 256 to 128 units, followed by batch normalization, ReLU activation, and dropout with probability $p = 0.5$.
    \item \textbf{Output Layer}: Linear layer mapping from 128 to \texttt{num\_classes} (6) units, producing logits for multiclass classification.
\end{itemize}

The model applies a argmax activation to the output logits during training and evaluation to produce probabilities for each class. 

\subsection{Hyperparameters}

The model uses the following hyperparameters, defined as constants in the implementation:

\begin{table}[h]
\centering
\caption{Hyperparameters of the Model}
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Batch Size & 32 \\
Learning Rate & 0.001 \\
Number of Classes & 6 \\
Train/Validation/Test Split & [0.7, 0.15, 0.15] \\
Maximum Epochs & 100 \\
Loss Weights & [4.0,4.0,2.0,3.0,5.0,5.0]\\
Dropout Probability & 0.35 \\
Reduce LR Patience & 10 \\
Early Stopping Patience & 20 \\

\bottomrule
\end{tabular}
\end{table}

The loss function is \texttt{CrossEntropyLoss} with class-specific weights [5.0, 5.0, 2.0, 3.0, 6.0, 6.0] to address class imbalance. The Adam optimizer is used with an initial learning rate of 0.001. A \texttt{ReduceLROnPlateau} scheduler reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 10 epochs. Early stopping is implemented with a patience of 20 epochs and a minimum delta of 0.01.

\subsection{Training Procedure}

The training procedure is implemented in the \texttt{fit} method, which processes the training data over a specified max number of epochs (default 100). For each epoch, the model:

\begin{enumerate}
    \item Iterates over batches of the training data loader.
    \item Computes the forward pass to obtain logits, applies a sigmoid activation to produce probabilities, and calculates the weighted cross-entropy loss.
    \item Updates model parameters using backpropagation and the Adam optimizer.
    \item Tracks training metrics: loss, macro-averaged accuracy, and macro-averaged precision.
    \item Evaluates the model on the validation set using the \texttt{evaluate} method.
    \item Saves the model weights if the validation loss improves.
    \item Adjusts the learning rate using the \texttt{ReduceLROnPlateau} scheduler based on validation loss.
    \item Checks for early stopping based on validation loss stagnation.
\end{enumerate}




\subsection{Evaluation Metrics}

The model evaluates performance using the following metrics, computed for both training and validation sets:

\begin{itemize}
    \item \textbf{Loss}: Weighted cross-entropy loss, accounting for class-specific weights.
    \item \textbf{Accuracy}: Macro-averaged accuracy across all classes, computed using \texttt{torchmetrics.Accuracy} with \texttt{task="multiclass"} and \texttt{average="macro"}.
    \item \textbf{Precision}: Macro-averaged precision across all classes, computed using \texttt{torchmetrics.Precision} with \texttt{task="multiclass"} and \texttt{average="macro"}.
\end{itemize}

The \texttt{evaluate} method computes these metrics on the validation set without gradient computation, ensuring efficient evaluation.

\section{Results}

\subsection{Binary Classification}

\noindent\textbf{Test set results:}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Class Positive & 52 \\
Class Negative & 28 \\

\hline
\end{tabular}
\caption{Class distribution in the test set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{0.8152}
    \item Test Accuracy: \textbf{0.8000}
    \item Test Negative Precision: \textbf{0.8103}
    \item Test Positive Precision: \textbf{0.7727}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.95\textwidth]{imgs/binary-test-conf.png}%
    }
    \caption{Confusion Matrix Test set}
    \label{fig:figure2}
\end{figure}


\subsection{Multiclass Classification}



\noindent\textbf{Test set results:}
\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Class} & \textbf{Count} \\
\hline
Pastry & 110 \\
Z\_Scratch & 28 \\
K\_Scratch & 26 \\
Stains & 11 \\
Dirtiness & 8 \\
Bumps & 59 \\
\hline
\end{tabular}
\caption{Class distribution in the test set.}
\end{table}

\begin{itemize}
    \item Test Loss: \textbf{1.2503}
    \item Test Accuracy: \textbf{0.7988}
    \item Test Precision: \textbf{0.7985}
\end{itemize}
\begin{figure}[H]
    \noindent\makebox[\textwidth][c]{%
        \includegraphics[width=0.95\textwidth]{imgs/test-conf.png}%
    }
    \caption{Confusion Matrix Test set}
    \label{fig:figure2}
\end{figure}




\end{document}