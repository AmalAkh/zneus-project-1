{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 6\n",
    "\n",
    "TRAIN_VAL_TEST_SPLIT = [0.7, 0.15, 0.15]\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.CrossEntropyLoss(weight=torch.tensor([2.0, 1.0,3.0,4.0,8.0,4.0]))#pos_weight=torch.tensor([10.0])\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0\n",
    "\n",
    "REDUCE_LR_PATIENCE = 10\n",
    "EARLY_STOPPING_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddbbace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nwandb.init(project=\"zneus-project-1-binary\", name=\"Basic model\")\\n\\n# Config hyperparameters\\nconfig = wandb.config\\nconfig.batch_size = BATCH_SIZE\\nconfig.learning_rate = LR\\nconfig.epochs = EPOCHS\\nconfig.desc = \"\"\\nconfig.loss_function = \"Crossentropy \"\\nconfig.reduce_lr_patience = REDUCE_LR_PATIENCE\\nconfig.early_stopping_patience = EARLY_STOPPING_PATIENCE\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "wandb.init(project=\"zneus-project-1-binary\", name=\"Basic model\")\n",
    "\n",
    "# Config hyperparameters\n",
    "config = wandb.config\n",
    "config.batch_size = BATCH_SIZE\n",
    "config.learning_rate = LR\n",
    "config.epochs = EPOCHS\n",
    "config.desc = \"\"\n",
    "config.loss_function = \"Crossentropy \"\n",
    "config.reduce_lr_patience = REDUCE_LR_PATIENCE\n",
    "config.early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac1a177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5],\n",
       " ['Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(6)], [*\"Pastry Z_Scratch K_Scratch Stains Dirtiness Bumps\".split(\" \")] # V28 V29 V30 V31 V32 V33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"Pastry Z_Scratch K_Scratch Stains Dirtiness Bumps\".split(\" \"))] ,axis= 1).values.tolist() # V28 V29 V30 V31 V32 V33\n",
    "        self.labels =  self.df[[*\"Pastry Z_Scratch K_Scratch Stains Dirtiness Bumps\".split(\" \")]].dot([i for i in range(6)]).values.tolist() # V28 V29 V30 V31 V32 V33\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index]).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b052c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.15991109 -5.15991109 -5.15991109 ... -5.15991109 -5.15991109\n",
      " -5.15991109]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "labels = np.array(dataset.labels)\n",
    "print(labels)\n",
    "train_size, val_size, test_size = TRAIN_VAL_TEST_SPLIT\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=TRAIN_VAL_TEST_SPLIT[1],\n",
    "    stratify=dataset.labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Update labels for train+val indices\n",
    "train_val_labels = labels[train_val_indices]\n",
    "\n",
    "# Second split: train vs val\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_val_indices,\n",
    "    test_size=val_size / (train_size + val_size),  # Adjust proportion for train+val\n",
    "    stratify=train_val_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1358\n",
      "Validation dataset size: 291\n",
      "Test dataset size: 292\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    \n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "121c6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            min_delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4f70375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2,reduce_lr_patience=10, early_stopping_patience=10):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes,average=\"macro\")\n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.early_stopping = EarlyStopping(patience=early_stopping_patience, min_delta=0.01)\n",
    "\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=reduce_lr_patience\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "\n",
    "        self.precision.reset()\n",
    "        self.accuracy.reset()\n",
    "\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.precision.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        self.train()\n",
    "        best_val_loss = 9999\n",
    "\n",
    "     \n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_precision = self.precision.compute()\n",
    "\n",
    "       \n",
    "\n",
    "            val_loss, val_acc, val_precision = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-multiclass.pth\")\n",
    "\n",
    "            self.early_stopping(val_loss)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} Precision:{epoch_precision:.4f} Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val Precision:{val_precision:.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    input_size=len(dataset.features[0]),\n",
    "    num_classes=CLASSES, \n",
    "    loss_fn=LOSS_FUNCTION, lr=LR, \n",
    "    reduce_lr_patience=REDUCE_LR_PATIENCE, \n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:1.1044 Accuracy:0.2454 Precision:0.2500 Val Loss:1.0459 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 2 Loss:1.0518 Accuracy:0.4982 Precision:0.5000 Val Loss:1.0445 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 3 Loss:1.0472 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0450 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 4 Loss:1.0448 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0442 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 5 Loss:1.0445 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0440 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 6 Loss:1.0442 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0438 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 7 Loss:nan Accuracy:1.0000 Precision:1.0000 Val Loss:1.0438 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 8 Loss:1.0439 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0437 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 9 Loss:1.0438 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0437 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Epoch 10 Loss:1.0438 Accuracy:1.0000 Precision:1.0000 Val Loss:1.0437 Val Accuracy:1.0000 Val Precision:1.0000 LR = 0.001\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "994e8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.1754, -5.1599, -5.1599,  7.1754, -0.1746, -5.1599, -5.1599,  7.1754,\n",
      "         7.1754,  7.1754, -5.1599, -0.1746, -5.1599, -5.1599, -0.1746, -5.1599,\n",
      "        -5.1599, -5.1599, -5.1599, -5.1599, -5.1599, -1.7956, -0.1746, -5.1599,\n",
      "        -5.1599,  7.1754, -0.1746, 18.9403, -5.1599,  7.1754, -5.1599, -0.1746,\n",
      "        -5.1599,  7.1754, -5.1599, -5.1599, -1.7956,  7.1754, -0.1746,  7.1754,\n",
      "        -0.1746, -0.1746, -0.1746,  7.1754, -5.1599, -5.1599, -5.1599, -5.1599,\n",
      "        -1.7956, -1.7956, -0.1746, -1.7956, -1.7956, -5.1599, -0.1746,  7.1754,\n",
      "        -0.1746, -5.1599, 10.7096, -5.1599,  7.1754, 18.9403,  7.1754,  7.1754,\n",
      "         7.1754, -5.1599,  7.1754, -5.1599,  7.1754, -5.1599, 10.7096, -5.1599,\n",
      "        -5.1599, -1.7956, -1.7956, -5.1599, -5.1599,  7.1754,  7.1754, -1.7956,\n",
      "        -1.7956, -0.1746, -5.1599, -5.1599,  7.1754,  7.1754, -5.1599, -5.1599,\n",
      "        -5.1599, -5.1599,  7.1754, -5.1599, -5.1599, -5.1599, -0.1746, -0.1746,\n",
      "        -1.7956,  7.1754, 10.7096, 18.9403, -0.1746, -1.7956, -0.1746, -5.1599,\n",
      "        10.7096,  7.1754, 18.9403, -5.1599,  7.1754, -5.1599, -5.1599, -5.1599,\n",
      "        -0.1746,  7.1754,  7.1754, -5.1599, -0.1746, -0.1746,  7.1754, -5.1599,\n",
      "        -5.1599, -0.1746,  7.1754, -0.1746, -5.1599, -1.7956, -5.1599,  7.1754,\n",
      "         7.1754, -1.7956,  7.1754, -5.1599, -0.1746, -0.1746, -1.7956, -5.1599,\n",
      "         7.1754, -0.1746, -5.1599, -5.1599, -5.1599, -1.7956, -5.1599, -5.1599,\n",
      "         7.1754,  7.1754, -5.1599, -5.1599, -5.1599, -5.1599, -0.1746, -1.7956,\n",
      "        -5.1599, -0.1746,  7.1754, -5.1599, -5.1599, -1.7956, -5.1599, 18.9403,\n",
      "         7.1754,  7.1754, -5.1599, -5.1599, -0.1746, -5.1599,  7.1754, -1.7956,\n",
      "        -5.1599, -0.1746,  7.1754, -5.1599, -1.7956, -1.7956, -5.1599, -0.1746,\n",
      "         7.1754, -5.1599, -5.1599, -0.1746, -5.1599, -0.1746,  7.1754, -5.1599,\n",
      "        10.7096, -0.1746, -0.1746, -5.1599, -5.1599, -5.1599, -5.1599, -1.7956,\n",
      "        -0.1746, -0.1746,  7.1754, -5.1599, -5.1599, -5.1599, 10.7096, -5.1599,\n",
      "        -5.1599, -5.1599, 18.9403,  7.1754, -5.1599, -5.1599, -0.1746, -0.1746,\n",
      "        -0.1746, -5.1599, -0.1746, -0.1746, -5.1599, -5.1599,  7.1754, -5.1599,\n",
      "         7.1754, -0.1746,  7.1754,  7.1754, -1.7956, -0.1746, 10.7096,  7.1754,\n",
      "        -0.1746, -0.1746, -5.1599, -5.1599, -5.1599, -5.1599, -1.7956,  7.1754,\n",
      "        -5.1599, -5.1599, -5.1599, 18.9403, -5.1599, -0.1746, -0.1746, -5.1599,\n",
      "        -5.1599,  7.1754, -5.1599, -0.1746, -1.7956,  7.1754, -0.1746, -5.1599,\n",
      "        10.7096, -5.1599, -5.1599, -5.1599, -5.1599,  7.1754, -5.1599, 10.7096,\n",
      "        -0.1746, -5.1599,  7.1754,  7.1754, -0.1746, 18.9403, -5.1599, -0.1746,\n",
      "         7.1754, -0.1746, -5.1599, -5.1599, -5.1599,  7.1754, -5.1599, -5.1599,\n",
      "        -0.1746, -0.1746, -0.1746, 10.7096, -5.1599, -5.1599, -1.7956,  7.1754,\n",
      "        -5.1599, -0.1746,  7.1754, 10.7096, -5.1599, -5.1599, -1.7956, -5.1599,\n",
      "        -0.1746, -5.1599, -1.7956])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m y_true_flat = y_true.flatten()\n\u001b[32m     25\u001b[39m y_pred_flat = y_pred.flatten()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m cm = \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Convert to percentage (%)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Subjects_ZS_2025/ZNEUS/zneus-project-1/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Subjects_ZS_2025/ZNEUS/zneus-project-1/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:467\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m(0, 2, 1, 1)\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m % y_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Subjects_ZS_2025/ZNEUS/zneus-project-1/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:106\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m    103\u001b[39m     y_type = {\u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mClassification metrics can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m targets\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    108\u001b[39m             type_true, type_pred\n\u001b[32m    109\u001b[39m         )\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[32m    113\u001b[39m y_type = y_type.pop()\n",
      "\u001b[31mValueError\u001b[39m: Classification metrics can't handle a mix of continuous and multiclass targets"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(ground_truth)\n",
    "predictions =  torch.argmax(predictions, dim=1).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1, 2,3,4,5])\n",
    "print(cm)\n",
    "# Convert to percentage (%)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Pastry\", \"Z_Scratch\",\"K_Scratch\"\t,\"Stains\"\t,\"Dirtiness\"\t,\"Bumps\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(ground_truth)\n",
    "predictions = torch.argmax(predictions, dim=1).int()\n",
    "\n",
    "\n",
    "y_true = ground_truth.numpy()\n",
    "print(y_true)\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1, 2,3,4,5])\n",
    "print(cm)\n",
    "# Convert to percentage (%)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Pastry\", \"Z_Scratch\",\"K_Scratch\"\t,\"Stains\"\t,\"Dirtiness\"\t,\"Bumps\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(test_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "print(predictions.shape)\n",
    "predictions =  torch.argmax(predictions, dim=1).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "y_true_flat = y_true.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1, 2,3,4,5])\n",
    "print(cm)\n",
    "# Convert to percentage (%)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Pastry\", \"Z_Scratch\",\t\"K_Scratch\"\t,\"Stains\"\t,\"Dirtiness\"\t,\"Bumps\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3509fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=[0, 1, 2,3,4,5])\n",
    "print(cm)\n",
    "# Convert to percentage (%)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Pastry\", \"Z_Scratch\",\t\"K_Scratch\"\t,\"Stains\"\t,\"Dirtiness\"\t,\"Bumps\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7c054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
