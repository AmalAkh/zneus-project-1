{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_TEST_SPLIT = [0.9, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"Pastry Z_Scratch K_Scratch Stains Dirtiness Bumps\".split(\" \"))] ,axis= 1).values.tolist() # V28 V29 V30 V31 V32 V33\n",
    "        self.labels = self.df[\"Class\"].to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b052c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, TRAIN_VAL_TEST_SPLIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 2\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.BCEWithLogitsLoss()\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision, F1Score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"binary\", num_labels=num_classes)\n",
    "        self.f1 = F1Score(task=\"binary\", num_labels=num_classes, average='macro')\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.f1.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        self.train()\n",
    "        best_val_loss = 9999\n",
    "\n",
    "        train_loss_hist = []\n",
    "        train_accuracy_hist = []\n",
    "        train_f1_hist = []\n",
    "\n",
    "        val_loss_hist = []\n",
    "        val_accuracy_hist = []\n",
    "        val_f1_hist = []\n",
    "\n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.f1(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_f1 = self.f1.compute()\n",
    "\n",
    "       \n",
    "            train_accuracy_hist.append(epoch_acc.item())\n",
    "            train_loss_hist.append(epoch_loss)\n",
    "            train_f1_hist.append(epoch_f1.item())\n",
    "\n",
    "            val_loss, val_acc, val_f1 = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-by-loss.pth\")\n",
    "\n",
    "            \n",
    "            val_accuracy_hist.append(val_acc.item())\n",
    "            val_loss_hist.append(val_loss)\n",
    "            val_f1_hist.append(val_f1.item())\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} F1:{epoch_f1:.4f}  Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val F1:{val_f1:.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        return (train_loss_hist, train_accuracy_hist,train_f1_hist), (val_loss_hist, val_accuracy_hist,val_f1_hist)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1747\n",
      "Validation dataset size: 97\n",
      "Test dataset size: 97\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size=27,num_classes=CLASSES, loss_fn=LOSS_FUNCTION, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:0.7019 Accuracy:0.6497 F1:0.0129  Val Loss:0.6932 Val Accuracy:0.6491 Val F1:0.0129 LR = 0.001\n",
      "Epoch 2 Loss:0.6932 Accuracy:0.6520 F1:0.0065  Val Loss:0.6932 Val Accuracy:0.6513 Val F1:0.0065 LR = 0.001\n",
      "Epoch 3 Loss:0.6931 Accuracy:0.6520 F1:0.0044  Val Loss:0.6932 Val Accuracy:0.6513 Val F1:0.0044 LR = 0.001\n",
      "Epoch 4 Loss:0.6931 Accuracy:0.6520 F1:0.0033  Val Loss:0.6932 Val Accuracy:0.6513 Val F1:0.0033 LR = 0.001\n",
      "Epoch 5 Loss:0.6931 Accuracy:0.6520 F1:0.0026  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0026 LR = 0.001\n",
      "Epoch 6 Loss:0.6931 Accuracy:0.6520 F1:0.0022  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0022 LR = 0.001\n",
      "Epoch 7 Loss:0.6931 Accuracy:0.6520 F1:0.0019  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0019 LR = 0.001\n",
      "Epoch 8 Loss:0.6931 Accuracy:0.6520 F1:0.0016  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0016 LR = 0.001\n",
      "Epoch 9 Loss:0.6931 Accuracy:0.6520 F1:0.0015  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0015 LR = 0.001\n",
      "Epoch 10 Loss:0.6931 Accuracy:0.6520 F1:0.0013  Val Loss:0.6931 Val Accuracy:0.6513 Val F1:0.0013 LR = 0.001\n",
      "Epoch 11 Loss:0.6875 Accuracy:0.6697 F1:0.0145  Val Loss:0.7463 Val Accuracy:0.6725 Val F1:0.0145 LR = 0.001\n",
      "Epoch 12 Loss:0.6758 Accuracy:0.7127 F1:0.0526  Val Loss:0.6853 Val Accuracy:0.7110 Val F1:0.0526 LR = 0.001\n",
      "Epoch 13 Loss:0.6732 Accuracy:0.7138 F1:0.0817  Val Loss:0.7632 Val Accuracy:0.7131 Val F1:0.0817 LR = 0.001\n",
      "Epoch 14 Loss:0.6674 Accuracy:0.7275 F1:0.1084  Val Loss:0.6808 Val Accuracy:0.7267 Val F1:0.1084 LR = 0.001\n",
      "Epoch 15 Loss:0.6675 Accuracy:0.7315 F1:0.1345  Val Loss:0.6943 Val Accuracy:0.7267 Val F1:0.1345 LR = 0.001\n",
      "Epoch 16 Loss:0.6659 Accuracy:0.7293 F1:0.1545  Val Loss:0.8169 Val Accuracy:0.7305 Val F1:0.1545 LR = 0.001\n",
      "Epoch 17 Loss:0.6660 Accuracy:0.7338 F1:0.1741  Val Loss:0.6871 Val Accuracy:0.7321 Val F1:0.1741 LR = 0.001\n",
      "Epoch 18 Loss:0.6647 Accuracy:0.7367 F1:0.1912  Val Loss:0.6850 Val Accuracy:0.7348 Val F1:0.1912 LR = 0.001\n",
      "Epoch 19 Loss:0.6631 Accuracy:0.7384 F1:0.2063  Val Loss:0.6995 Val Accuracy:0.7370 Val F1:0.2063 LR = 0.001\n",
      "Epoch 20 Loss:0.6639 Accuracy:0.7373 F1:0.2195  Val Loss:0.6826 Val Accuracy:0.7354 Val F1:0.2195 LR = 0.001\n",
      "Epoch 21 Loss:0.6635 Accuracy:0.7396 F1:0.2322  Val Loss:0.6834 Val Accuracy:0.7370 Val F1:0.2322 LR = 0.001\n",
      "Epoch 22 Loss:0.6624 Accuracy:0.7418 F1:0.2429  Val Loss:0.6803 Val Accuracy:0.7397 Val F1:0.2429 LR = 0.001\n",
      "Epoch 23 Loss:0.6631 Accuracy:0.7401 F1:0.2532  Val Loss:0.6851 Val Accuracy:0.7386 Val F1:0.2532 LR = 0.001\n",
      "Epoch 24 Loss:0.6611 Accuracy:0.7447 F1:0.2624  Val Loss:0.6727 Val Accuracy:0.7440 Val F1:0.2624 LR = 0.001\n",
      "Epoch 25 Loss:0.6626 Accuracy:0.7424 F1:0.2713  Val Loss:0.6803 Val Accuracy:0.7402 Val F1:0.2713 LR = 0.001\n",
      "Epoch 26 Loss:0.6612 Accuracy:0.7447 F1:0.2795  Val Loss:0.6793 Val Accuracy:0.7430 Val F1:0.2795 LR = 0.001\n",
      "Epoch 27 Loss:0.6595 Accuracy:0.7499 F1:0.2876  Val Loss:0.6788 Val Accuracy:0.7478 Val F1:0.2876 LR = 0.001\n",
      "Epoch 28 Loss:0.6611 Accuracy:0.7447 F1:0.2944  Val Loss:0.6821 Val Accuracy:0.7419 Val F1:0.2944 LR = 0.001\n",
      "Epoch 29 Loss:0.6626 Accuracy:0.7407 F1:0.3000  Val Loss:0.6792 Val Accuracy:0.7392 Val F1:0.3000 LR = 0.001\n",
      "Epoch 30 Loss:0.6590 Accuracy:0.7487 F1:0.3059  Val Loss:0.6795 Val Accuracy:0.7462 Val F1:0.3059 LR = 0.001\n",
      "Epoch 31 Loss:0.6601 Accuracy:0.7476 F1:0.3112  Val Loss:0.6765 Val Accuracy:0.7462 Val F1:0.3112 LR = 0.001\n",
      "Epoch 32 Loss:0.6582 Accuracy:0.7539 F1:0.3176  Val Loss:0.6792 Val Accuracy:0.7516 Val F1:0.3176 LR = 0.001\n",
      "Epoch 33 Loss:0.6591 Accuracy:0.7504 F1:0.3229  Val Loss:0.6816 Val Accuracy:0.7473 Val F1:0.3229 LR = 0.001\n",
      "Epoch 34 Loss:0.6599 Accuracy:0.7470 F1:0.3277  Val Loss:0.6772 Val Accuracy:0.7451 Val F1:0.3277 LR = 0.001\n",
      "Epoch 35 Loss:0.6590 Accuracy:0.7527 F1:0.3324  Val Loss:0.6788 Val Accuracy:0.7505 Val F1:0.3324 LR = 0.0005\n",
      "Epoch 36 Loss:0.6576 Accuracy:0.7539 F1:0.3369  Val Loss:0.6783 Val Accuracy:0.7516 Val F1:0.3369 LR = 0.0005\n",
      "Epoch 37 Loss:0.6568 Accuracy:0.7556 F1:0.3413  Val Loss:0.6785 Val Accuracy:0.7533 Val F1:0.3413 LR = 0.0005\n",
      "Epoch 38 Loss:0.6576 Accuracy:0.7499 F1:0.3450  Val Loss:0.6776 Val Accuracy:0.7478 Val F1:0.3450 LR = 0.0005\n",
      "Epoch 39 Loss:0.6565 Accuracy:0.7562 F1:0.3491  Val Loss:0.6786 Val Accuracy:0.7538 Val F1:0.3491 LR = 0.0005\n",
      "Epoch 40 Loss:0.6562 Accuracy:0.7590 F1:0.3531  Val Loss:0.6791 Val Accuracy:0.7565 Val F1:0.3531 LR = 0.0005\n",
      "Epoch 41 Loss:0.6565 Accuracy:0.7544 F1:0.3566  Val Loss:0.6779 Val Accuracy:0.7522 Val F1:0.3566 LR = 0.0005\n",
      "Epoch 42 Loss:0.6561 Accuracy:0.7590 F1:0.3603  Val Loss:0.6780 Val Accuracy:0.7565 Val F1:0.3603 LR = 0.0005\n",
      "Epoch 43 Loss:0.6553 Accuracy:0.7602 F1:0.3638  Val Loss:0.6776 Val Accuracy:0.7576 Val F1:0.3638 LR = 0.0005\n",
      "Epoch 44 Loss:0.6554 Accuracy:0.7613 F1:0.3673  Val Loss:0.6779 Val Accuracy:0.7587 Val F1:0.3673 LR = 0.0005\n",
      "Epoch 45 Loss:0.6563 Accuracy:0.7562 F1:0.3700  Val Loss:0.6768 Val Accuracy:0.7538 Val F1:0.3700 LR = 0.0005\n",
      "Epoch 46 Loss:0.6555 Accuracy:0.7613 F1:0.3733  Val Loss:0.6788 Val Accuracy:0.7587 Val F1:0.3733 LR = 0.00025\n",
      "Epoch 47 Loss:0.6549 Accuracy:0.7590 F1:0.3761  Val Loss:0.6770 Val Accuracy:0.7565 Val F1:0.3761 LR = 0.00025\n",
      "Epoch 48 Loss:0.6548 Accuracy:0.7584 F1:0.3787  Val Loss:0.6778 Val Accuracy:0.7560 Val F1:0.3787 LR = 0.00025\n",
      "Epoch 49 Loss:0.6538 Accuracy:0.7624 F1:0.3815  Val Loss:0.6778 Val Accuracy:0.7598 Val F1:0.3815 LR = 0.00025\n",
      "Epoch 50 Loss:0.6541 Accuracy:0.7636 F1:0.3842  Val Loss:0.6776 Val Accuracy:0.7608 Val F1:0.3842 LR = 0.00025\n",
      "Epoch 51 Loss:0.6536 Accuracy:0.7619 F1:0.3867  Val Loss:0.6778 Val Accuracy:0.7592 Val F1:0.3867 LR = 0.00025\n",
      "Epoch 52 Loss:0.6542 Accuracy:0.7613 F1:0.3890  Val Loss:0.6778 Val Accuracy:0.7587 Val F1:0.3890 LR = 0.00025\n",
      "Epoch 53 Loss:0.6541 Accuracy:0.7642 F1:0.3915  Val Loss:0.6776 Val Accuracy:0.7614 Val F1:0.3915 LR = 0.00025\n",
      "Epoch 54 Loss:0.6539 Accuracy:0.7624 F1:0.3937  Val Loss:0.6770 Val Accuracy:0.7598 Val F1:0.3937 LR = 0.00025\n",
      "Epoch 55 Loss:0.6543 Accuracy:0.7619 F1:0.3959  Val Loss:0.6779 Val Accuracy:0.7592 Val F1:0.3959 LR = 0.00025\n",
      "Epoch 56 Loss:0.6538 Accuracy:0.7624 F1:0.3979  Val Loss:0.6778 Val Accuracy:0.7598 Val F1:0.3979 LR = 0.00025\n",
      "Epoch 57 Loss:0.6533 Accuracy:0.7653 F1:0.4000  Val Loss:0.6778 Val Accuracy:0.7625 Val F1:0.4000 LR = 0.000125\n",
      "Epoch 58 Loss:0.6534 Accuracy:0.7659 F1:0.4021  Val Loss:0.6778 Val Accuracy:0.7630 Val F1:0.4021 LR = 0.000125\n",
      "Epoch 59 Loss:0.6527 Accuracy:0.7659 F1:0.4040  Val Loss:0.6778 Val Accuracy:0.7630 Val F1:0.4040 LR = 0.000125\n",
      "Epoch 60 Loss:0.6527 Accuracy:0.7665 F1:0.4059  Val Loss:0.6778 Val Accuracy:0.7636 Val F1:0.4059 LR = 0.000125\n",
      "Epoch 61 Loss:0.6532 Accuracy:0.7670 F1:0.4079  Val Loss:0.6778 Val Accuracy:0.7641 Val F1:0.4079 LR = 0.000125\n",
      "Epoch 62 Loss:0.6528 Accuracy:0.7659 F1:0.4097  Val Loss:0.6777 Val Accuracy:0.7630 Val F1:0.4097 LR = 0.000125\n",
      "Epoch 63 Loss:0.6532 Accuracy:0.7659 F1:0.4114  Val Loss:0.6777 Val Accuracy:0.7630 Val F1:0.4114 LR = 0.000125\n",
      "Epoch 64 Loss:0.6525 Accuracy:0.7659 F1:0.4131  Val Loss:0.6778 Val Accuracy:0.7630 Val F1:0.4131 LR = 0.000125\n",
      "Epoch 65 Loss:0.6528 Accuracy:0.7665 F1:0.4147  Val Loss:0.6778 Val Accuracy:0.7636 Val F1:0.4147 LR = 0.000125\n",
      "Epoch 66 Loss:0.6528 Accuracy:0.7665 F1:0.4163  Val Loss:0.6777 Val Accuracy:0.7636 Val F1:0.4163 LR = 0.000125\n",
      "Epoch 67 Loss:0.6522 Accuracy:0.7676 F1:0.4179  Val Loss:0.6776 Val Accuracy:0.7646 Val F1:0.4179 LR = 0.000125\n",
      "Epoch 68 Loss:0.6524 Accuracy:0.7676 F1:0.4194  Val Loss:0.6778 Val Accuracy:0.7646 Val F1:0.4194 LR = 6.25e-05\n",
      "Epoch 69 Loss:0.6524 Accuracy:0.7676 F1:0.4209  Val Loss:0.6777 Val Accuracy:0.7646 Val F1:0.4209 LR = 6.25e-05\n",
      "Epoch 70 Loss:0.6520 Accuracy:0.7676 F1:0.4223  Val Loss:0.6777 Val Accuracy:0.7646 Val F1:0.4223 LR = 6.25e-05\n",
      "Epoch 71 Loss:0.6525 Accuracy:0.7676 F1:0.4237  Val Loss:0.6777 Val Accuracy:0.7646 Val F1:0.4237 LR = 6.25e-05\n",
      "Epoch 72 Loss:0.6523 Accuracy:0.7682 F1:0.4251  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4251 LR = 6.25e-05\n",
      "Epoch 73 Loss:0.6521 Accuracy:0.7682 F1:0.4264  Val Loss:0.6776 Val Accuracy:0.7652 Val F1:0.4264 LR = 6.25e-05\n",
      "Epoch 74 Loss:0.6524 Accuracy:0.7676 F1:0.4277  Val Loss:0.6778 Val Accuracy:0.7646 Val F1:0.4277 LR = 6.25e-05\n",
      "Epoch 75 Loss:0.6524 Accuracy:0.7676 F1:0.4289  Val Loss:0.6777 Val Accuracy:0.7646 Val F1:0.4289 LR = 6.25e-05\n",
      "Epoch 76 Loss:0.6522 Accuracy:0.7682 F1:0.4302  Val Loss:0.6776 Val Accuracy:0.7652 Val F1:0.4302 LR = 6.25e-05\n",
      "Epoch 77 Loss:0.6526 Accuracy:0.7682 F1:0.4313  Val Loss:0.6776 Val Accuracy:0.7652 Val F1:0.4313 LR = 6.25e-05\n",
      "Epoch 78 Loss:0.6527 Accuracy:0.7682 F1:0.4325  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4325 LR = 6.25e-05\n",
      "Epoch 79 Loss:0.6520 Accuracy:0.7682 F1:0.4336  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4336 LR = 3.125e-05\n",
      "Epoch 80 Loss:0.6521 Accuracy:0.7687 F1:0.4348  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4348 LR = 3.125e-05\n",
      "Epoch 81 Loss:0.6521 Accuracy:0.7687 F1:0.4359  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4359 LR = 3.125e-05\n",
      "Epoch 82 Loss:0.6522 Accuracy:0.7682 F1:0.4369  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4369 LR = 3.125e-05\n",
      "Epoch 83 Loss:0.6518 Accuracy:0.7687 F1:0.4379  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4379 LR = 3.125e-05\n",
      "Epoch 84 Loss:0.6523 Accuracy:0.7687 F1:0.4389  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4389 LR = 3.125e-05\n",
      "Epoch 85 Loss:0.6523 Accuracy:0.7682 F1:0.4399  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4399 LR = 3.125e-05\n",
      "Epoch 86 Loss:0.6519 Accuracy:0.7682 F1:0.4408  Val Loss:0.6777 Val Accuracy:0.7652 Val F1:0.4408 LR = 3.125e-05\n",
      "Epoch 87 Loss:0.6518 Accuracy:0.7687 F1:0.4418  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4418 LR = 3.125e-05\n",
      "Epoch 88 Loss:0.6517 Accuracy:0.7687 F1:0.4427  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4427 LR = 3.125e-05\n",
      "Epoch 89 Loss:0.6519 Accuracy:0.7676 F1:0.4436  Val Loss:0.6776 Val Accuracy:0.7646 Val F1:0.4436 LR = 3.125e-05\n",
      "Epoch 90 Loss:0.6520 Accuracy:0.7687 F1:0.4444  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4444 LR = 1.5625e-05\n",
      "Epoch 91 Loss:0.6521 Accuracy:0.7687 F1:0.4453  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4453 LR = 1.5625e-05\n",
      "Epoch 92 Loss:0.6516 Accuracy:0.7687 F1:0.4461  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4461 LR = 1.5625e-05\n",
      "Epoch 93 Loss:0.6521 Accuracy:0.7687 F1:0.4469  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4469 LR = 1.5625e-05\n",
      "Epoch 94 Loss:0.6514 Accuracy:0.7687 F1:0.4477  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4477 LR = 1.5625e-05\n",
      "Epoch 95 Loss:0.6522 Accuracy:0.7687 F1:0.4485  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4485 LR = 1.5625e-05\n",
      "Epoch 96 Loss:0.6522 Accuracy:0.7687 F1:0.4493  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4493 LR = 1.5625e-05\n",
      "Epoch 97 Loss:0.6522 Accuracy:0.7687 F1:0.4500  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4500 LR = 1.5625e-05\n",
      "Epoch 98 Loss:0.6520 Accuracy:0.7687 F1:0.4508  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4508 LR = 1.5625e-05\n",
      "Epoch 99 Loss:0.6522 Accuracy:0.7687 F1:0.4515  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4515 LR = 1.5625e-05\n",
      "Epoch 100 Loss:0.6521 Accuracy:0.7687 F1:0.4522  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4522 LR = 1.5625e-05\n",
      "Epoch 101 Loss:0.6520 Accuracy:0.7687 F1:0.4529  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4529 LR = 7.8125e-06\n",
      "Epoch 102 Loss:0.6517 Accuracy:0.7687 F1:0.4536  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4536 LR = 7.8125e-06\n",
      "Epoch 103 Loss:0.6520 Accuracy:0.7687 F1:0.4542  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4542 LR = 7.8125e-06\n",
      "Epoch 104 Loss:0.6518 Accuracy:0.7687 F1:0.4549  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4549 LR = 7.8125e-06\n",
      "Epoch 105 Loss:0.6518 Accuracy:0.7687 F1:0.4555  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4555 LR = 7.8125e-06\n",
      "Epoch 106 Loss:0.6515 Accuracy:0.7687 F1:0.4561  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4561 LR = 7.8125e-06\n",
      "Epoch 107 Loss:0.6519 Accuracy:0.7687 F1:0.4567  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4567 LR = 7.8125e-06\n",
      "Epoch 108 Loss:0.6517 Accuracy:0.7687 F1:0.4573  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4573 LR = 7.8125e-06\n",
      "Epoch 109 Loss:0.6520 Accuracy:0.7687 F1:0.4579  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4579 LR = 7.8125e-06\n",
      "Epoch 110 Loss:0.6521 Accuracy:0.7687 F1:0.4585  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4585 LR = 7.8125e-06\n",
      "Epoch 111 Loss:0.6518 Accuracy:0.7687 F1:0.4591  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4591 LR = 7.8125e-06\n",
      "Epoch 112 Loss:0.6520 Accuracy:0.7687 F1:0.4596  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4596 LR = 3.90625e-06\n",
      "Epoch 113 Loss:0.6518 Accuracy:0.7687 F1:0.4602  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4602 LR = 3.90625e-06\n",
      "Epoch 114 Loss:0.6515 Accuracy:0.7687 F1:0.4607  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4607 LR = 3.90625e-06\n",
      "Epoch 115 Loss:0.6518 Accuracy:0.7687 F1:0.4612  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4612 LR = 3.90625e-06\n",
      "Epoch 116 Loss:0.6523 Accuracy:0.7687 F1:0.4617  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4617 LR = 3.90625e-06\n",
      "Epoch 117 Loss:0.6516 Accuracy:0.7687 F1:0.4622  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4622 LR = 3.90625e-06\n",
      "Epoch 118 Loss:0.6515 Accuracy:0.7687 F1:0.4627  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4627 LR = 3.90625e-06\n",
      "Epoch 119 Loss:0.6523 Accuracy:0.7687 F1:0.4632  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4632 LR = 3.90625e-06\n",
      "Epoch 120 Loss:0.6517 Accuracy:0.7687 F1:0.4637  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4637 LR = 3.90625e-06\n",
      "Epoch 121 Loss:0.6518 Accuracy:0.7687 F1:0.4642  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4642 LR = 3.90625e-06\n",
      "Epoch 122 Loss:0.6518 Accuracy:0.7687 F1:0.4646  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4646 LR = 3.90625e-06\n",
      "Epoch 123 Loss:0.6515 Accuracy:0.7687 F1:0.4651  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4651 LR = 1.953125e-06\n",
      "Epoch 124 Loss:0.6520 Accuracy:0.7687 F1:0.4655  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4655 LR = 1.953125e-06\n",
      "Epoch 125 Loss:0.6517 Accuracy:0.7687 F1:0.4660  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4660 LR = 1.953125e-06\n",
      "Epoch 126 Loss:0.6518 Accuracy:0.7687 F1:0.4664  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4664 LR = 1.953125e-06\n",
      "Epoch 127 Loss:0.6520 Accuracy:0.7687 F1:0.4668  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4668 LR = 1.953125e-06\n",
      "Epoch 128 Loss:0.6522 Accuracy:0.7687 F1:0.4673  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4673 LR = 1.953125e-06\n",
      "Epoch 129 Loss:0.6515 Accuracy:0.7687 F1:0.4677  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4677 LR = 1.953125e-06\n",
      "Epoch 130 Loss:0.6518 Accuracy:0.7687 F1:0.4681  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4681 LR = 1.953125e-06\n",
      "Epoch 131 Loss:0.6518 Accuracy:0.7687 F1:0.4685  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4685 LR = 1.953125e-06\n",
      "Epoch 132 Loss:0.6518 Accuracy:0.7687 F1:0.4689  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4689 LR = 1.953125e-06\n",
      "Epoch 133 Loss:0.6516 Accuracy:0.7687 F1:0.4693  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4693 LR = 1.953125e-06\n",
      "Epoch 134 Loss:0.6520 Accuracy:0.7687 F1:0.4697  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4697 LR = 9.765625e-07\n",
      "Epoch 135 Loss:0.6516 Accuracy:0.7687 F1:0.4700  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4700 LR = 9.765625e-07\n",
      "Epoch 136 Loss:0.6514 Accuracy:0.7687 F1:0.4704  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4704 LR = 9.765625e-07\n",
      "Epoch 137 Loss:0.6518 Accuracy:0.7687 F1:0.4708  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4708 LR = 9.765625e-07\n",
      "Epoch 138 Loss:0.6515 Accuracy:0.7687 F1:0.4711  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4711 LR = 9.765625e-07\n",
      "Epoch 139 Loss:0.6517 Accuracy:0.7687 F1:0.4715  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4715 LR = 9.765625e-07\n",
      "Epoch 140 Loss:0.6518 Accuracy:0.7687 F1:0.4718  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4718 LR = 9.765625e-07\n",
      "Epoch 141 Loss:0.6522 Accuracy:0.7687 F1:0.4722  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4722 LR = 9.765625e-07\n",
      "Epoch 142 Loss:0.6518 Accuracy:0.7687 F1:0.4725  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4725 LR = 9.765625e-07\n",
      "Epoch 143 Loss:0.6516 Accuracy:0.7687 F1:0.4728  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4728 LR = 9.765625e-07\n",
      "Epoch 144 Loss:0.6516 Accuracy:0.7687 F1:0.4732  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4732 LR = 9.765625e-07\n",
      "Epoch 145 Loss:0.6516 Accuracy:0.7687 F1:0.4735  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4735 LR = 4.8828125e-07\n",
      "Epoch 146 Loss:0.6516 Accuracy:0.7687 F1:0.4738  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4738 LR = 4.8828125e-07\n",
      "Epoch 147 Loss:0.6516 Accuracy:0.7687 F1:0.4741  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4741 LR = 4.8828125e-07\n",
      "Epoch 148 Loss:0.6517 Accuracy:0.7687 F1:0.4744  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4744 LR = 4.8828125e-07\n",
      "Epoch 149 Loss:0.6516 Accuracy:0.7687 F1:0.4748  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4748 LR = 4.8828125e-07\n",
      "Epoch 150 Loss:0.6515 Accuracy:0.7687 F1:0.4751  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4751 LR = 4.8828125e-07\n",
      "Epoch 151 Loss:0.6515 Accuracy:0.7687 F1:0.4754  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4754 LR = 4.8828125e-07\n",
      "Epoch 152 Loss:0.6519 Accuracy:0.7687 F1:0.4757  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4757 LR = 4.8828125e-07\n",
      "Epoch 153 Loss:0.6516 Accuracy:0.7687 F1:0.4759  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4759 LR = 4.8828125e-07\n",
      "Epoch 154 Loss:0.6519 Accuracy:0.7687 F1:0.4762  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4762 LR = 4.8828125e-07\n",
      "Epoch 155 Loss:0.6519 Accuracy:0.7687 F1:0.4765  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4765 LR = 4.8828125e-07\n",
      "Epoch 156 Loss:0.6516 Accuracy:0.7687 F1:0.4768  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4768 LR = 2.44140625e-07\n",
      "Epoch 157 Loss:0.6517 Accuracy:0.7687 F1:0.4771  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4771 LR = 2.44140625e-07\n",
      "Epoch 158 Loss:0.6519 Accuracy:0.7687 F1:0.4773  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4773 LR = 2.44140625e-07\n",
      "Epoch 159 Loss:0.6518 Accuracy:0.7687 F1:0.4776  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4776 LR = 2.44140625e-07\n",
      "Epoch 160 Loss:0.6514 Accuracy:0.7687 F1:0.4779  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4779 LR = 2.44140625e-07\n",
      "Epoch 161 Loss:0.6518 Accuracy:0.7687 F1:0.4781  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4781 LR = 2.44140625e-07\n",
      "Epoch 162 Loss:0.6513 Accuracy:0.7687 F1:0.4784  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4784 LR = 2.44140625e-07\n",
      "Epoch 163 Loss:0.6519 Accuracy:0.7687 F1:0.4787  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4787 LR = 2.44140625e-07\n",
      "Epoch 164 Loss:0.6522 Accuracy:0.7687 F1:0.4789  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4789 LR = 2.44140625e-07\n",
      "Epoch 165 Loss:0.6519 Accuracy:0.7687 F1:0.4792  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4792 LR = 2.44140625e-07\n",
      "Epoch 166 Loss:0.6518 Accuracy:0.7687 F1:0.4794  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4794 LR = 2.44140625e-07\n",
      "Epoch 167 Loss:0.6521 Accuracy:0.7687 F1:0.4796  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4796 LR = 1.220703125e-07\n",
      "Epoch 168 Loss:0.6513 Accuracy:0.7687 F1:0.4799  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4799 LR = 1.220703125e-07\n",
      "Epoch 169 Loss:0.6519 Accuracy:0.7687 F1:0.4801  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4801 LR = 1.220703125e-07\n",
      "Epoch 170 Loss:0.6518 Accuracy:0.7687 F1:0.4804  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4804 LR = 1.220703125e-07\n",
      "Epoch 171 Loss:0.6518 Accuracy:0.7687 F1:0.4806  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4806 LR = 1.220703125e-07\n",
      "Epoch 172 Loss:0.6523 Accuracy:0.7687 F1:0.4808  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4808 LR = 1.220703125e-07\n",
      "Epoch 173 Loss:0.6519 Accuracy:0.7687 F1:0.4810  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4810 LR = 1.220703125e-07\n",
      "Epoch 174 Loss:0.6521 Accuracy:0.7687 F1:0.4813  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4813 LR = 1.220703125e-07\n",
      "Epoch 175 Loss:0.6518 Accuracy:0.7687 F1:0.4815  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4815 LR = 1.220703125e-07\n",
      "Epoch 176 Loss:0.6514 Accuracy:0.7687 F1:0.4817  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4817 LR = 1.220703125e-07\n",
      "Epoch 177 Loss:0.6516 Accuracy:0.7687 F1:0.4819  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4819 LR = 1.220703125e-07\n",
      "Epoch 178 Loss:0.6518 Accuracy:0.7687 F1:0.4821  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4821 LR = 6.103515625e-08\n",
      "Epoch 179 Loss:0.6516 Accuracy:0.7687 F1:0.4823  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4823 LR = 6.103515625e-08\n",
      "Epoch 180 Loss:0.6515 Accuracy:0.7687 F1:0.4825  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4825 LR = 6.103515625e-08\n",
      "Epoch 181 Loss:0.6519 Accuracy:0.7687 F1:0.4828  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4828 LR = 6.103515625e-08\n",
      "Epoch 182 Loss:0.6517 Accuracy:0.7687 F1:0.4830  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4830 LR = 6.103515625e-08\n",
      "Epoch 183 Loss:0.6518 Accuracy:0.7687 F1:0.4832  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4832 LR = 6.103515625e-08\n",
      "Epoch 184 Loss:0.6519 Accuracy:0.7687 F1:0.4834  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4834 LR = 6.103515625e-08\n",
      "Epoch 185 Loss:0.6510 Accuracy:0.7687 F1:0.4836  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4836 LR = 6.103515625e-08\n",
      "Epoch 186 Loss:0.6514 Accuracy:0.7687 F1:0.4838  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4838 LR = 6.103515625e-08\n",
      "Epoch 187 Loss:0.6521 Accuracy:0.7687 F1:0.4839  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4839 LR = 6.103515625e-08\n",
      "Epoch 188 Loss:0.6518 Accuracy:0.7687 F1:0.4841  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4841 LR = 6.103515625e-08\n",
      "Epoch 189 Loss:0.6519 Accuracy:0.7687 F1:0.4843  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4843 LR = 3.0517578125e-08\n",
      "Epoch 190 Loss:0.6519 Accuracy:0.7687 F1:0.4845  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4845 LR = 3.0517578125e-08\n",
      "Epoch 191 Loss:0.6516 Accuracy:0.7687 F1:0.4847  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4847 LR = 3.0517578125e-08\n",
      "Epoch 192 Loss:0.6518 Accuracy:0.7687 F1:0.4849  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4849 LR = 3.0517578125e-08\n",
      "Epoch 193 Loss:0.6519 Accuracy:0.7687 F1:0.4851  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4851 LR = 3.0517578125e-08\n",
      "Epoch 194 Loss:0.6516 Accuracy:0.7687 F1:0.4852  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4852 LR = 3.0517578125e-08\n",
      "Epoch 195 Loss:0.6515 Accuracy:0.7687 F1:0.4854  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4854 LR = 3.0517578125e-08\n",
      "Epoch 196 Loss:0.6518 Accuracy:0.7687 F1:0.4856  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4856 LR = 3.0517578125e-08\n",
      "Epoch 197 Loss:0.6519 Accuracy:0.7687 F1:0.4858  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4858 LR = 3.0517578125e-08\n",
      "Epoch 198 Loss:0.6516 Accuracy:0.7687 F1:0.4859  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4859 LR = 3.0517578125e-08\n",
      "Epoch 199 Loss:0.6516 Accuracy:0.7687 F1:0.4861  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4861 LR = 3.0517578125e-08\n",
      "Epoch 200 Loss:0.6518 Accuracy:0.7687 F1:0.4863  Val Loss:0.6777 Val Accuracy:0.7657 Val F1:0.4863 LR = 1.52587890625e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([0.7019132928414779,\n",
       "   0.6931506568735296,\n",
       "   0.6931497768922286,\n",
       "   0.6931489250876687,\n",
       "   0.6931480461900884,\n",
       "   0.693147477236661,\n",
       "   0.6931472832506353,\n",
       "   0.6931471705436707,\n",
       "   0.6931469711390409,\n",
       "   0.6931121056730097,\n",
       "   0.6875372333960099,\n",
       "   0.6757726669311523,\n",
       "   0.6731557640162381,\n",
       "   0.6673652312972329,\n",
       "   0.6674859339540655,\n",
       "   0.6659338094971397,\n",
       "   0.6660365928303111,\n",
       "   0.6647123239257119,\n",
       "   0.6630613760514693,\n",
       "   0.6639198216524991,\n",
       "   0.6634709520773454,\n",
       "   0.6623709093440663,\n",
       "   0.6630706906318664,\n",
       "   0.6610744151202115,\n",
       "   0.6625717434016141,\n",
       "   0.6611752759326588,\n",
       "   0.6594665657390247,\n",
       "   0.6611161448738792,\n",
       "   0.6625533114780079,\n",
       "   0.6589552196589383,\n",
       "   0.6601053162054582,\n",
       "   0.6581683917479082,\n",
       "   0.6591029925779863,\n",
       "   0.6599001060832631,\n",
       "   0.6589713974432512,\n",
       "   0.6576126846400174,\n",
       "   0.6568413918668573,\n",
       "   0.6575597232038325,\n",
       "   0.6564569722522389,\n",
       "   0.6562481186606667,\n",
       "   0.6564919883554632,\n",
       "   0.6560956911607222,\n",
       "   0.6553360028700395,\n",
       "   0.6553703373128718,\n",
       "   0.6562582872130654,\n",
       "   0.6554826552217657,\n",
       "   0.6548554322936319,\n",
       "   0.6548408540812406,\n",
       "   0.6537537780675021,\n",
       "   0.6541362383148887,\n",
       "   0.6535539334470576,\n",
       "   0.6541854630817067,\n",
       "   0.6540601025928151,\n",
       "   0.6538700699806214,\n",
       "   0.6542872548103332,\n",
       "   0.6538185889070685,\n",
       "   0.6532902229915966,\n",
       "   0.6534152442758734,\n",
       "   0.6527495817704634,\n",
       "   0.6527289293029092,\n",
       "   0.6532327283512462,\n",
       "   0.6527738148515875,\n",
       "   0.6532213297757236,\n",
       "   0.652522745999423,\n",
       "   0.6527879747477445,\n",
       "   0.6527957872910933,\n",
       "   0.6522454511035572,\n",
       "   0.6523601737889376,\n",
       "   0.6524140520529313,\n",
       "   0.652014702016657,\n",
       "   0.652465896172957,\n",
       "   0.6522557280280373,\n",
       "   0.6521122520620173,\n",
       "   0.6523929606784474,\n",
       "   0.6524237155914306,\n",
       "   0.6522389921275052,\n",
       "   0.6525764454494823,\n",
       "   0.6526794867082075,\n",
       "   0.65195213989778,\n",
       "   0.6520707964897156,\n",
       "   0.652050524408167,\n",
       "   0.6521733836694197,\n",
       "   0.6517801869999279,\n",
       "   0.6523220322348855,\n",
       "   0.6522521842609752,\n",
       "   0.6518832791935314,\n",
       "   0.6518435023047707,\n",
       "   0.651731323112141,\n",
       "   0.6518889026208358,\n",
       "   0.6519629337570884,\n",
       "   0.6520506913011724,\n",
       "   0.6516001506285234,\n",
       "   0.6521283637393605,\n",
       "   0.6514449921521274,\n",
       "   0.6522119088606401,\n",
       "   0.652198501066728,\n",
       "   0.6521845405752008,\n",
       "   0.6520369085398587,\n",
       "   0.652214392748746,\n",
       "   0.6520609823140231,\n",
       "   0.6519862305034291,\n",
       "   0.6516822413964705,\n",
       "   0.6519781990484758,\n",
       "   0.6518411972305992,\n",
       "   0.6518285372040489,\n",
       "   0.6515104987404563,\n",
       "   0.651865950497714,\n",
       "   0.6516742717136036,\n",
       "   0.6519649223847823,\n",
       "   0.652113349871202,\n",
       "   0.6518189625306563,\n",
       "   0.6519579789855263,\n",
       "   0.6518179568377408,\n",
       "   0.6515004060485147,\n",
       "   0.6518011483279141,\n",
       "   0.652269277789376,\n",
       "   0.6516248453747142,\n",
       "   0.6515051614154469,\n",
       "   0.6523235646161166,\n",
       "   0.6516516934741627,\n",
       "   0.6517866329713301,\n",
       "   0.6517842173576355,\n",
       "   0.6515453620390459,\n",
       "   0.6520199905742299,\n",
       "   0.6516589034687389,\n",
       "   0.6517688491127708,\n",
       "   0.6520303281870755,\n",
       "   0.6521695592186668,\n",
       "   0.6514798532832753,\n",
       "   0.6517897757616911,\n",
       "   0.6517746578563344,\n",
       "   0.6517721284519542,\n",
       "   0.6515974695032293,\n",
       "   0.6520144993608649,\n",
       "   0.6516211455518549,\n",
       "   0.6513560674407265,\n",
       "   0.6517687743360346,\n",
       "   0.6514723095026883,\n",
       "   0.6517278476194902,\n",
       "   0.6517926064404574,\n",
       "   0.6521827177567916,\n",
       "   0.6517672939734025,\n",
       "   0.6516264156861739,\n",
       "   0.6516180742870678,\n",
       "   0.6516165364872325,\n",
       "   0.6516205896030772,\n",
       "   0.6516208312728188,\n",
       "   0.6517070206728849,\n",
       "   0.6516151265664534,\n",
       "   0.6515022743831981,\n",
       "   0.6514670816334811,\n",
       "   0.6519096385348927,\n",
       "   0.6516153422268954,\n",
       "   0.6519101392139088,\n",
       "   0.651933753490448,\n",
       "   0.6515994949774309,\n",
       "   0.6517177451740611,\n",
       "   0.6519091096791354,\n",
       "   0.6517635442993858,\n",
       "   0.6513684782114896,\n",
       "   0.6517663673921065,\n",
       "   0.6513176083564758,\n",
       "   0.6519070766188881,\n",
       "   0.6521681081164967,\n",
       "   0.6519129027019848,\n",
       "   0.6517604556950656,\n",
       "   0.6520550641146573,\n",
       "   0.6513342824849215,\n",
       "   0.6519072901118885,\n",
       "   0.6517595767974853,\n",
       "   0.6518045663833618,\n",
       "   0.6523278518156572,\n",
       "   0.6519405234943737,\n",
       "   0.6521414680914446,\n",
       "   0.651760010285811,\n",
       "   0.6513772390105508,\n",
       "   0.6516117377714677,\n",
       "   0.6517593838951804,\n",
       "   0.6515554503961043,\n",
       "   0.6514650962569497,\n",
       "   0.6518504630435596,\n",
       "   0.6517109578306025,\n",
       "   0.6517587065696716,\n",
       "   0.6519081906838851,\n",
       "   0.651046709580855,\n",
       "   0.6514449693939902,\n",
       "   0.6521473516117443,\n",
       "   0.6517641858621077,\n",
       "   0.6519265716726129,\n",
       "   0.6519036650657654,\n",
       "   0.651611252264543,\n",
       "   0.6517615209926259,\n",
       "   0.6519062345678156,\n",
       "   0.6516108241948214,\n",
       "   0.6514788259159435,\n",
       "   0.6517595334486528,\n",
       "   0.6519143483855507,\n",
       "   0.6516106594692577,\n",
       "   0.6516116293993863,\n",
       "   0.6517637512900613],\n",
       "  [0.6496852040290833,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6519747972488403,\n",
       "   0.6697195172309875,\n",
       "   0.7126502394676208,\n",
       "   0.7137950658798218,\n",
       "   0.7275329232215881,\n",
       "   0.731539785861969,\n",
       "   0.7292501330375671,\n",
       "   0.7338294386863708,\n",
       "   0.7366914749145508,\n",
       "   0.7384086847305298,\n",
       "   0.7372638583183289,\n",
       "   0.7395535111427307,\n",
       "   0.7418431639671326,\n",
       "   0.7401259541511536,\n",
       "   0.7447052001953125,\n",
       "   0.7424155473709106,\n",
       "   0.7447052001953125,\n",
       "   0.7498568892478943,\n",
       "   0.7447052001953125,\n",
       "   0.7406983375549316,\n",
       "   0.7487120628356934,\n",
       "   0.7475672364234924,\n",
       "   0.7538637518882751,\n",
       "   0.7504293322563171,\n",
       "   0.7469948530197144,\n",
       "   0.7527189254760742,\n",
       "   0.7538637518882751,\n",
       "   0.7555810213088989,\n",
       "   0.7498568892478943,\n",
       "   0.756153404712677,\n",
       "   0.7590154409408569,\n",
       "   0.754436194896698,\n",
       "   0.7590154409408569,\n",
       "   0.7601602673530579,\n",
       "   0.7613050937652588,\n",
       "   0.756153404712677,\n",
       "   0.7613050937652588,\n",
       "   0.7590154409408569,\n",
       "   0.7584430575370789,\n",
       "   0.7624499201774597,\n",
       "   0.7635947465896606,\n",
       "   0.7618774771690369,\n",
       "   0.7613050937652588,\n",
       "   0.7641671299934387,\n",
       "   0.7624499201774597,\n",
       "   0.7618774771690369,\n",
       "   0.7624499201774597,\n",
       "   0.7653119564056396,\n",
       "   0.7658843994140625,\n",
       "   0.7658843994140625,\n",
       "   0.7664567828178406,\n",
       "   0.7670291662216187,\n",
       "   0.7658843994140625,\n",
       "   0.7658843994140625,\n",
       "   0.7658843994140625,\n",
       "   0.7664567828178406,\n",
       "   0.7664567828178406,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7681739926338196,\n",
       "   0.7681739926338196,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7681739926338196,\n",
       "   0.7681739926338196,\n",
       "   0.7681739926338196,\n",
       "   0.7681739926338196,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7681739926338196,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7681739926338196,\n",
       "   0.7681739926338196,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7676016092300415,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424,\n",
       "   0.7687464356422424],\n",
       "  [0.012903225608170033,\n",
       "   0.0065146577544510365,\n",
       "   0.004357298370450735,\n",
       "   0.00327332247979939,\n",
       "   0.0026212320663034916,\n",
       "   0.0021857924293726683,\n",
       "   0.0018744142726063728,\n",
       "   0.001640689093619585,\n",
       "   0.0014587892219424248,\n",
       "   0.00131319765932858,\n",
       "   0.014499186538159847,\n",
       "   0.05263853818178177,\n",
       "   0.0817255824804306,\n",
       "   0.10844561457633972,\n",
       "   0.13452644646167755,\n",
       "   0.15454798936843872,\n",
       "   0.1740785539150238,\n",
       "   0.19123828411102295,\n",
       "   0.20632633566856384,\n",
       "   0.2195226550102234,\n",
       "   0.23216575384140015,\n",
       "   0.24286262691020966,\n",
       "   0.253192275762558,\n",
       "   0.2624014914035797,\n",
       "   0.27128952741622925,\n",
       "   0.27953463792800903,\n",
       "   0.2875817120075226,\n",
       "   0.2944409251213074,\n",
       "   0.3000187277793884,\n",
       "   0.30588236451148987,\n",
       "   0.3112444579601288,\n",
       "   0.3175908029079437,\n",
       "   0.3229310214519501,\n",
       "   0.3276875913143158,\n",
       "   0.3324219584465027,\n",
       "   0.33694130182266235,\n",
       "   0.34132400155067444,\n",
       "   0.3450166583061218,\n",
       "   0.34911641478538513,\n",
       "   0.35309168696403503,\n",
       "   0.3565589487552643,\n",
       "   0.36025500297546387,\n",
       "   0.36378514766693115,\n",
       "   0.36727455258369446,\n",
       "   0.37002626061439514,\n",
       "   0.37331005930900574,\n",
       "   0.37605613470077515,\n",
       "   0.37866926193237305,\n",
       "   0.3815311789512634,\n",
       "   0.38416188955307007,\n",
       "   0.38668379187583923,\n",
       "   0.3890314996242523,\n",
       "   0.39147886633872986,\n",
       "   0.39368584752082825,\n",
       "   0.3958541452884674,\n",
       "   0.3978661298751831,\n",
       "   0.3999820947647095,\n",
       "   0.4020552933216095,\n",
       "   0.4040277600288391,\n",
       "   0.40593892335891724,\n",
       "   0.4078660011291504,\n",
       "   0.4096617102622986,\n",
       "   0.41144418716430664,\n",
       "   0.41309764981269836,\n",
       "   0.4147280752658844,\n",
       "   0.41628387570381165,\n",
       "   0.4178502857685089,\n",
       "   0.419389545917511,\n",
       "   0.4208604097366333,\n",
       "   0.4223077595233917,\n",
       "   0.42371198534965515,\n",
       "   0.4250824749469757,\n",
       "   0.4264134168624878,\n",
       "   0.42767977714538574,\n",
       "   0.42891067266464233,\n",
       "   0.43015220761299133,\n",
       "   0.431341290473938,\n",
       "   0.43251657485961914,\n",
       "   0.43364259600639343,\n",
       "   0.4347635805606842,\n",
       "   0.435855507850647,\n",
       "   0.4368957281112671,\n",
       "   0.43793314695358276,\n",
       "   0.4389447271823883,\n",
       "   0.439908504486084,\n",
       "   0.44084885716438293,\n",
       "   0.441788911819458,\n",
       "   0.4427066445350647,\n",
       "   0.44355931878089905,\n",
       "   0.4444352388381958,\n",
       "   0.44529107213020325,\n",
       "   0.4461274743080139,\n",
       "   0.4469451308250427,\n",
       "   0.4477446377277374,\n",
       "   0.44852662086486816,\n",
       "   0.4492916464805603,\n",
       "   0.4500402510166168,\n",
       "   0.4507729411125183,\n",
       "   0.45149025321006775,\n",
       "   0.45219263434410095,\n",
       "   0.4528805613517761,\n",
       "   0.45355451107025146,\n",
       "   0.45421484112739563,\n",
       "   0.45486196875572205,\n",
       "   0.45549634099006653,\n",
       "   0.45611828565597534,\n",
       "   0.45672816038131714,\n",
       "   0.45732635259628296,\n",
       "   0.4579131305217743,\n",
       "   0.4584888815879822,\n",
       "   0.4590539038181305,\n",
       "   0.4596084654331207,\n",
       "   0.46015286445617676,\n",
       "   0.46068739891052246,\n",
       "   0.46121230721473694,\n",
       "   0.4617278575897217,\n",
       "   0.4622342884540558,\n",
       "   0.46273183822631836,\n",
       "   0.4632207751274109,\n",
       "   0.4637012779712677,\n",
       "   0.4641736149787903,\n",
       "   0.4646379053592682,\n",
       "   0.4650944471359253,\n",
       "   0.46554335951805115,\n",
       "   0.46598488092422485,\n",
       "   0.46641919016838074,\n",
       "   0.46684643626213074,\n",
       "   0.4672667980194092,\n",
       "   0.467680424451828,\n",
       "   0.4680875241756439,\n",
       "   0.4684881865978241,\n",
       "   0.46888265013694763,\n",
       "   0.4692709743976593,\n",
       "   0.46965333819389343,\n",
       "   0.4700298607349396,\n",
       "   0.47040069103240967,\n",
       "   0.47076594829559326,\n",
       "   0.4711257815361023,\n",
       "   0.47148028016090393,\n",
       "   0.4718295633792877,\n",
       "   0.4721737802028656,\n",
       "   0.47251299023628235,\n",
       "   0.4728473424911499,\n",
       "   0.47317689657211304,\n",
       "   0.4735018014907837,\n",
       "   0.47382214665412903,\n",
       "   0.4741379916667938,\n",
       "   0.47444948554039,\n",
       "   0.47475665807724,\n",
       "   0.4750596582889557,\n",
       "   0.4753585159778595,\n",
       "   0.47565335035324097,\n",
       "   0.4759442210197449,\n",
       "   0.47623124718666077,\n",
       "   0.47651445865631104,\n",
       "   0.47679394483566284,\n",
       "   0.47706979513168335,\n",
       "   0.47734203934669495,\n",
       "   0.4776107966899872,\n",
       "   0.47787609696388245,\n",
       "   0.4781380295753479,\n",
       "   0.4783966541290283,\n",
       "   0.4786520302295685,\n",
       "   0.47890421748161316,\n",
       "   0.47915327548980713,\n",
       "   0.47939926385879517,\n",
       "   0.47964224219322205,\n",
       "   0.47988224029541016,\n",
       "   0.48011934757232666,\n",
       "   0.48035359382629395,\n",
       "   0.4805850386619568,\n",
       "   0.48081374168395996,\n",
       "   0.48103973269462585,\n",
       "   0.48126307129859924,\n",
       "   0.4814837872982025,\n",
       "   0.48170197010040283,\n",
       "   0.4819176197052002,\n",
       "   0.482130765914917,\n",
       "   0.4823414981365204,\n",
       "   0.48254984617233276,\n",
       "   0.4827558398246765,\n",
       "   0.482959508895874,\n",
       "   0.48316091299057007,\n",
       "   0.48336008191108704,\n",
       "   0.4835570752620697,\n",
       "   0.4837518632411957,\n",
       "   0.4839445650577545,\n",
       "   0.48413515090942383,\n",
       "   0.4843236804008484,\n",
       "   0.4845101833343506,\n",
       "   0.4846946895122528,\n",
       "   0.48487722873687744,\n",
       "   0.48505786061286926,\n",
       "   0.48523658514022827,\n",
       "   0.48541343212127686,\n",
       "   0.4855884611606598,\n",
       "   0.4857616424560547,\n",
       "   0.4859330654144287,\n",
       "   0.48610273003578186,\n",
       "   0.4862706661224365]),\n",
       " ([0.6931564062833786,\n",
       "   0.6931534856557846,\n",
       "   0.6931519061326981,\n",
       "   0.6931501030921936,\n",
       "   0.6931484490633011,\n",
       "   0.6931478381156921,\n",
       "   0.6931476444005966,\n",
       "   0.693147599697113,\n",
       "   0.6931476145982742,\n",
       "   0.6930552870035172,\n",
       "   0.7463078796863556,\n",
       "   0.6853159070014954,\n",
       "   0.7631767839193344,\n",
       "   0.6807599663734436,\n",
       "   0.6942887902259827,\n",
       "   0.8168805241584778,\n",
       "   0.6871188282966614,\n",
       "   0.6849650144577026,\n",
       "   0.6995199620723724,\n",
       "   0.6826414465904236,\n",
       "   0.6833958923816681,\n",
       "   0.6803110390901566,\n",
       "   0.6850635558366776,\n",
       "   0.6726749539375305,\n",
       "   0.6802587360143661,\n",
       "   0.6793070137500763,\n",
       "   0.6788386851549149,\n",
       "   0.6821169257164001,\n",
       "   0.679205596446991,\n",
       "   0.6794784516096115,\n",
       "   0.6765156686306,\n",
       "   0.6791587024927139,\n",
       "   0.6815962642431259,\n",
       "   0.6772153079509735,\n",
       "   0.6787521988153458,\n",
       "   0.6783032566308975,\n",
       "   0.6785405576229095,\n",
       "   0.6776392757892609,\n",
       "   0.678566113114357,\n",
       "   0.6791244149208069,\n",
       "   0.6779423654079437,\n",
       "   0.6779715865850449,\n",
       "   0.6775662451982498,\n",
       "   0.677933469414711,\n",
       "   0.6767522245645523,\n",
       "   0.6787646859884262,\n",
       "   0.677033007144928,\n",
       "   0.6778140366077423,\n",
       "   0.6777990162372589,\n",
       "   0.6776275485754013,\n",
       "   0.677832767367363,\n",
       "   0.6777798682451248,\n",
       "   0.6776463836431503,\n",
       "   0.6769646257162094,\n",
       "   0.6778844147920609,\n",
       "   0.677785113453865,\n",
       "   0.6778328120708466,\n",
       "   0.6777548044919968,\n",
       "   0.6778003573417664,\n",
       "   0.6777724921703339,\n",
       "   0.6778151839971542,\n",
       "   0.6776899695396423,\n",
       "   0.6776696145534515,\n",
       "   0.6777582764625549,\n",
       "   0.6777938306331635,\n",
       "   0.6777064055204391,\n",
       "   0.6776484102010727,\n",
       "   0.6778273582458496,\n",
       "   0.6776853650808334,\n",
       "   0.6776925623416901,\n",
       "   0.677733838558197,\n",
       "   0.6776870936155319,\n",
       "   0.6776494532823563,\n",
       "   0.6777547001838684,\n",
       "   0.677691325545311,\n",
       "   0.6776469796895981,\n",
       "   0.6776266545057297,\n",
       "   0.6777155548334122,\n",
       "   0.6776557266712189,\n",
       "   0.6776674091815948,\n",
       "   0.6776695847511292,\n",
       "   0.6777180731296539,\n",
       "   0.6776934415102005,\n",
       "   0.677667036652565,\n",
       "   0.6776812672615051,\n",
       "   0.6776623874902725,\n",
       "   0.6776833534240723,\n",
       "   0.6776836216449738,\n",
       "   0.6776229739189148,\n",
       "   0.6776819378137589,\n",
       "   0.677681639790535,\n",
       "   0.6776984184980392,\n",
       "   0.6776878982782364,\n",
       "   0.6776688545942307,\n",
       "   0.677684023976326,\n",
       "   0.6776599138975143,\n",
       "   0.6776839047670364,\n",
       "   0.6776526123285294,\n",
       "   0.6776653826236725,\n",
       "   0.6776861548423767,\n",
       "   0.6776771247386932,\n",
       "   0.6776673942804337,\n",
       "   0.6776680946350098,\n",
       "   0.6776852011680603,\n",
       "   0.6776700466871262,\n",
       "   0.6776716113090515,\n",
       "   0.6776740103960037,\n",
       "   0.6776729375123978,\n",
       "   0.677670031785965,\n",
       "   0.677673727273941,\n",
       "   0.6776788085699081,\n",
       "   0.6776683777570724,\n",
       "   0.677666649222374,\n",
       "   0.6776752918958664,\n",
       "   0.677675724029541,\n",
       "   0.6776673495769501,\n",
       "   0.6776715070009232,\n",
       "   0.6776756346225739,\n",
       "   0.6776617765426636,\n",
       "   0.6776647120714188,\n",
       "   0.677677571773529,\n",
       "   0.6776711195707321,\n",
       "   0.6776795089244843,\n",
       "   0.6776747852563858,\n",
       "   0.6776715815067291,\n",
       "   0.6776730269193649,\n",
       "   0.6776712238788605,\n",
       "   0.6776726096868515,\n",
       "   0.677671492099762,\n",
       "   0.6776719242334366,\n",
       "   0.6776699870824814,\n",
       "   0.6776726990938187,\n",
       "   0.6776689141988754,\n",
       "   0.6776688396930695,\n",
       "   0.6776695996522903,\n",
       "   0.6776692122220993,\n",
       "   0.6776715368032455,\n",
       "   0.6776699274778366,\n",
       "   0.6776711940765381,\n",
       "   0.6776718646287918,\n",
       "   0.6776714771986008,\n",
       "   0.6776688545942307,\n",
       "   0.6776708960533142,\n",
       "   0.6776687949895859,\n",
       "   0.6776694506406784,\n",
       "   0.6776698976755142,\n",
       "   0.6776692867279053,\n",
       "   0.6776698678731918,\n",
       "   0.6776698976755142,\n",
       "   0.677670493721962,\n",
       "   0.6776686459779739,\n",
       "   0.6776694506406784,\n",
       "   0.6776690185070038,\n",
       "   0.6776684522628784,\n",
       "   0.6776706278324127,\n",
       "   0.6776690632104874,\n",
       "   0.6776688694953918,\n",
       "   0.6776692718267441,\n",
       "   0.6776693761348724,\n",
       "   0.677669107913971,\n",
       "   0.6776690781116486,\n",
       "   0.6776685416698456,\n",
       "   0.6776690632104874,\n",
       "   0.6776691973209381,\n",
       "   0.6776693165302277,\n",
       "   0.6776687204837799,\n",
       "   0.6776690632104874,\n",
       "   0.6776691377162933,\n",
       "   0.6776686161756516,\n",
       "   0.6776689291000366,\n",
       "   0.6776691526174545,\n",
       "   0.6776688992977142,\n",
       "   0.6776690930128098,\n",
       "   0.6776686906814575,\n",
       "   0.6776690930128098,\n",
       "   0.6776690781116486,\n",
       "   0.6776691675186157,\n",
       "   0.6776690185070038,\n",
       "   0.6776690930128098,\n",
       "   0.677669107913971,\n",
       "   0.6776690483093262,\n",
       "   0.6776690185070038,\n",
       "   0.6776691526174545,\n",
       "   0.6776691228151321,\n",
       "   0.677669107913971,\n",
       "   0.6776689887046814,\n",
       "   0.6776691526174545,\n",
       "   0.6776690185070038,\n",
       "   0.6776690483093262,\n",
       "   0.6776690781116486,\n",
       "   0.6776690632104874,\n",
       "   0.6776690632104874,\n",
       "   0.6776691228151321,\n",
       "   0.6776690930128098,\n",
       "   0.6776691377162933,\n",
       "   0.677669107913971,\n",
       "   0.6776690930128098,\n",
       "   0.677669107913971,\n",
       "   0.6776690632104874,\n",
       "   0.6776691228151321],\n",
       "  [0.6491323113441467,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6513015031814575,\n",
       "   0.6724511981010437,\n",
       "   0.7109544277191162,\n",
       "   0.713123619556427,\n",
       "   0.726681113243103,\n",
       "   0.726681113243103,\n",
       "   0.7304772138595581,\n",
       "   0.7321041226387024,\n",
       "   0.7348155975341797,\n",
       "   0.7369847893714905,\n",
       "   0.735357940196991,\n",
       "   0.7369847893714905,\n",
       "   0.7396963238716125,\n",
       "   0.7386116981506348,\n",
       "   0.7440347075462341,\n",
       "   0.740238606929779,\n",
       "   0.7429500818252563,\n",
       "   0.7478308081626892,\n",
       "   0.7418655157089233,\n",
       "   0.739154040813446,\n",
       "   0.7462038993835449,\n",
       "   0.7462038993835449,\n",
       "   0.7516269087791443,\n",
       "   0.7472885251045227,\n",
       "   0.7451193332672119,\n",
       "   0.7505422830581665,\n",
       "   0.7516269087791443,\n",
       "   0.7532538175582886,\n",
       "   0.7478308081626892,\n",
       "   0.7537961006164551,\n",
       "   0.7565075755119324,\n",
       "   0.7521691918373108,\n",
       "   0.7565075755119324,\n",
       "   0.7575922012329102,\n",
       "   0.7586767673492432,\n",
       "   0.7537961006164551,\n",
       "   0.7586767673492432,\n",
       "   0.7565075755119324,\n",
       "   0.7559652924537659,\n",
       "   0.759761393070221,\n",
       "   0.760845959186554,\n",
       "   0.7592191100120544,\n",
       "   0.7586767673492432,\n",
       "   0.7613883018493652,\n",
       "   0.759761393070221,\n",
       "   0.7592191100120544,\n",
       "   0.759761393070221,\n",
       "   0.7624728679656982,\n",
       "   0.7630152106285095,\n",
       "   0.7630152106285095,\n",
       "   0.763557493686676,\n",
       "   0.7640997767448425,\n",
       "   0.7630152106285095,\n",
       "   0.7630152106285095,\n",
       "   0.7630152106285095,\n",
       "   0.763557493686676,\n",
       "   0.763557493686676,\n",
       "   0.764642059803009,\n",
       "   0.764642059803009,\n",
       "   0.764642059803009,\n",
       "   0.764642059803009,\n",
       "   0.764642059803009,\n",
       "   0.7651844024658203,\n",
       "   0.7651844024658203,\n",
       "   0.764642059803009,\n",
       "   0.764642059803009,\n",
       "   0.7651844024658203,\n",
       "   0.7651844024658203,\n",
       "   0.7651844024658203,\n",
       "   0.7651844024658203,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7651844024658203,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7651844024658203,\n",
       "   0.7651844024658203,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.764642059803009,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868,\n",
       "   0.7657266855239868],\n",
       "  [0.012903225608170033,\n",
       "   0.0065146577544510365,\n",
       "   0.004357298370450735,\n",
       "   0.00327332247979939,\n",
       "   0.0026212320663034916,\n",
       "   0.0021857924293726683,\n",
       "   0.0018744142726063728,\n",
       "   0.001640689093619585,\n",
       "   0.0014587892219424248,\n",
       "   0.00131319765932858,\n",
       "   0.014499186538159847,\n",
       "   0.05263853818178177,\n",
       "   0.0817255824804306,\n",
       "   0.10844561457633972,\n",
       "   0.13452644646167755,\n",
       "   0.15454798936843872,\n",
       "   0.1740785539150238,\n",
       "   0.19123828411102295,\n",
       "   0.20632633566856384,\n",
       "   0.2195226550102234,\n",
       "   0.23216575384140015,\n",
       "   0.24286262691020966,\n",
       "   0.253192275762558,\n",
       "   0.2624014914035797,\n",
       "   0.27128952741622925,\n",
       "   0.27953463792800903,\n",
       "   0.2875817120075226,\n",
       "   0.2944409251213074,\n",
       "   0.3000187277793884,\n",
       "   0.30588236451148987,\n",
       "   0.3112444579601288,\n",
       "   0.3175908029079437,\n",
       "   0.3229310214519501,\n",
       "   0.3276875913143158,\n",
       "   0.3324219584465027,\n",
       "   0.33694130182266235,\n",
       "   0.34132400155067444,\n",
       "   0.3450166583061218,\n",
       "   0.34911641478538513,\n",
       "   0.35309168696403503,\n",
       "   0.3565589487552643,\n",
       "   0.36025500297546387,\n",
       "   0.36378514766693115,\n",
       "   0.36727455258369446,\n",
       "   0.37002626061439514,\n",
       "   0.37331005930900574,\n",
       "   0.37605613470077515,\n",
       "   0.37866926193237305,\n",
       "   0.3815311789512634,\n",
       "   0.38416188955307007,\n",
       "   0.38668379187583923,\n",
       "   0.3890314996242523,\n",
       "   0.39147886633872986,\n",
       "   0.39368584752082825,\n",
       "   0.3958541452884674,\n",
       "   0.3978661298751831,\n",
       "   0.3999820947647095,\n",
       "   0.4020552933216095,\n",
       "   0.4040277600288391,\n",
       "   0.40593892335891724,\n",
       "   0.4078660011291504,\n",
       "   0.4096617102622986,\n",
       "   0.41144418716430664,\n",
       "   0.41309764981269836,\n",
       "   0.4147280752658844,\n",
       "   0.41628387570381165,\n",
       "   0.4178502857685089,\n",
       "   0.419389545917511,\n",
       "   0.4208604097366333,\n",
       "   0.4223077595233917,\n",
       "   0.42371198534965515,\n",
       "   0.4250824749469757,\n",
       "   0.4264134168624878,\n",
       "   0.42767977714538574,\n",
       "   0.42891067266464233,\n",
       "   0.43015220761299133,\n",
       "   0.431341290473938,\n",
       "   0.43251657485961914,\n",
       "   0.43364259600639343,\n",
       "   0.4347635805606842,\n",
       "   0.435855507850647,\n",
       "   0.4368957281112671,\n",
       "   0.43793314695358276,\n",
       "   0.4389447271823883,\n",
       "   0.439908504486084,\n",
       "   0.44084885716438293,\n",
       "   0.441788911819458,\n",
       "   0.4427066445350647,\n",
       "   0.44355931878089905,\n",
       "   0.4444352388381958,\n",
       "   0.44529107213020325,\n",
       "   0.4461274743080139,\n",
       "   0.4469451308250427,\n",
       "   0.4477446377277374,\n",
       "   0.44852662086486816,\n",
       "   0.4492916464805603,\n",
       "   0.4500402510166168,\n",
       "   0.4507729411125183,\n",
       "   0.45149025321006775,\n",
       "   0.45219263434410095,\n",
       "   0.4528805613517761,\n",
       "   0.45355451107025146,\n",
       "   0.45421484112739563,\n",
       "   0.45486196875572205,\n",
       "   0.45549634099006653,\n",
       "   0.45611828565597534,\n",
       "   0.45672816038131714,\n",
       "   0.45732635259628296,\n",
       "   0.4579131305217743,\n",
       "   0.4584888815879822,\n",
       "   0.4590539038181305,\n",
       "   0.4596084654331207,\n",
       "   0.46015286445617676,\n",
       "   0.46068739891052246,\n",
       "   0.46121230721473694,\n",
       "   0.4617278575897217,\n",
       "   0.4622342884540558,\n",
       "   0.46273183822631836,\n",
       "   0.4632207751274109,\n",
       "   0.4637012779712677,\n",
       "   0.4641736149787903,\n",
       "   0.4646379053592682,\n",
       "   0.4650944471359253,\n",
       "   0.46554335951805115,\n",
       "   0.46598488092422485,\n",
       "   0.46641919016838074,\n",
       "   0.46684643626213074,\n",
       "   0.4672667980194092,\n",
       "   0.467680424451828,\n",
       "   0.4680875241756439,\n",
       "   0.4684881865978241,\n",
       "   0.46888265013694763,\n",
       "   0.4692709743976593,\n",
       "   0.46965333819389343,\n",
       "   0.4700298607349396,\n",
       "   0.47040069103240967,\n",
       "   0.47076594829559326,\n",
       "   0.4711257815361023,\n",
       "   0.47148028016090393,\n",
       "   0.4718295633792877,\n",
       "   0.4721737802028656,\n",
       "   0.47251299023628235,\n",
       "   0.4728473424911499,\n",
       "   0.47317689657211304,\n",
       "   0.4735018014907837,\n",
       "   0.47382214665412903,\n",
       "   0.4741379916667938,\n",
       "   0.47444948554039,\n",
       "   0.47475665807724,\n",
       "   0.4750596582889557,\n",
       "   0.4753585159778595,\n",
       "   0.47565335035324097,\n",
       "   0.4759442210197449,\n",
       "   0.47623124718666077,\n",
       "   0.47651445865631104,\n",
       "   0.47679394483566284,\n",
       "   0.47706979513168335,\n",
       "   0.47734203934669495,\n",
       "   0.4776107966899872,\n",
       "   0.47787609696388245,\n",
       "   0.4781380295753479,\n",
       "   0.4783966541290283,\n",
       "   0.4786520302295685,\n",
       "   0.47890421748161316,\n",
       "   0.47915327548980713,\n",
       "   0.47939926385879517,\n",
       "   0.47964224219322205,\n",
       "   0.47988224029541016,\n",
       "   0.48011934757232666,\n",
       "   0.48035359382629395,\n",
       "   0.4805850386619568,\n",
       "   0.48081374168395996,\n",
       "   0.48103973269462585,\n",
       "   0.48126307129859924,\n",
       "   0.4814837872982025,\n",
       "   0.48170197010040283,\n",
       "   0.4819176197052002,\n",
       "   0.482130765914917,\n",
       "   0.4823414981365204,\n",
       "   0.48254984617233276,\n",
       "   0.4827558398246765,\n",
       "   0.482959508895874,\n",
       "   0.48316091299057007,\n",
       "   0.48336008191108704,\n",
       "   0.4835570752620697,\n",
       "   0.4837518632411957,\n",
       "   0.4839445650577545,\n",
       "   0.48413515090942383,\n",
       "   0.4843236804008484,\n",
       "   0.4845101833343506,\n",
       "   0.4846946895122528,\n",
       "   0.48487722873687744,\n",
       "   0.48505786061286926,\n",
       "   0.48523658514022827,\n",
       "   0.48541343212127686,\n",
       "   0.4855884611606598,\n",
       "   0.4857616424560547,\n",
       "   0.4859330654144287,\n",
       "   0.48610273003578186,\n",
       "   0.4862706661224365]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2736b0bfcb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
