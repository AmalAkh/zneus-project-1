{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_TEST_SPLIT = [0.7, 0.15, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"V28 V29 V30 V31 V32 V33\".split(\" \"))] ,axis= 1).values.tolist()\n",
    "        self.labels = self.df[\"Class\"].to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b052c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, TRAIN_VAL_TEST_SPLIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 2\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.BCEWithLogitsLoss()\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision, F1Score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"binary\", num_labels=num_classes)\n",
    "        self.f1 = F1Score(task=\"binary\", num_labels=num_classes, average='macro')\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.f1.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        \n",
    "        best_val_loss = 9999\n",
    "\n",
    "        train_loss_hist = []\n",
    "        train_accuracy_hist = []\n",
    "        train_f1_hist = []\n",
    "\n",
    "        val_loss_hist = []\n",
    "        val_accuracy_hist = []\n",
    "        val_f1_hist = []\n",
    "\n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.f1(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_f1 = self.f1.compute()\n",
    "\n",
    "       \n",
    "            train_accuracy_hist.append(epoch_acc.item())\n",
    "            train_loss_hist.append(epoch_loss)\n",
    "            train_f1_hist.append(epoch_f1.item())\n",
    "\n",
    "            val_loss, val_acc, val_f1 = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-by-loss.pth\")\n",
    "\n",
    "            \n",
    "            val_accuracy_hist.append(val_acc.item())\n",
    "            val_loss_hist.append(val_loss)\n",
    "            val_f1_hist.append(val_f1.item())\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} F1:{epoch_f1:.4f}  Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val F1:{val_f1:.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        return (train_loss_hist, train_accuracy_hist,train_f1_hist), (val_loss_hist, val_accuracy_hist,val_f1_hist)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1359\n",
      "Validation dataset size: 291\n",
      "Test dataset size: 291\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size=27,num_classes=CLASSES, loss_fn=LOSS_FUNCTION, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:0.7180 Accuracy:0.6446 F1:0.0359  Val Loss:0.6953 Val Accuracy:0.6479 Val F1:0.0359 LR = 0.001\n",
      "Epoch 2 Loss:0.6938 Accuracy:0.6468 F1:0.0183  Val Loss:0.6937 Val Accuracy:0.6497 Val F1:0.0183 LR = 0.001\n",
      "Epoch 3 Loss:0.6934 Accuracy:0.6468 F1:0.0123  Val Loss:0.6934 Val Accuracy:0.6497 Val F1:0.0123 LR = 0.001\n",
      "Epoch 4 Loss:0.6933 Accuracy:0.6468 F1:0.0093  Val Loss:0.6933 Val Accuracy:0.6497 Val F1:0.0093 LR = 0.001\n",
      "Epoch 5 Loss:0.6932 Accuracy:0.6468 F1:0.0074  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0074 LR = 0.001\n",
      "Epoch 6 Loss:0.6932 Accuracy:0.6468 F1:0.0062  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0062 LR = 0.001\n",
      "Epoch 7 Loss:0.6932 Accuracy:0.6468 F1:0.0053  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0053 LR = 0.001\n",
      "Epoch 8 Loss:0.6932 Accuracy:0.6468 F1:0.0047  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0047 LR = 0.001\n",
      "Epoch 9 Loss:0.6932 Accuracy:0.6468 F1:0.0041  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0041 LR = 0.001\n",
      "Epoch 10 Loss:0.6932 Accuracy:0.6468 F1:0.0037  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0037 LR = 0.001\n",
      "Epoch 11 Loss:0.6932 Accuracy:0.6468 F1:0.0034  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0034 LR = 0.001\n",
      "Epoch 12 Loss:0.6932 Accuracy:0.6468 F1:0.0031  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0031 LR = 0.0005\n",
      "Epoch 13 Loss:0.6932 Accuracy:0.6468 F1:0.0029  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0029 LR = 0.0005\n",
      "Epoch 14 Loss:0.6932 Accuracy:0.6468 F1:0.0027  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0027 LR = 0.0005\n",
      "Epoch 15 Loss:0.6932 Accuracy:0.6468 F1:0.0025  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0025 LR = 0.0005\n",
      "Epoch 16 Loss:0.6931 Accuracy:0.6468 F1:0.0023  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0023 LR = 0.0005\n",
      "Epoch 17 Loss:0.6931 Accuracy:0.6468 F1:0.0022  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0022 LR = 0.0005\n",
      "Epoch 18 Loss:0.6931 Accuracy:0.6468 F1:0.0021  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0021 LR = 0.00025\n",
      "Epoch 19 Loss:0.6931 Accuracy:0.6468 F1:0.0020  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0020 LR = 0.00025\n",
      "Epoch 20 Loss:0.6931 Accuracy:0.6468 F1:0.0019  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0019 LR = 0.00025\n",
      "Epoch 21 Loss:0.6931 Accuracy:0.6468 F1:0.0018  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0018 LR = 0.00025\n",
      "Epoch 22 Loss:0.6931 Accuracy:0.6468 F1:0.0017  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0017 LR = 0.00025\n",
      "Epoch 23 Loss:0.6931 Accuracy:0.6468 F1:0.0016  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0016 LR = 0.00025\n",
      "Epoch 24 Loss:0.6931 Accuracy:0.6468 F1:0.0016  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0016 LR = 0.000125\n",
      "Epoch 25 Loss:0.6931 Accuracy:0.6468 F1:0.0015  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0015 LR = 0.000125\n",
      "Epoch 26 Loss:0.6931 Accuracy:0.6468 F1:0.0014  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0014 LR = 0.000125\n",
      "Epoch 27 Loss:0.6931 Accuracy:0.6468 F1:0.0014  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0014 LR = 0.000125\n",
      "Epoch 28 Loss:0.6931 Accuracy:0.6468 F1:0.0013  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0013 LR = 0.000125\n",
      "Epoch 29 Loss:0.6931 Accuracy:0.6468 F1:0.0013  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0013 LR = 0.000125\n",
      "Epoch 30 Loss:0.6931 Accuracy:0.6468 F1:0.0012  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0012 LR = 6.25e-05\n",
      "Epoch 31 Loss:0.6931 Accuracy:0.6468 F1:0.0012  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0012 LR = 6.25e-05\n",
      "Epoch 32 Loss:0.6931 Accuracy:0.6468 F1:0.0012  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0012 LR = 6.25e-05\n",
      "Epoch 33 Loss:0.6931 Accuracy:0.6468 F1:0.0011  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0011 LR = 6.25e-05\n",
      "Epoch 34 Loss:0.6931 Accuracy:0.6468 F1:0.0011  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0011 LR = 6.25e-05\n",
      "Epoch 35 Loss:0.6931 Accuracy:0.6468 F1:0.0011  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0011 LR = 6.25e-05\n",
      "Epoch 36 Loss:0.6931 Accuracy:0.6468 F1:0.0010  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0010 LR = 3.125e-05\n",
      "Epoch 37 Loss:0.6931 Accuracy:0.6468 F1:0.0010  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0010 LR = 3.125e-05\n",
      "Epoch 38 Loss:0.6931 Accuracy:0.6468 F1:0.0010  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0010 LR = 3.125e-05\n",
      "Epoch 39 Loss:0.6931 Accuracy:0.6468 F1:0.0010  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0010 LR = 3.125e-05\n",
      "Epoch 40 Loss:0.6931 Accuracy:0.6468 F1:0.0009  Val Loss:0.6932 Val Accuracy:0.6497 Val F1:0.0009 LR = 3.125e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([0.7179890593817068,\n",
       "   0.6938437694726989,\n",
       "   0.6933660742848419,\n",
       "   0.6932524830795997,\n",
       "   0.6932045484698096,\n",
       "   0.6931819707848305,\n",
       "   0.6931689062783885,\n",
       "   0.6931631620540175,\n",
       "   0.6931579307068226,\n",
       "   0.6931555118671683,\n",
       "   0.6931531526321588,\n",
       "   0.6931517262791478,\n",
       "   0.6931512924127801,\n",
       "   0.693150986072629,\n",
       "   0.6931503442830818,\n",
       "   0.6931494127872379,\n",
       "   0.6931493892226108,\n",
       "   0.6931490565455237,\n",
       "   0.6931490537732147,\n",
       "   0.6931489456531613,\n",
       "   0.6931485436683478,\n",
       "   0.6931483412897864,\n",
       "   0.6931481957435608,\n",
       "   0.6931474000908607,\n",
       "   0.6931477715802747,\n",
       "   0.6931479198988094,\n",
       "   0.693147809006447,\n",
       "   0.693147036918374,\n",
       "   0.6931470452353011,\n",
       "   0.6931475109832231,\n",
       "   0.6931477327679478,\n",
       "   0.6931472600892533,\n",
       "   0.6931475179139958,\n",
       "   0.6931473127631254,\n",
       "   0.6931466474089512,\n",
       "   0.6931473210800526,\n",
       "   0.6931470965230188,\n",
       "   0.693147133949191,\n",
       "   0.6931471089984096,\n",
       "   0.693147246227708],\n",
       "  [0.6445916295051575,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141,\n",
       "   0.6467990875244141],\n",
       "  [0.0359281450510025,\n",
       "   0.01834862306714058,\n",
       "   0.012320328503847122,\n",
       "   0.009273570030927658,\n",
       "   0.0074349441565573215,\n",
       "   0.006204756908118725,\n",
       "   0.005323868710547686,\n",
       "   0.004662004765123129,\n",
       "   0.004146509803831577,\n",
       "   0.0037336652167141438,\n",
       "   0.003395585808902979,\n",
       "   0.0031136481557041407,\n",
       "   0.0028749401681125164,\n",
       "   0.0026702270843088627,\n",
       "   0.00249272957444191,\n",
       "   0.0023373588919639587,\n",
       "   0.002200220013037324,\n",
       "   0.0020782819483429193,\n",
       "   0.001969150034710765,\n",
       "   0.0018709073774516582,\n",
       "   0.001782001811079681,\n",
       "   0.0017011624295264482,\n",
       "   0.0016273392830044031,\n",
       "   0.001559656928293407,\n",
       "   0.0014973796205595136,\n",
       "   0.0014398847706615925,\n",
       "   0.0013866419903934002,\n",
       "   0.001337196328677237,\n",
       "   0.001291155582293868,\n",
       "   0.0012481797020882368,\n",
       "   0.0012079726438969374,\n",
       "   0.0011702750343829393,\n",
       "   0.0011348590487614274,\n",
       "   0.0011015237541869283,\n",
       "   0.001070090918801725,\n",
       "   0.0010404023341834545,\n",
       "   0.0010123165557160974,\n",
       "   0.0009857072727754712,\n",
       "   0.0009604610386304557,\n",
       "   0.0009364757570438087]),\n",
       " ([0.6952801525592804,\n",
       "   0.6936588168144227,\n",
       "   0.6933893680572509,\n",
       "   0.6932789802551269,\n",
       "   0.693227082490921,\n",
       "   0.6932000756263733,\n",
       "   0.6931839466094971,\n",
       "   0.6931755185127259,\n",
       "   0.6931686699390411,\n",
       "   0.6931645810604096,\n",
       "   0.6931612730026245,\n",
       "   0.693158996105194,\n",
       "   0.693157947063446,\n",
       "   0.6931569933891296,\n",
       "   0.6931563496589661,\n",
       "   0.6931556165218353,\n",
       "   0.6931549608707428,\n",
       "   0.6931542813777923,\n",
       "   0.6931540489196777,\n",
       "   0.693153727054596,\n",
       "   0.6931534767150879,\n",
       "   0.6931532144546508,\n",
       "   0.6931529402732849,\n",
       "   0.6931526839733124,\n",
       "   0.6931525766849518,\n",
       "   0.6931524395942688,\n",
       "   0.6931523442268371,\n",
       "   0.6931522011756897,\n",
       "   0.6931521177291871,\n",
       "   0.6931519985198975,\n",
       "   0.6931519508361816,\n",
       "   0.6931518614292145,\n",
       "   0.6931518197059632,\n",
       "   0.6931517541408538,\n",
       "   0.6931516766548157,\n",
       "   0.6931516289710998,\n",
       "   0.6931515693664551,\n",
       "   0.6931515455245971,\n",
       "   0.6931515097618103,\n",
       "   0.6931514859199523],\n",
       "  [0.6478787660598755,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104,\n",
       "   0.649696946144104],\n",
       "  [0.0359281450510025,\n",
       "   0.01834862306714058,\n",
       "   0.012320328503847122,\n",
       "   0.009273570030927658,\n",
       "   0.0074349441565573215,\n",
       "   0.006204756908118725,\n",
       "   0.005323868710547686,\n",
       "   0.004662004765123129,\n",
       "   0.004146509803831577,\n",
       "   0.0037336652167141438,\n",
       "   0.003395585808902979,\n",
       "   0.0031136481557041407,\n",
       "   0.0028749401681125164,\n",
       "   0.0026702270843088627,\n",
       "   0.00249272957444191,\n",
       "   0.0023373588919639587,\n",
       "   0.002200220013037324,\n",
       "   0.0020782819483429193,\n",
       "   0.001969150034710765,\n",
       "   0.0018709073774516582,\n",
       "   0.001782001811079681,\n",
       "   0.0017011624295264482,\n",
       "   0.0016273392830044031,\n",
       "   0.001559656928293407,\n",
       "   0.0014973796205595136,\n",
       "   0.0014398847706615925,\n",
       "   0.0013866419903934002,\n",
       "   0.001337196328677237,\n",
       "   0.001291155582293868,\n",
       "   0.0012481797020882368,\n",
       "   0.0012079726438969374,\n",
       "   0.0011702750343829393,\n",
       "   0.0011348590487614274,\n",
       "   0.0011015237541869283,\n",
       "   0.001070090918801725,\n",
       "   0.0010404023341834545,\n",
       "   0.0010123165557160974,\n",
       "   0.0009857072727754712,\n",
       "   0.0009604610386304557,\n",
       "   0.0009364757570438087]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x11cd69e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd282a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d408fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
