{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 2\n",
    "\n",
    "TRAIN_VAL_TEST_SPLIT = [0.9, 0.05, 0.05]\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.BCEWithLogitsLoss()#pos_weight=torch.tensor([10.0])\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"V28 V29 V30 V31 V32 V33\".split(\" \"))] ,axis= 1).values.tolist()\n",
    "        self.labels = self.df[\"Class\"].to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b052c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, TRAIN_VAL_TEST_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1747\n",
      "Validation dataset size: 97\n",
      "Test dataset size: 97\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f70375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"binary\", num_classes=num_classes)\n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.precision.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        self.train()\n",
    "        best_val_loss = 9999\n",
    "\n",
    "     \n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_precision = self.precision.compute()\n",
    "\n",
    "       \n",
    "\n",
    "            val_loss, val_acc, val_precision = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-by-loss.pth\")\n",
    "\n",
    "         \n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} Positive Precision:{epoch_precision[1].item():.4f} Negative Precision:{epoch_precision[0].item():.4f}  Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val Positive Precision:{val_precision[1].item():.4f} Val Negative Precision:{val_precision[0].item():.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size=27,num_classes=CLASSES, loss_fn=LOSS_FUNCTION, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:0.7019 Accuracy:0.6497 Positive Precision:0.0000 Negative Precision:0.6520  Val Loss:0.6932 Val Accuracy:0.6491 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 2 Loss:0.6932 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6516  Val Loss:0.6932 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 3 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6515  Val Loss:0.6932 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 4 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6515  Val Loss:0.6932 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 5 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 6 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 7 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 8 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 9 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 10 Loss:0.6931 Accuracy:0.6520 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6931 Val Accuracy:0.6513 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 11 Loss:0.6875 Accuracy:0.6697 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.7463 Val Accuracy:0.6725 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 12 Loss:0.6758 Accuracy:0.7127 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.6853 Val Accuracy:0.7110 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 13 Loss:0.6732 Accuracy:0.7138 Positive Precision:0.0000 Negative Precision:0.6514  Val Loss:0.7647 Val Accuracy:0.7131 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 14 Loss:0.6674 Accuracy:0.7287 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6806 Val Accuracy:0.7278 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 15 Loss:0.6676 Accuracy:0.7310 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6944 Val Accuracy:0.7261 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 16 Loss:0.6659 Accuracy:0.7298 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.8168 Val Accuracy:0.7310 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 17 Loss:0.6661 Accuracy:0.7333 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6944 Val Accuracy:0.7321 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 18 Loss:0.6645 Accuracy:0.7378 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6834 Val Accuracy:0.7359 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 19 Loss:0.6630 Accuracy:0.7401 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.7081 Val Accuracy:0.7386 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 20 Loss:0.6638 Accuracy:0.7384 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6825 Val Accuracy:0.7364 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 21 Loss:0.6631 Accuracy:0.7413 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6832 Val Accuracy:0.7386 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 22 Loss:0.6624 Accuracy:0.7418 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6851 Val Accuracy:0.7397 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 23 Loss:0.6614 Accuracy:0.7447 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6825 Val Accuracy:0.7430 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 24 Loss:0.6615 Accuracy:0.7407 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6732 Val Accuracy:0.7397 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 25 Loss:0.6633 Accuracy:0.7407 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6819 Val Accuracy:0.7386 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 26 Loss:0.6624 Accuracy:0.7407 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6774 Val Accuracy:0.7392 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 27 Loss:0.6623 Accuracy:0.7447 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6774 Val Accuracy:0.7430 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 28 Loss:0.6621 Accuracy:0.7441 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6792 Val Accuracy:0.7424 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 29 Loss:0.6623 Accuracy:0.7441 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6791 Val Accuracy:0.7424 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 30 Loss:0.6588 Accuracy:0.7476 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6796 Val Accuracy:0.7457 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 31 Loss:0.6613 Accuracy:0.7424 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6828 Val Accuracy:0.7397 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 32 Loss:0.6602 Accuracy:0.7447 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6769 Val Accuracy:0.7430 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 33 Loss:0.6597 Accuracy:0.7493 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6802 Val Accuracy:0.7467 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 34 Loss:0.6604 Accuracy:0.7464 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6839 Val Accuracy:0.7430 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 35 Loss:0.6594 Accuracy:0.7481 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6832 Val Accuracy:0.7451 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 36 Loss:0.6592 Accuracy:0.7470 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6794 Val Accuracy:0.7451 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 37 Loss:0.6583 Accuracy:0.7539 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7516 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 38 Loss:0.6593 Accuracy:0.7487 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6794 Val Accuracy:0.7467 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 39 Loss:0.6580 Accuracy:0.7527 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6789 Val Accuracy:0.7505 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 40 Loss:0.6579 Accuracy:0.7516 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6787 Val Accuracy:0.7495 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 41 Loss:0.6569 Accuracy:0.7533 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6783 Val Accuracy:0.7511 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 42 Loss:0.6572 Accuracy:0.7544 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6792 Val Accuracy:0.7522 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 43 Loss:0.6564 Accuracy:0.7556 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7533 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 44 Loss:0.6569 Accuracy:0.7579 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7554 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 45 Loss:0.6575 Accuracy:0.7533 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6783 Val Accuracy:0.7511 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.0005\n",
      "Epoch 46 Loss:0.6563 Accuracy:0.7590 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6842 Val Accuracy:0.7554 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 47 Loss:0.6564 Accuracy:0.7533 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7511 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 48 Loss:0.6560 Accuracy:0.7573 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6785 Val Accuracy:0.7549 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 49 Loss:0.6553 Accuracy:0.7584 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6790 Val Accuracy:0.7560 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 50 Loss:0.6553 Accuracy:0.7596 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7570 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 51 Loss:0.6551 Accuracy:0.7602 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6790 Val Accuracy:0.7576 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 52 Loss:0.6555 Accuracy:0.7607 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6783 Val Accuracy:0.7581 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 53 Loss:0.6552 Accuracy:0.7602 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7576 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 54 Loss:0.6548 Accuracy:0.7613 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7587 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 55 Loss:0.6554 Accuracy:0.7607 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7581 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 56 Loss:0.6547 Accuracy:0.7613 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7587 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.00025\n",
      "Epoch 57 Loss:0.6543 Accuracy:0.7630 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7603 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 58 Loss:0.6544 Accuracy:0.7619 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7592 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 59 Loss:0.6538 Accuracy:0.7642 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7614 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 60 Loss:0.6538 Accuracy:0.7642 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7614 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 61 Loss:0.6543 Accuracy:0.7647 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6782 Val Accuracy:0.7619 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 62 Loss:0.6538 Accuracy:0.7636 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7608 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 63 Loss:0.6543 Accuracy:0.7636 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7608 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 64 Loss:0.6535 Accuracy:0.7636 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7608 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 65 Loss:0.6537 Accuracy:0.7642 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7614 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 66 Loss:0.6537 Accuracy:0.7642 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7614 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 67 Loss:0.6532 Accuracy:0.7647 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7619 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.000125\n",
      "Epoch 68 Loss:0.6533 Accuracy:0.7653 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6781 Val Accuracy:0.7625 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 69 Loss:0.6533 Accuracy:0.7653 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7625 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 70 Loss:0.6529 Accuracy:0.7665 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7636 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 71 Loss:0.6533 Accuracy:0.7665 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7636 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 72 Loss:0.6531 Accuracy:0.7659 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7630 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 73 Loss:0.6529 Accuracy:0.7659 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7630 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 74 Loss:0.6531 Accuracy:0.7665 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7636 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 75 Loss:0.6532 Accuracy:0.7659 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7630 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 76 Loss:0.6530 Accuracy:0.7653 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7625 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 77 Loss:0.6533 Accuracy:0.7659 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7630 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 78 Loss:0.6535 Accuracy:0.7659 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7630 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.25e-05\n",
      "Epoch 79 Loss:0.6527 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 80 Loss:0.6528 Accuracy:0.7665 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7636 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 81 Loss:0.6528 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 82 Loss:0.6529 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 83 Loss:0.6525 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 84 Loss:0.6531 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 85 Loss:0.6530 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 86 Loss:0.6526 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 87 Loss:0.6526 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 88 Loss:0.6524 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 89 Loss:0.6526 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.125e-05\n",
      "Epoch 90 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 91 Loss:0.6528 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 92 Loss:0.6523 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6780 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 93 Loss:0.6528 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 94 Loss:0.6522 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 95 Loss:0.6529 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 96 Loss:0.6529 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 97 Loss:0.6529 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 98 Loss:0.6527 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 99 Loss:0.6529 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 100 Loss:0.6528 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.5625e-05\n",
      "Epoch 101 Loss:0.6527 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 102 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 103 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 104 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 105 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 106 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 107 Loss:0.6525 Accuracy:0.7670 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7641 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 108 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 109 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 110 Loss:0.6528 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 111 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 7.8125e-06\n",
      "Epoch 112 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 113 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 114 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 115 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 116 Loss:0.6529 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 117 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 118 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 119 Loss:0.6529 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 120 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 121 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 122 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.90625e-06\n",
      "Epoch 123 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 124 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 125 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 126 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 127 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 128 Loss:0.6529 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 129 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 130 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 131 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 132 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 133 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.953125e-06\n",
      "Epoch 134 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 135 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 136 Loss:0.6520 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 137 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 138 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 139 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 140 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 141 Loss:0.6529 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 142 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 143 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 144 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 9.765625e-07\n",
      "Epoch 145 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 146 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 147 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 148 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 149 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 150 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 151 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 152 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 153 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 154 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 155 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 4.8828125e-07\n",
      "Epoch 156 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 157 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 158 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 159 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 160 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 161 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 162 Loss:0.6520 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 163 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 164 Loss:0.6528 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 165 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 166 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 2.44140625e-07\n",
      "Epoch 167 Loss:0.6527 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 168 Loss:0.6520 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 169 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 170 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 171 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 172 Loss:0.6530 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 173 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 174 Loss:0.6528 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 175 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 176 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 177 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.220703125e-07\n",
      "Epoch 178 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 179 Loss:0.6522 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 180 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 181 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 182 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 183 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 184 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 185 Loss:0.6518 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 186 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 187 Loss:0.6528 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 188 Loss:0.6525 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 6.103515625e-08\n",
      "Epoch 189 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 190 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 191 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 192 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 193 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 194 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 195 Loss:0.6521 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 196 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 197 Loss:0.6526 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 198 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 199 Loss:0.6523 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 3.0517578125e-08\n",
      "Epoch 200 Loss:0.6524 Accuracy:0.7676 Positive Precision:0.0000 Negative Precision:0.6513  Val Loss:0.6779 Val Accuracy:0.7646 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 1.52587890625e-08\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59  3]\n",
      " [25 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1342c0830>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a114d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "test_loss, test_acc, test_precision = model.evaluate(test_dataloader)\n",
    "\n",
    "\n",
    "features, ground_truth = next(iter(test_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3509fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65  2]\n",
      " [18 12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x11d2bb390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7c054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
