{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_TEST_SPLIT = [0.9, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"V28 V29 V30 V31 V32 V33\".split(\" \"))] ,axis= 1).values.tolist()\n",
    "        self.labels = self.df[\"Class\"].to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b052c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, TRAIN_VAL_TEST_SPLIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 2\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.BCEWithLogitsLoss()\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision, F1Score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"binary\", num_labels=num_classes)\n",
    "        self.f1 = F1Score(task=\"binary\", num_labels=num_classes, average='macro')\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.f1.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        self.train()\n",
    "        best_val_loss = 9999\n",
    "\n",
    "        train_loss_hist = []\n",
    "        train_accuracy_hist = []\n",
    "        train_f1_hist = []\n",
    "\n",
    "        val_loss_hist = []\n",
    "        val_accuracy_hist = []\n",
    "        val_f1_hist = []\n",
    "\n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.f1(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_f1 = self.f1.compute()\n",
    "\n",
    "       \n",
    "            train_accuracy_hist.append(epoch_acc.item())\n",
    "            train_loss_hist.append(epoch_loss)\n",
    "            train_f1_hist.append(epoch_f1.item())\n",
    "\n",
    "            val_loss, val_acc, val_f1 = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-by-loss.pth\")\n",
    "\n",
    "            \n",
    "            val_accuracy_hist.append(val_acc.item())\n",
    "            val_loss_hist.append(val_loss)\n",
    "            val_f1_hist.append(val_f1.item())\n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} F1:{epoch_f1:.4f}  Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val F1:{val_f1:.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        return (train_loss_hist, train_accuracy_hist,train_f1_hist), (val_loss_hist, val_accuracy_hist,val_f1_hist)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1747\n",
      "Validation dataset size: 97\n",
      "Test dataset size: 97\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size=27,num_classes=CLASSES, loss_fn=LOSS_FUNCTION, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:0.6527 Accuracy:0.7670 F1:0.4470  Val Loss:0.6595 Val Accuracy:0.7684 Val F1:0.4470 LR = 1.5625e-05\n",
      "Epoch 2 Loss:0.6527 Accuracy:0.7670 F1:0.4477  Val Loss:0.6595 Val Accuracy:0.7684 Val F1:0.4477 LR = 1.5625e-05\n",
      "Epoch 3 Loss:0.6528 Accuracy:0.7670 F1:0.4483  Val Loss:0.6594 Val Accuracy:0.7684 Val F1:0.4483 LR = 1.5625e-05\n",
      "Epoch 4 Loss:0.6525 Accuracy:0.7676 F1:0.4490  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4490 LR = 1.5625e-05\n",
      "Epoch 5 Loss:0.6527 Accuracy:0.7676 F1:0.4497  Val Loss:0.6595 Val Accuracy:0.7690 Val F1:0.4497 LR = 1.5625e-05\n",
      "Epoch 6 Loss:0.6528 Accuracy:0.7676 F1:0.4503  Val Loss:0.6595 Val Accuracy:0.7690 Val F1:0.4503 LR = 1.5625e-05\n",
      "Epoch 7 Loss:0.6527 Accuracy:0.7670 F1:0.4509  Val Loss:0.6597 Val Accuracy:0.7684 Val F1:0.4509 LR = 1.5625e-05\n",
      "Epoch 8 Loss:0.6526 Accuracy:0.7676 F1:0.4515  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4515 LR = 1.5625e-05\n",
      "Epoch 9 Loss:0.6524 Accuracy:0.7676 F1:0.4521  Val Loss:0.6595 Val Accuracy:0.7690 Val F1:0.4521 LR = 1.5625e-05\n",
      "Epoch 10 Loss:0.6525 Accuracy:0.7676 F1:0.4527  Val Loss:0.6595 Val Accuracy:0.7690 Val F1:0.4527 LR = 1.5625e-05\n",
      "Epoch 11 Loss:0.6528 Accuracy:0.7676 F1:0.4533  Val Loss:0.6595 Val Accuracy:0.7690 Val F1:0.4533 LR = 1.5625e-05\n",
      "Epoch 12 Loss:0.6523 Accuracy:0.7676 F1:0.4538  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4538 LR = 7.8125e-06\n",
      "Epoch 13 Loss:0.6527 Accuracy:0.7676 F1:0.4544  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4544 LR = 7.8125e-06\n",
      "Epoch 14 Loss:0.6526 Accuracy:0.7676 F1:0.4549  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4549 LR = 7.8125e-06\n",
      "Epoch 15 Loss:0.6523 Accuracy:0.7676 F1:0.4555  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4555 LR = 7.8125e-06\n",
      "Epoch 16 Loss:0.6523 Accuracy:0.7676 F1:0.4560  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4560 LR = 7.8125e-06\n",
      "Epoch 17 Loss:0.6524 Accuracy:0.7676 F1:0.4565  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4565 LR = 7.8125e-06\n",
      "Epoch 18 Loss:0.6521 Accuracy:0.7676 F1:0.4570  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4570 LR = 7.8125e-06\n",
      "Epoch 19 Loss:0.6524 Accuracy:0.7676 F1:0.4575  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4575 LR = 7.8125e-06\n",
      "Epoch 20 Loss:0.6524 Accuracy:0.7676 F1:0.4580  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4580 LR = 7.8125e-06\n",
      "Epoch 21 Loss:0.6522 Accuracy:0.7676 F1:0.4585  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4585 LR = 7.8125e-06\n",
      "Epoch 22 Loss:0.6524 Accuracy:0.7676 F1:0.4589  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4589 LR = 7.8125e-06\n",
      "Epoch 23 Loss:0.6521 Accuracy:0.7676 F1:0.4594  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4594 LR = 3.90625e-06\n",
      "Epoch 24 Loss:0.6522 Accuracy:0.7676 F1:0.4599  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4599 LR = 3.90625e-06\n",
      "Epoch 25 Loss:0.6523 Accuracy:0.7676 F1:0.4603  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4603 LR = 3.90625e-06\n",
      "Epoch 26 Loss:0.6526 Accuracy:0.7676 F1:0.4608  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4608 LR = 3.90625e-06\n",
      "Epoch 27 Loss:0.6518 Accuracy:0.7676 F1:0.4612  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4612 LR = 3.90625e-06\n",
      "Epoch 28 Loss:0.6527 Accuracy:0.7676 F1:0.4616  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4616 LR = 3.90625e-06\n",
      "Epoch 29 Loss:0.6523 Accuracy:0.7676 F1:0.4620  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4620 LR = 3.90625e-06\n",
      "Epoch 30 Loss:0.6527 Accuracy:0.7676 F1:0.4624  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4624 LR = 3.90625e-06\n",
      "Epoch 31 Loss:0.6525 Accuracy:0.7676 F1:0.4629  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4629 LR = 3.90625e-06\n",
      "Epoch 32 Loss:0.6526 Accuracy:0.7676 F1:0.4633  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4633 LR = 3.90625e-06\n",
      "Epoch 33 Loss:0.6524 Accuracy:0.7676 F1:0.4637  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4637 LR = 3.90625e-06\n",
      "Epoch 34 Loss:0.6525 Accuracy:0.7676 F1:0.4640  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4640 LR = 1.953125e-06\n",
      "Epoch 35 Loss:0.6525 Accuracy:0.7676 F1:0.4644  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4644 LR = 1.953125e-06\n",
      "Epoch 36 Loss:0.6526 Accuracy:0.7676 F1:0.4648  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4648 LR = 1.953125e-06\n",
      "Epoch 37 Loss:0.6525 Accuracy:0.7676 F1:0.4652  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4652 LR = 1.953125e-06\n",
      "Epoch 38 Loss:0.6526 Accuracy:0.7676 F1:0.4655  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4655 LR = 1.953125e-06\n",
      "Epoch 39 Loss:0.6525 Accuracy:0.7676 F1:0.4659  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4659 LR = 1.953125e-06\n",
      "Epoch 40 Loss:0.6524 Accuracy:0.7676 F1:0.4663  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4663 LR = 1.953125e-06\n",
      "Epoch 41 Loss:0.6524 Accuracy:0.7676 F1:0.4666  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4666 LR = 1.953125e-06\n",
      "Epoch 42 Loss:0.6529 Accuracy:0.7676 F1:0.4670  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4670 LR = 1.953125e-06\n",
      "Epoch 43 Loss:0.6522 Accuracy:0.7676 F1:0.4673  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4673 LR = 1.953125e-06\n",
      "Epoch 44 Loss:0.6523 Accuracy:0.7676 F1:0.4676  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4676 LR = 1.953125e-06\n",
      "Epoch 45 Loss:0.6521 Accuracy:0.7676 F1:0.4680  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4680 LR = 9.765625e-07\n",
      "Epoch 46 Loss:0.6521 Accuracy:0.7676 F1:0.4683  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4683 LR = 9.765625e-07\n",
      "Epoch 47 Loss:0.6525 Accuracy:0.7676 F1:0.4686  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4686 LR = 9.765625e-07\n",
      "Epoch 48 Loss:0.6527 Accuracy:0.7676 F1:0.4689  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4689 LR = 9.765625e-07\n",
      "Epoch 49 Loss:0.6524 Accuracy:0.7676 F1:0.4692  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4692 LR = 9.765625e-07\n",
      "Epoch 50 Loss:0.6526 Accuracy:0.7676 F1:0.4695  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4695 LR = 9.765625e-07\n",
      "Epoch 51 Loss:0.6523 Accuracy:0.7676 F1:0.4699  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4699 LR = 9.765625e-07\n",
      "Epoch 52 Loss:0.6524 Accuracy:0.7676 F1:0.4702  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4702 LR = 9.765625e-07\n",
      "Epoch 53 Loss:0.6524 Accuracy:0.7676 F1:0.4704  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4704 LR = 9.765625e-07\n",
      "Epoch 54 Loss:0.6525 Accuracy:0.7676 F1:0.4707  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4707 LR = 9.765625e-07\n",
      "Epoch 55 Loss:0.6525 Accuracy:0.7676 F1:0.4710  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4710 LR = 9.765625e-07\n",
      "Epoch 56 Loss:0.6521 Accuracy:0.7676 F1:0.4713  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4713 LR = 4.8828125e-07\n",
      "Epoch 57 Loss:0.6525 Accuracy:0.7676 F1:0.4716  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4716 LR = 4.8828125e-07\n",
      "Epoch 58 Loss:0.6524 Accuracy:0.7676 F1:0.4719  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4719 LR = 4.8828125e-07\n",
      "Epoch 59 Loss:0.6524 Accuracy:0.7676 F1:0.4721  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4721 LR = 4.8828125e-07\n",
      "Epoch 60 Loss:0.6528 Accuracy:0.7676 F1:0.4724  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4724 LR = 4.8828125e-07\n",
      "Epoch 61 Loss:0.6525 Accuracy:0.7676 F1:0.4727  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4727 LR = 4.8828125e-07\n",
      "Epoch 62 Loss:0.6525 Accuracy:0.7676 F1:0.4729  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4729 LR = 4.8828125e-07\n",
      "Epoch 63 Loss:0.6525 Accuracy:0.7676 F1:0.4732  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4732 LR = 4.8828125e-07\n",
      "Epoch 64 Loss:0.6523 Accuracy:0.7676 F1:0.4735  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4735 LR = 4.8828125e-07\n",
      "Epoch 65 Loss:0.6524 Accuracy:0.7676 F1:0.4737  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4737 LR = 4.8828125e-07\n",
      "Epoch 66 Loss:0.6524 Accuracy:0.7676 F1:0.4740  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4740 LR = 4.8828125e-07\n",
      "Epoch 67 Loss:0.6528 Accuracy:0.7676 F1:0.4742  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4742 LR = 2.44140625e-07\n",
      "Epoch 68 Loss:0.6527 Accuracy:0.7676 F1:0.4745  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4745 LR = 2.44140625e-07\n",
      "Epoch 69 Loss:0.6524 Accuracy:0.7676 F1:0.4747  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4747 LR = 2.44140625e-07\n",
      "Epoch 70 Loss:0.6523 Accuracy:0.7676 F1:0.4749  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4749 LR = 2.44140625e-07\n",
      "Epoch 71 Loss:0.6525 Accuracy:0.7676 F1:0.4752  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4752 LR = 2.44140625e-07\n",
      "Epoch 72 Loss:0.6527 Accuracy:0.7676 F1:0.4754  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4754 LR = 2.44140625e-07\n",
      "Epoch 73 Loss:0.6524 Accuracy:0.7676 F1:0.4756  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4756 LR = 2.44140625e-07\n",
      "Epoch 74 Loss:0.6529 Accuracy:0.7676 F1:0.4759  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4759 LR = 2.44140625e-07\n",
      "Epoch 75 Loss:0.6521 Accuracy:0.7676 F1:0.4761  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4761 LR = 2.44140625e-07\n",
      "Epoch 76 Loss:0.6524 Accuracy:0.7676 F1:0.4763  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4763 LR = 2.44140625e-07\n",
      "Epoch 77 Loss:0.6524 Accuracy:0.7676 F1:0.4765  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4765 LR = 2.44140625e-07\n",
      "Epoch 78 Loss:0.6521 Accuracy:0.7676 F1:0.4767  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4767 LR = 1.220703125e-07\n",
      "Epoch 79 Loss:0.6524 Accuracy:0.7676 F1:0.4770  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4770 LR = 1.220703125e-07\n",
      "Epoch 80 Loss:0.6524 Accuracy:0.7676 F1:0.4772  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4772 LR = 1.220703125e-07\n",
      "Epoch 81 Loss:0.6524 Accuracy:0.7676 F1:0.4774  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4774 LR = 1.220703125e-07\n",
      "Epoch 82 Loss:0.6519 Accuracy:0.7676 F1:0.4776  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4776 LR = 1.220703125e-07\n",
      "Epoch 83 Loss:0.6521 Accuracy:0.7676 F1:0.4778  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4778 LR = 1.220703125e-07\n",
      "Epoch 84 Loss:0.6523 Accuracy:0.7676 F1:0.4780  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4780 LR = 1.220703125e-07\n",
      "Epoch 85 Loss:0.6525 Accuracy:0.7676 F1:0.4782  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4782 LR = 1.220703125e-07\n",
      "Epoch 86 Loss:0.6524 Accuracy:0.7676 F1:0.4784  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4784 LR = 1.220703125e-07\n",
      "Epoch 87 Loss:0.6520 Accuracy:0.7676 F1:0.4786  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4786 LR = 1.220703125e-07\n",
      "Epoch 88 Loss:0.6526 Accuracy:0.7676 F1:0.4788  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4788 LR = 1.220703125e-07\n",
      "Epoch 89 Loss:0.6525 Accuracy:0.7676 F1:0.4790  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4790 LR = 6.103515625e-08\n",
      "Epoch 90 Loss:0.6525 Accuracy:0.7676 F1:0.4792  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4792 LR = 6.103515625e-08\n",
      "Epoch 91 Loss:0.6523 Accuracy:0.7676 F1:0.4794  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4794 LR = 6.103515625e-08\n",
      "Epoch 92 Loss:0.6522 Accuracy:0.7676 F1:0.4795  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4795 LR = 6.103515625e-08\n",
      "Epoch 93 Loss:0.6525 Accuracy:0.7676 F1:0.4797  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4797 LR = 6.103515625e-08\n",
      "Epoch 94 Loss:0.6525 Accuracy:0.7676 F1:0.4799  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4799 LR = 6.103515625e-08\n",
      "Epoch 95 Loss:0.6524 Accuracy:0.7676 F1:0.4801  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4801 LR = 6.103515625e-08\n",
      "Epoch 96 Loss:0.6528 Accuracy:0.7676 F1:0.4803  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4803 LR = 6.103515625e-08\n",
      "Epoch 97 Loss:0.6521 Accuracy:0.7676 F1:0.4804  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4804 LR = 6.103515625e-08\n",
      "Epoch 98 Loss:0.6526 Accuracy:0.7676 F1:0.4806  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4806 LR = 6.103515625e-08\n",
      "Epoch 99 Loss:0.6521 Accuracy:0.7676 F1:0.4808  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4808 LR = 6.103515625e-08\n",
      "Epoch 100 Loss:0.6522 Accuracy:0.7676 F1:0.4810  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4810 LR = 3.0517578125e-08\n",
      "Epoch 101 Loss:0.6521 Accuracy:0.7676 F1:0.4811  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4811 LR = 3.0517578125e-08\n",
      "Epoch 102 Loss:0.6524 Accuracy:0.7676 F1:0.4813  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4813 LR = 3.0517578125e-08\n",
      "Epoch 103 Loss:0.6525 Accuracy:0.7676 F1:0.4815  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4815 LR = 3.0517578125e-08\n",
      "Epoch 104 Loss:0.6527 Accuracy:0.7676 F1:0.4816  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4816 LR = 3.0517578125e-08\n",
      "Epoch 105 Loss:0.6525 Accuracy:0.7676 F1:0.4818  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4818 LR = 3.0517578125e-08\n",
      "Epoch 106 Loss:0.6524 Accuracy:0.7676 F1:0.4820  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4820 LR = 3.0517578125e-08\n",
      "Epoch 107 Loss:0.6525 Accuracy:0.7676 F1:0.4821  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4821 LR = 3.0517578125e-08\n",
      "Epoch 108 Loss:0.6516 Accuracy:0.7676 F1:0.4823  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4823 LR = 3.0517578125e-08\n",
      "Epoch 109 Loss:0.6524 Accuracy:0.7676 F1:0.4824  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4824 LR = 3.0517578125e-08\n",
      "Epoch 110 Loss:0.6521 Accuracy:0.7676 F1:0.4826  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4826 LR = 3.0517578125e-08\n",
      "Epoch 111 Loss:0.6522 Accuracy:0.7676 F1:0.4827  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4827 LR = 1.52587890625e-08\n",
      "Epoch 112 Loss:0.6524 Accuracy:0.7676 F1:0.4829  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4829 LR = 1.52587890625e-08\n",
      "Epoch 113 Loss:0.6524 Accuracy:0.7676 F1:0.4830  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4830 LR = 1.52587890625e-08\n",
      "Epoch 114 Loss:0.6528 Accuracy:0.7676 F1:0.4832  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4832 LR = 1.52587890625e-08\n",
      "Epoch 115 Loss:0.6525 Accuracy:0.7676 F1:0.4833  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4833 LR = 1.52587890625e-08\n",
      "Epoch 116 Loss:0.6521 Accuracy:0.7676 F1:0.4835  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4835 LR = 1.52587890625e-08\n",
      "Epoch 117 Loss:0.6522 Accuracy:0.7676 F1:0.4836  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4836 LR = 1.52587890625e-08\n",
      "Epoch 118 Loss:0.6521 Accuracy:0.7676 F1:0.4838  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4838 LR = 1.52587890625e-08\n",
      "Epoch 119 Loss:0.6526 Accuracy:0.7676 F1:0.4839  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4839 LR = 1.52587890625e-08\n",
      "Epoch 120 Loss:0.6528 Accuracy:0.7676 F1:0.4841  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4841 LR = 1.52587890625e-08\n",
      "Epoch 121 Loss:0.6524 Accuracy:0.7676 F1:0.4842  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4842 LR = 1.52587890625e-08\n",
      "Epoch 122 Loss:0.6525 Accuracy:0.7676 F1:0.4843  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4843 LR = 1.52587890625e-08\n",
      "Epoch 123 Loss:0.6522 Accuracy:0.7676 F1:0.4845  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4845 LR = 1.52587890625e-08\n",
      "Epoch 124 Loss:0.6519 Accuracy:0.7676 F1:0.4846  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4846 LR = 1.52587890625e-08\n",
      "Epoch 125 Loss:0.6524 Accuracy:0.7676 F1:0.4847  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4847 LR = 1.52587890625e-08\n",
      "Epoch 126 Loss:0.6521 Accuracy:0.7676 F1:0.4849  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4849 LR = 1.52587890625e-08\n",
      "Epoch 127 Loss:0.6525 Accuracy:0.7676 F1:0.4850  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4850 LR = 1.52587890625e-08\n",
      "Epoch 128 Loss:0.6519 Accuracy:0.7676 F1:0.4851  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4851 LR = 1.52587890625e-08\n",
      "Epoch 129 Loss:0.6522 Accuracy:0.7676 F1:0.4853  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4853 LR = 1.52587890625e-08\n",
      "Epoch 130 Loss:0.6521 Accuracy:0.7676 F1:0.4854  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4854 LR = 1.52587890625e-08\n",
      "Epoch 131 Loss:0.6528 Accuracy:0.7676 F1:0.4855  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4855 LR = 1.52587890625e-08\n",
      "Epoch 132 Loss:0.6523 Accuracy:0.7676 F1:0.4856  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4856 LR = 1.52587890625e-08\n",
      "Epoch 133 Loss:0.6522 Accuracy:0.7676 F1:0.4858  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4858 LR = 1.52587890625e-08\n",
      "Epoch 134 Loss:0.6522 Accuracy:0.7676 F1:0.4859  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4859 LR = 1.52587890625e-08\n",
      "Epoch 135 Loss:0.6524 Accuracy:0.7676 F1:0.4860  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4860 LR = 1.52587890625e-08\n",
      "Epoch 136 Loss:0.6527 Accuracy:0.7676 F1:0.4861  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4861 LR = 1.52587890625e-08\n",
      "Epoch 137 Loss:0.6524 Accuracy:0.7676 F1:0.4863  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4863 LR = 1.52587890625e-08\n",
      "Epoch 138 Loss:0.6524 Accuracy:0.7676 F1:0.4864  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4864 LR = 1.52587890625e-08\n",
      "Epoch 139 Loss:0.6523 Accuracy:0.7676 F1:0.4865  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4865 LR = 1.52587890625e-08\n",
      "Epoch 140 Loss:0.6531 Accuracy:0.7676 F1:0.4866  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4866 LR = 1.52587890625e-08\n",
      "Epoch 141 Loss:0.6524 Accuracy:0.7676 F1:0.4867  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4867 LR = 1.52587890625e-08\n",
      "Epoch 142 Loss:0.6521 Accuracy:0.7676 F1:0.4868  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4868 LR = 1.52587890625e-08\n",
      "Epoch 143 Loss:0.6522 Accuracy:0.7676 F1:0.4870  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4870 LR = 1.52587890625e-08\n",
      "Epoch 144 Loss:0.6525 Accuracy:0.7676 F1:0.4871  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4871 LR = 1.52587890625e-08\n",
      "Epoch 145 Loss:0.6522 Accuracy:0.7676 F1:0.4872  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4872 LR = 1.52587890625e-08\n",
      "Epoch 146 Loss:0.6524 Accuracy:0.7676 F1:0.4873  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4873 LR = 1.52587890625e-08\n",
      "Epoch 147 Loss:0.6521 Accuracy:0.7676 F1:0.4874  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4874 LR = 1.52587890625e-08\n",
      "Epoch 148 Loss:0.6527 Accuracy:0.7676 F1:0.4875  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4875 LR = 1.52587890625e-08\n",
      "Epoch 149 Loss:0.6525 Accuracy:0.7676 F1:0.4876  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4876 LR = 1.52587890625e-08\n",
      "Epoch 150 Loss:0.6521 Accuracy:0.7676 F1:0.4877  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4877 LR = 1.52587890625e-08\n",
      "Epoch 151 Loss:0.6526 Accuracy:0.7676 F1:0.4878  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4878 LR = 1.52587890625e-08\n",
      "Epoch 152 Loss:0.6521 Accuracy:0.7676 F1:0.4880  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4880 LR = 1.52587890625e-08\n",
      "Epoch 153 Loss:0.6521 Accuracy:0.7676 F1:0.4881  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4881 LR = 1.52587890625e-08\n",
      "Epoch 154 Loss:0.6526 Accuracy:0.7676 F1:0.4882  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4882 LR = 1.52587890625e-08\n",
      "Epoch 155 Loss:0.6524 Accuracy:0.7676 F1:0.4883  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4883 LR = 1.52587890625e-08\n",
      "Epoch 156 Loss:0.6522 Accuracy:0.7676 F1:0.4884  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4884 LR = 1.52587890625e-08\n",
      "Epoch 157 Loss:0.6522 Accuracy:0.7676 F1:0.4885  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4885 LR = 1.52587890625e-08\n",
      "Epoch 158 Loss:0.6523 Accuracy:0.7676 F1:0.4886  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4886 LR = 1.52587890625e-08\n",
      "Epoch 159 Loss:0.6525 Accuracy:0.7676 F1:0.4887  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4887 LR = 1.52587890625e-08\n",
      "Epoch 160 Loss:0.6527 Accuracy:0.7676 F1:0.4888  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4888 LR = 1.52587890625e-08\n",
      "Epoch 161 Loss:0.6526 Accuracy:0.7676 F1:0.4889  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4889 LR = 1.52587890625e-08\n",
      "Epoch 162 Loss:0.6526 Accuracy:0.7676 F1:0.4890  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4890 LR = 1.52587890625e-08\n",
      "Epoch 163 Loss:0.6527 Accuracy:0.7676 F1:0.4891  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4891 LR = 1.52587890625e-08\n",
      "Epoch 164 Loss:0.6524 Accuracy:0.7676 F1:0.4892  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4892 LR = 1.52587890625e-08\n",
      "Epoch 165 Loss:0.6525 Accuracy:0.7676 F1:0.4893  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4893 LR = 1.52587890625e-08\n",
      "Epoch 166 Loss:0.6526 Accuracy:0.7676 F1:0.4894  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4894 LR = 1.52587890625e-08\n",
      "Epoch 167 Loss:0.6522 Accuracy:0.7676 F1:0.4895  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4895 LR = 1.52587890625e-08\n",
      "Epoch 168 Loss:0.6523 Accuracy:0.7676 F1:0.4896  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4896 LR = 1.52587890625e-08\n",
      "Epoch 169 Loss:0.6526 Accuracy:0.7676 F1:0.4896  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4896 LR = 1.52587890625e-08\n",
      "Epoch 170 Loss:0.6524 Accuracy:0.7676 F1:0.4897  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4897 LR = 1.52587890625e-08\n",
      "Epoch 171 Loss:0.6525 Accuracy:0.7676 F1:0.4898  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4898 LR = 1.52587890625e-08\n",
      "Epoch 172 Loss:0.6524 Accuracy:0.7676 F1:0.4899  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4899 LR = 1.52587890625e-08\n",
      "Epoch 173 Loss:0.6524 Accuracy:0.7676 F1:0.4900  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4900 LR = 1.52587890625e-08\n",
      "Epoch 174 Loss:0.6526 Accuracy:0.7676 F1:0.4901  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4901 LR = 1.52587890625e-08\n",
      "Epoch 175 Loss:0.6529 Accuracy:0.7676 F1:0.4902  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4902 LR = 1.52587890625e-08\n",
      "Epoch 176 Loss:0.6520 Accuracy:0.7676 F1:0.4903  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4903 LR = 1.52587890625e-08\n",
      "Epoch 177 Loss:0.6522 Accuracy:0.7676 F1:0.4904  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4904 LR = 1.52587890625e-08\n",
      "Epoch 178 Loss:0.6521 Accuracy:0.7676 F1:0.4905  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4905 LR = 1.52587890625e-08\n",
      "Epoch 179 Loss:0.6522 Accuracy:0.7676 F1:0.4905  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4905 LR = 1.52587890625e-08\n",
      "Epoch 180 Loss:0.6524 Accuracy:0.7676 F1:0.4906  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4906 LR = 1.52587890625e-08\n",
      "Epoch 181 Loss:0.6526 Accuracy:0.7676 F1:0.4907  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4907 LR = 1.52587890625e-08\n",
      "Epoch 182 Loss:0.6522 Accuracy:0.7676 F1:0.4908  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4908 LR = 1.52587890625e-08\n",
      "Epoch 183 Loss:0.6525 Accuracy:0.7676 F1:0.4909  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4909 LR = 1.52587890625e-08\n",
      "Epoch 184 Loss:0.6524 Accuracy:0.7676 F1:0.4910  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4910 LR = 1.52587890625e-08\n",
      "Epoch 185 Loss:0.6523 Accuracy:0.7676 F1:0.4911  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4911 LR = 1.52587890625e-08\n",
      "Epoch 186 Loss:0.6527 Accuracy:0.7676 F1:0.4911  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4911 LR = 1.52587890625e-08\n",
      "Epoch 187 Loss:0.6525 Accuracy:0.7676 F1:0.4912  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4912 LR = 1.52587890625e-08\n",
      "Epoch 188 Loss:0.6524 Accuracy:0.7676 F1:0.4913  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4913 LR = 1.52587890625e-08\n",
      "Epoch 189 Loss:0.6520 Accuracy:0.7676 F1:0.4914  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4914 LR = 1.52587890625e-08\n",
      "Epoch 190 Loss:0.6525 Accuracy:0.7676 F1:0.4915  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4915 LR = 1.52587890625e-08\n",
      "Epoch 191 Loss:0.6524 Accuracy:0.7676 F1:0.4915  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4915 LR = 1.52587890625e-08\n",
      "Epoch 192 Loss:0.6524 Accuracy:0.7676 F1:0.4916  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4916 LR = 1.52587890625e-08\n",
      "Epoch 193 Loss:0.6526 Accuracy:0.7676 F1:0.4917  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4917 LR = 1.52587890625e-08\n",
      "Epoch 194 Loss:0.6524 Accuracy:0.7676 F1:0.4918  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4918 LR = 1.52587890625e-08\n",
      "Epoch 195 Loss:0.6522 Accuracy:0.7676 F1:0.4919  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4919 LR = 1.52587890625e-08\n",
      "Epoch 196 Loss:0.6524 Accuracy:0.7676 F1:0.4919  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4919 LR = 1.52587890625e-08\n",
      "Epoch 197 Loss:0.6526 Accuracy:0.7676 F1:0.4920  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4920 LR = 1.52587890625e-08\n",
      "Epoch 198 Loss:0.6524 Accuracy:0.7676 F1:0.4921  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4921 LR = 1.52587890625e-08\n",
      "Epoch 199 Loss:0.6519 Accuracy:0.7676 F1:0.4922  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4922 LR = 1.52587890625e-08\n",
      "Epoch 200 Loss:0.6525 Accuracy:0.7676 F1:0.4922  Val Loss:0.6596 Val Accuracy:0.7690 Val F1:0.4922 LR = 1.52587890625e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([0.6526880665258927,\n",
       "   0.6526857365261425,\n",
       "   0.6528409080071883,\n",
       "   0.6525335051796654,\n",
       "   0.652682060545141,\n",
       "   0.6528290856968273,\n",
       "   0.6526794986291365,\n",
       "   0.6526307333599437,\n",
       "   0.6523572488264604,\n",
       "   0.6524934053421021,\n",
       "   0.6527780012650923,\n",
       "   0.6523225524208762,\n",
       "   0.6527492187239907,\n",
       "   0.6525607445023277,\n",
       "   0.6523486343297091,\n",
       "   0.6523002418604764,\n",
       "   0.6524471467191523,\n",
       "   0.6521468986164439,\n",
       "   0.6524292035536332,\n",
       "   0.6523643970489502,\n",
       "   0.6521602424708279,\n",
       "   0.6523795864798806,\n",
       "   0.6521300077438354,\n",
       "   0.6521751262924887,\n",
       "   0.6522661555897106,\n",
       "   0.6526487599719655,\n",
       "   0.6518420761281793,\n",
       "   0.6527005390687423,\n",
       "   0.6522832371971824,\n",
       "   0.6527056650681929,\n",
       "   0.6525497501546687,\n",
       "   0.652635675126856,\n",
       "   0.6524134635925293,\n",
       "   0.6525402426719665,\n",
       "   0.6524842641570351,\n",
       "   0.6525961258194664,\n",
       "   0.6525368224490773,\n",
       "   0.652631722797047,\n",
       "   0.6525389790534973,\n",
       "   0.652386364069852,\n",
       "   0.6523830110376532,\n",
       "   0.6528515284711665,\n",
       "   0.6522357680580833,\n",
       "   0.6523263356902382,\n",
       "   0.652086463841525,\n",
       "   0.6520955714312466,\n",
       "   0.6525323271751404,\n",
       "   0.6526719223369252,\n",
       "   0.6523969932035967,\n",
       "   0.6525817524303089,\n",
       "   0.6523222825743935,\n",
       "   0.6523759679360823,\n",
       "   0.6523757024244828,\n",
       "   0.6525223959576,\n",
       "   0.652521422776309,\n",
       "   0.6520783987912264,\n",
       "   0.6525098713961515,\n",
       "   0.6523792613636363,\n",
       "   0.6523718714714051,\n",
       "   0.6527676918289879,\n",
       "   0.6524632204662669,\n",
       "   0.6524769793857228,\n",
       "   0.6525280616500161,\n",
       "   0.6523220831697637,\n",
       "   0.6523720221085982,\n",
       "   0.6523534124547785,\n",
       "   0.6527763713489879,\n",
       "   0.6526653235608881,\n",
       "   0.6523696953600103,\n",
       "   0.6523332270708951,\n",
       "   0.6525170727209612,\n",
       "   0.6526750141924078,\n",
       "   0.6523693084716797,\n",
       "   0.6528500491922552,\n",
       "   0.6520777052099055,\n",
       "   0.6523871172558178,\n",
       "   0.6523673176765442,\n",
       "   0.6520740498195995,\n",
       "   0.6523815653540871,\n",
       "   0.6523698481646452,\n",
       "   0.6523680426857689,\n",
       "   0.6519261945377697,\n",
       "   0.6520858818834478,\n",
       "   0.652293860912323,\n",
       "   0.6525179776278409,\n",
       "   0.6524457801472057,\n",
       "   0.6519661925055764,\n",
       "   0.6525948708707636,\n",
       "   0.6525153658606789,\n",
       "   0.6525156931443648,\n",
       "   0.6523126309568231,\n",
       "   0.6522307656028054,\n",
       "   0.6525190245021474,\n",
       "   0.6525151350281455,\n",
       "   0.6523688478903337,\n",
       "   0.6527560754255815,\n",
       "   0.652078716321425,\n",
       "   0.6525828979232094,\n",
       "   0.6520878477530045,\n",
       "   0.6522225466641512,\n",
       "   0.652073487368497,\n",
       "   0.6523676254532554,\n",
       "   0.6525147481398149,\n",
       "   0.652663960240104,\n",
       "   0.6525147849863225,\n",
       "   0.6523751399733804,\n",
       "   0.6525163108652289,\n",
       "   0.651643122326244,\n",
       "   0.6524085597558456,\n",
       "   0.6520975188775496,\n",
       "   0.6522367835044861,\n",
       "   0.6523788647218184,\n",
       "   0.6523670359091325,\n",
       "   0.6527561296116222,\n",
       "   0.652533850886605,\n",
       "   0.6520715756849809,\n",
       "   0.652219726822593,\n",
       "   0.6520904714410956,\n",
       "   0.6526059963486411,\n",
       "   0.6527550979094072,\n",
       "   0.6523681217973882,\n",
       "   0.652514560656114,\n",
       "   0.6522211042317477,\n",
       "   0.6519240541891618,\n",
       "   0.6523679505694996,\n",
       "   0.6520863272927024,\n",
       "   0.6524710384282199,\n",
       "   0.6519264264540239,\n",
       "   0.6522219560363076,\n",
       "   0.6520873925902627,\n",
       "   0.6527551195838235,\n",
       "   0.6522947138006037,\n",
       "   0.65224074233662,\n",
       "   0.652235413681377,\n",
       "   0.6524446140636098,\n",
       "   0.6526707118207759,\n",
       "   0.6523675203323365,\n",
       "   0.6524117599834095,\n",
       "   0.6523253343322061,\n",
       "   0.6531291896646673,\n",
       "   0.6523671301928433,\n",
       "   0.652072315866297,\n",
       "   0.6522398081692782,\n",
       "   0.6525145238096064,\n",
       "   0.6522230245850303,\n",
       "   0.6523686994205822,\n",
       "   0.6520880980925127,\n",
       "   0.6526621287519282,\n",
       "   0.6525145519863476,\n",
       "   0.652093150398948,\n",
       "   0.6525940472429449,\n",
       "   0.6521161848848517,\n",
       "   0.6520785613493486,\n",
       "   0.6525969234379855,\n",
       "   0.6523667899045077,\n",
       "   0.6522303917191246,\n",
       "   0.6522401918064464,\n",
       "   0.6522883881222118,\n",
       "   0.6524538950486617,\n",
       "   0.6526620821519331,\n",
       "   0.6526202115145596,\n",
       "   0.6525993824005127,\n",
       "   0.6526620810682123,\n",
       "   0.6523692629554055,\n",
       "   0.6525045795874163,\n",
       "   0.6526117205619812,\n",
       "   0.6522229205478322,\n",
       "   0.6522823897275057,\n",
       "   0.6526259454813871,\n",
       "   0.652366945960305,\n",
       "   0.6525334629145536,\n",
       "   0.6523672949184071,\n",
       "   0.6524005207148466,\n",
       "   0.6526079167019238,\n",
       "   0.6529028794982217,\n",
       "   0.651973848993128,\n",
       "   0.652229292826219,\n",
       "   0.6520724101500077,\n",
       "   0.6522426583550193,\n",
       "   0.652378820289265,\n",
       "   0.6526157476685264,\n",
       "   0.6522466215220365,\n",
       "   0.6525144479491494,\n",
       "   0.6523648370396007,\n",
       "   0.6522764433513988,\n",
       "   0.6526615587147799,\n",
       "   0.652525223385204,\n",
       "   0.6523673046718944,\n",
       "   0.6519655032591386,\n",
       "   0.6524605415084145,\n",
       "   0.6523664225231517,\n",
       "   0.6523735111409967,\n",
       "   0.6526077779856595,\n",
       "   0.6523812933401628,\n",
       "   0.65224600055001,\n",
       "   0.6523666381835938,\n",
       "   0.6526072231206027,\n",
       "   0.652366858178919,\n",
       "   0.6519408345222473,\n",
       "   0.6525144782933322],\n",
       "  [0.7670291662216187,\n",
       "   0.7670291662216187,\n",
       "   0.7670291662216187,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7670291662216187,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415,\n",
       "   0.7676016092300415],\n",
       "  [0.44702988862991333,\n",
       "   0.44769614934921265,\n",
       "   0.4483489990234375,\n",
       "   0.4490073621273041,\n",
       "   0.44965270161628723,\n",
       "   0.4502854347229004,\n",
       "   0.4508879482746124,\n",
       "   0.4514966905117035,\n",
       "   0.45209386944770813,\n",
       "   0.4526798129081726,\n",
       "   0.4532548189163208,\n",
       "   0.453819215297699,\n",
       "   0.4543732702732086,\n",
       "   0.4549172818660736,\n",
       "   0.45545151829719543,\n",
       "   0.4559762179851532,\n",
       "   0.4564916491508484,\n",
       "   0.4569980800151825,\n",
       "   0.45749571919441223,\n",
       "   0.4579848051071167,\n",
       "   0.4584655165672302,\n",
       "   0.4589381217956543,\n",
       "   0.45940279960632324,\n",
       "   0.4598597586154938,\n",
       "   0.46030914783477783,\n",
       "   0.4607512056827545,\n",
       "   0.46118611097335815,\n",
       "   0.4616139829158783,\n",
       "   0.46203505992889404,\n",
       "   0.46244943141937256,\n",
       "   0.46285730600357056,\n",
       "   0.4632588028907776,\n",
       "   0.463654100894928,\n",
       "   0.46404334902763367,\n",
       "   0.4644266366958618,\n",
       "   0.46480414271354675,\n",
       "   0.465175986289978,\n",
       "   0.4655422866344452,\n",
       "   0.4659031629562378,\n",
       "   0.4662587344646454,\n",
       "   0.4666091501712799,\n",
       "   0.46695446968078613,\n",
       "   0.4672948718070984,\n",
       "   0.46763038635253906,\n",
       "   0.4679611623287201,\n",
       "   0.46828725934028625,\n",
       "   0.4686088263988495,\n",
       "   0.46892595291137695,\n",
       "   0.4692386984825134,\n",
       "   0.4695471525192261,\n",
       "   0.46985143423080444,\n",
       "   0.4701516032218933,\n",
       "   0.47044774889945984,\n",
       "   0.4707399606704712,\n",
       "   0.47102829813957214,\n",
       "   0.47131285071372986,\n",
       "   0.4715937077999115,\n",
       "   0.47187089920043945,\n",
       "   0.47214454412460327,\n",
       "   0.47241467237472534,\n",
       "   0.47268134355545044,\n",
       "   0.4729446768760681,\n",
       "   0.47320470213890076,\n",
       "   0.47346147894859314,\n",
       "   0.47371506690979004,\n",
       "   0.47396552562713623,\n",
       "   0.4742129147052765,\n",
       "   0.4744572937488556,\n",
       "   0.4746987223625183,\n",
       "   0.47493723034858704,\n",
       "   0.47517290711402893,\n",
       "   0.475405752658844,\n",
       "   0.4756358563899994,\n",
       "   0.4758632779121399,\n",
       "   0.4760880470275879,\n",
       "   0.47631019353866577,\n",
       "   0.4765297472476959,\n",
       "   0.4767468273639679,\n",
       "   0.4769614040851593,\n",
       "   0.4771735370159149,\n",
       "   0.4773833155632019,\n",
       "   0.4775907099246979,\n",
       "   0.47779580950737,\n",
       "   0.47799861431121826,\n",
       "   0.47819918394088745,\n",
       "   0.47839754819869995,\n",
       "   0.47859376668930054,\n",
       "   0.4787878394126892,\n",
       "   0.47897982597351074,\n",
       "   0.4791697561740875,\n",
       "   0.47935763001441956,\n",
       "   0.479543536901474,\n",
       "   0.47972747683525085,\n",
       "   0.4799094796180725,\n",
       "   0.48008957505226135,\n",
       "   0.48026779294013977,\n",
       "   0.48044416308403015,\n",
       "   0.4806187152862549,\n",
       "   0.48079150915145874,\n",
       "   0.48096251487731934,\n",
       "   0.48113179206848145,\n",
       "   0.48129937052726746,\n",
       "   0.48146525025367737,\n",
       "   0.48162949085235596,\n",
       "   0.4817920923233032,\n",
       "   0.48195308446884155,\n",
       "   0.48211249709129333,\n",
       "   0.48227033019065857,\n",
       "   0.48242664337158203,\n",
       "   0.4825814366340637,\n",
       "   0.482734739780426,\n",
       "   0.48288655281066895,\n",
       "   0.48303693532943726,\n",
       "   0.48318588733673096,\n",
       "   0.48333343863487244,\n",
       "   0.4834795594215393,\n",
       "   0.48362433910369873,\n",
       "   0.4837677776813507,\n",
       "   0.48390987515449524,\n",
       "   0.4840506315231323,\n",
       "   0.4841901361942291,\n",
       "   0.48432832956314087,\n",
       "   0.48446527123451233,\n",
       "   0.4846009612083435,\n",
       "   0.4847354590892792,\n",
       "   0.48486870527267456,\n",
       "   0.4850007891654968,\n",
       "   0.4851316809654236,\n",
       "   0.4852614104747772,\n",
       "   0.48538997769355774,\n",
       "   0.4855174422264099,\n",
       "   0.48564377427101135,\n",
       "   0.48576900362968445,\n",
       "   0.4858931303024292,\n",
       "   0.4860162138938904,\n",
       "   0.4861382246017456,\n",
       "   0.48625919222831726,\n",
       "   0.48637911677360535,\n",
       "   0.48649802803993225,\n",
       "   0.486615926027298,\n",
       "   0.4867328405380249,\n",
       "   0.48684877157211304,\n",
       "   0.48696374893188477,\n",
       "   0.4870777428150177,\n",
       "   0.4871908128261566,\n",
       "   0.4873029291629791,\n",
       "   0.4874141216278076,\n",
       "   0.4875244200229645,\n",
       "   0.4876338243484497,\n",
       "   0.4877423346042633,\n",
       "   0.4878499507904053,\n",
       "   0.487956702709198,\n",
       "   0.48806262016296387,\n",
       "   0.4881676733493805,\n",
       "   0.48827189207077026,\n",
       "   0.4883752763271332,\n",
       "   0.4884778559207916,\n",
       "   0.4885796308517456,\n",
       "   0.4886806011199951,\n",
       "   0.48878079652786255,\n",
       "   0.4888802170753479,\n",
       "   0.4889788329601288,\n",
       "   0.48907673358917236,\n",
       "   0.48917385935783386,\n",
       "   0.48927024006843567,\n",
       "   0.4893658757209778,\n",
       "   0.4894607961177826,\n",
       "   0.4895550012588501,\n",
       "   0.4896484911441803,\n",
       "   0.4897412657737732,\n",
       "   0.48983335494995117,\n",
       "   0.4899247884750366,\n",
       "   0.49001550674438477,\n",
       "   0.4901055693626404,\n",
       "   0.4901949465274811,\n",
       "   0.49028369784355164,\n",
       "   0.4903717637062073,\n",
       "   0.49045923352241516,\n",
       "   0.49054601788520813,\n",
       "   0.49063220620155334,\n",
       "   0.4907177686691284,\n",
       "   0.49080270528793335,\n",
       "   0.4908870458602905,\n",
       "   0.49097079038619995,\n",
       "   0.49105390906333923,\n",
       "   0.49113646149635315,\n",
       "   0.4912184476852417,\n",
       "   0.4912998378276825,\n",
       "   0.4913806617259979,\n",
       "   0.4914608895778656,\n",
       "   0.4915406107902527,\n",
       "   0.491619735956192,\n",
       "   0.49169832468032837,\n",
       "   0.49177637696266174,\n",
       "   0.4918539226055145,\n",
       "   0.49193090200424194,\n",
       "   0.4920073449611664,\n",
       "   0.4920833110809326,\n",
       "   0.4921587407588959,\n",
       "   0.49223363399505615]),\n",
       " ([0.6594862937927246,\n",
       "   0.6595039367675781,\n",
       "   0.6594417691230774,\n",
       "   0.6595549583435059,\n",
       "   0.6594944000244141,\n",
       "   0.6595096588134766,\n",
       "   0.659669041633606,\n",
       "   0.659555196762085,\n",
       "   0.6595356464385986,\n",
       "   0.659538209438324,\n",
       "   0.6594973206520081,\n",
       "   0.6595964431762695,\n",
       "   0.659579336643219,\n",
       "   0.6595599055290222,\n",
       "   0.6595669984817505,\n",
       "   0.6596113443374634,\n",
       "   0.6596118211746216,\n",
       "   0.6595813632011414,\n",
       "   0.6595823764801025,\n",
       "   0.659553050994873,\n",
       "   0.6596426367759705,\n",
       "   0.6595962047576904,\n",
       "   0.6595942974090576,\n",
       "   0.6595759987831116,\n",
       "   0.6595820188522339,\n",
       "   0.6596156358718872,\n",
       "   0.6596242785453796,\n",
       "   0.659587562084198,\n",
       "   0.659624457359314,\n",
       "   0.6595956087112427,\n",
       "   0.6596152186393738,\n",
       "   0.6595961451530457,\n",
       "   0.65959632396698,\n",
       "   0.659579873085022,\n",
       "   0.6595791578292847,\n",
       "   0.6595885753631592,\n",
       "   0.6595942974090576,\n",
       "   0.6595924496650696,\n",
       "   0.6595919728279114,\n",
       "   0.6595938801765442,\n",
       "   0.6595941781997681,\n",
       "   0.659591555595398,\n",
       "   0.6595919728279114,\n",
       "   0.6596008539199829,\n",
       "   0.6595963835716248,\n",
       "   0.6595959067344666,\n",
       "   0.6595982909202576,\n",
       "   0.6595996022224426,\n",
       "   0.6595911383628845,\n",
       "   0.6595914363861084,\n",
       "   0.6596003770828247,\n",
       "   0.6596037745475769,\n",
       "   0.6596055626869202,\n",
       "   0.6596024632453918,\n",
       "   0.6595991253852844,\n",
       "   0.6596019864082336,\n",
       "   0.6596028208732605,\n",
       "   0.6596016883850098,\n",
       "   0.6596001982688904,\n",
       "   0.6596025824546814,\n",
       "   0.6596022248268127,\n",
       "   0.6596004962921143,\n",
       "   0.6596030592918396,\n",
       "   0.6596015691757202,\n",
       "   0.6596019268035889,\n",
       "   0.6596034169197083,\n",
       "   0.6596022844314575,\n",
       "   0.6596022248268127,\n",
       "   0.6596038341522217,\n",
       "   0.6596035361289978,\n",
       "   0.6596015095710754,\n",
       "   0.6596013307571411,\n",
       "   0.6596009731292725,\n",
       "   0.6596007347106934,\n",
       "   0.6596034169197083,\n",
       "   0.6596019268035889,\n",
       "   0.6596015691757202,\n",
       "   0.6596019864082336,\n",
       "   0.6596016883850098,\n",
       "   0.6596019864082336,\n",
       "   0.6596019268035889,\n",
       "   0.6596019268035889,\n",
       "   0.6596024632453918,\n",
       "   0.6596022844314575,\n",
       "   0.6596009135246277,\n",
       "   0.6596019268035889,\n",
       "   0.6596011519432068,\n",
       "   0.6596014499664307,\n",
       "   0.6596014499664307,\n",
       "   0.6596016883850098,\n",
       "   0.6596013307571411,\n",
       "   0.6596013307571411,\n",
       "   0.659601092338562,\n",
       "   0.6596013307571411,\n",
       "   0.6596014499664307,\n",
       "   0.6596012115478516,\n",
       "   0.6596013307571411,\n",
       "   0.6596018075942993,\n",
       "   0.6596009731292725,\n",
       "   0.6596012711524963,\n",
       "   0.6596012711524963,\n",
       "   0.6596013307571411,\n",
       "   0.6596012711524963,\n",
       "   0.6596015095710754,\n",
       "   0.6596012115478516,\n",
       "   0.6596012711524963,\n",
       "   0.6596012711524963,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596013307571411,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596012711524963,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596011519432068,\n",
       "   0.6596012115478516,\n",
       "   0.6596013307571411,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596011519432068,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596012115478516,\n",
       "   0.6596011519432068,\n",
       "   0.659601092338562,\n",
       "   0.659601092338562,\n",
       "   0.659601092338562,\n",
       "   0.659601092338562,\n",
       "   0.6596010327339172,\n",
       "   0.6596010327339172,\n",
       "   0.659601092338562,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596008539199829,\n",
       "   0.6596009731292725,\n",
       "   0.6596010327339172,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009135246277,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009135246277,\n",
       "   0.6596009731292725,\n",
       "   0.6596009135246277,\n",
       "   0.6596008539199829,\n",
       "   0.6596009731292725,\n",
       "   0.6596008539199829,\n",
       "   0.6596009135246277,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596010327339172,\n",
       "   0.6596010327339172,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596010327339172,\n",
       "   0.6596009731292725,\n",
       "   0.6596010327339172,\n",
       "   0.659601092338562,\n",
       "   0.6596010327339172,\n",
       "   0.6596010327339172,\n",
       "   0.6596009731292725,\n",
       "   0.6596011519432068,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.659601092338562,\n",
       "   0.659601092338562,\n",
       "   0.6596010327339172,\n",
       "   0.6596010327339172,\n",
       "   0.6596010327339172,\n",
       "   0.6596009731292725,\n",
       "   0.6596008539199829,\n",
       "   0.6596009135246277,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596009731292725,\n",
       "   0.6596008539199829,\n",
       "   0.6596009135246277,\n",
       "   0.6596009135246277,\n",
       "   0.6596009135246277,\n",
       "   0.6596009135246277,\n",
       "   0.6596008539199829,\n",
       "   0.6596009731292725,\n",
       "   0.6596009135246277,\n",
       "   0.6596009135246277,\n",
       "   0.6596008539199829,\n",
       "   0.6596008539199829],\n",
       "  [0.7684381604194641,\n",
       "   0.7684381604194641,\n",
       "   0.7684381604194641,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7684381604194641,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754,\n",
       "   0.7689805030822754],\n",
       "  [0.44702988862991333,\n",
       "   0.44769614934921265,\n",
       "   0.4483489990234375,\n",
       "   0.4490073621273041,\n",
       "   0.44965270161628723,\n",
       "   0.4502854347229004,\n",
       "   0.4508879482746124,\n",
       "   0.4514966905117035,\n",
       "   0.45209386944770813,\n",
       "   0.4526798129081726,\n",
       "   0.4532548189163208,\n",
       "   0.453819215297699,\n",
       "   0.4543732702732086,\n",
       "   0.4549172818660736,\n",
       "   0.45545151829719543,\n",
       "   0.4559762179851532,\n",
       "   0.4564916491508484,\n",
       "   0.4569980800151825,\n",
       "   0.45749571919441223,\n",
       "   0.4579848051071167,\n",
       "   0.4584655165672302,\n",
       "   0.4589381217956543,\n",
       "   0.45940279960632324,\n",
       "   0.4598597586154938,\n",
       "   0.46030914783477783,\n",
       "   0.4607512056827545,\n",
       "   0.46118611097335815,\n",
       "   0.4616139829158783,\n",
       "   0.46203505992889404,\n",
       "   0.46244943141937256,\n",
       "   0.46285730600357056,\n",
       "   0.4632588028907776,\n",
       "   0.463654100894928,\n",
       "   0.46404334902763367,\n",
       "   0.4644266366958618,\n",
       "   0.46480414271354675,\n",
       "   0.465175986289978,\n",
       "   0.4655422866344452,\n",
       "   0.4659031629562378,\n",
       "   0.4662587344646454,\n",
       "   0.4666091501712799,\n",
       "   0.46695446968078613,\n",
       "   0.4672948718070984,\n",
       "   0.46763038635253906,\n",
       "   0.4679611623287201,\n",
       "   0.46828725934028625,\n",
       "   0.4686088263988495,\n",
       "   0.46892595291137695,\n",
       "   0.4692386984825134,\n",
       "   0.4695471525192261,\n",
       "   0.46985143423080444,\n",
       "   0.4701516032218933,\n",
       "   0.47044774889945984,\n",
       "   0.4707399606704712,\n",
       "   0.47102829813957214,\n",
       "   0.47131285071372986,\n",
       "   0.4715937077999115,\n",
       "   0.47187089920043945,\n",
       "   0.47214454412460327,\n",
       "   0.47241467237472534,\n",
       "   0.47268134355545044,\n",
       "   0.4729446768760681,\n",
       "   0.47320470213890076,\n",
       "   0.47346147894859314,\n",
       "   0.47371506690979004,\n",
       "   0.47396552562713623,\n",
       "   0.4742129147052765,\n",
       "   0.4744572937488556,\n",
       "   0.4746987223625183,\n",
       "   0.47493723034858704,\n",
       "   0.47517290711402893,\n",
       "   0.475405752658844,\n",
       "   0.4756358563899994,\n",
       "   0.4758632779121399,\n",
       "   0.4760880470275879,\n",
       "   0.47631019353866577,\n",
       "   0.4765297472476959,\n",
       "   0.4767468273639679,\n",
       "   0.4769614040851593,\n",
       "   0.4771735370159149,\n",
       "   0.4773833155632019,\n",
       "   0.4775907099246979,\n",
       "   0.47779580950737,\n",
       "   0.47799861431121826,\n",
       "   0.47819918394088745,\n",
       "   0.47839754819869995,\n",
       "   0.47859376668930054,\n",
       "   0.4787878394126892,\n",
       "   0.47897982597351074,\n",
       "   0.4791697561740875,\n",
       "   0.47935763001441956,\n",
       "   0.479543536901474,\n",
       "   0.47972747683525085,\n",
       "   0.4799094796180725,\n",
       "   0.48008957505226135,\n",
       "   0.48026779294013977,\n",
       "   0.48044416308403015,\n",
       "   0.4806187152862549,\n",
       "   0.48079150915145874,\n",
       "   0.48096251487731934,\n",
       "   0.48113179206848145,\n",
       "   0.48129937052726746,\n",
       "   0.48146525025367737,\n",
       "   0.48162949085235596,\n",
       "   0.4817920923233032,\n",
       "   0.48195308446884155,\n",
       "   0.48211249709129333,\n",
       "   0.48227033019065857,\n",
       "   0.48242664337158203,\n",
       "   0.4825814366340637,\n",
       "   0.482734739780426,\n",
       "   0.48288655281066895,\n",
       "   0.48303693532943726,\n",
       "   0.48318588733673096,\n",
       "   0.48333343863487244,\n",
       "   0.4834795594215393,\n",
       "   0.48362433910369873,\n",
       "   0.4837677776813507,\n",
       "   0.48390987515449524,\n",
       "   0.4840506315231323,\n",
       "   0.4841901361942291,\n",
       "   0.48432832956314087,\n",
       "   0.48446527123451233,\n",
       "   0.4846009612083435,\n",
       "   0.4847354590892792,\n",
       "   0.48486870527267456,\n",
       "   0.4850007891654968,\n",
       "   0.4851316809654236,\n",
       "   0.4852614104747772,\n",
       "   0.48538997769355774,\n",
       "   0.4855174422264099,\n",
       "   0.48564377427101135,\n",
       "   0.48576900362968445,\n",
       "   0.4858931303024292,\n",
       "   0.4860162138938904,\n",
       "   0.4861382246017456,\n",
       "   0.48625919222831726,\n",
       "   0.48637911677360535,\n",
       "   0.48649802803993225,\n",
       "   0.486615926027298,\n",
       "   0.4867328405380249,\n",
       "   0.48684877157211304,\n",
       "   0.48696374893188477,\n",
       "   0.4870777428150177,\n",
       "   0.4871908128261566,\n",
       "   0.4873029291629791,\n",
       "   0.4874141216278076,\n",
       "   0.4875244200229645,\n",
       "   0.4876338243484497,\n",
       "   0.4877423346042633,\n",
       "   0.4878499507904053,\n",
       "   0.487956702709198,\n",
       "   0.48806262016296387,\n",
       "   0.4881676733493805,\n",
       "   0.48827189207077026,\n",
       "   0.4883752763271332,\n",
       "   0.4884778559207916,\n",
       "   0.4885796308517456,\n",
       "   0.4886806011199951,\n",
       "   0.48878079652786255,\n",
       "   0.4888802170753479,\n",
       "   0.4889788329601288,\n",
       "   0.48907673358917236,\n",
       "   0.48917385935783386,\n",
       "   0.48927024006843567,\n",
       "   0.4893658757209778,\n",
       "   0.4894607961177826,\n",
       "   0.4895550012588501,\n",
       "   0.4896484911441803,\n",
       "   0.4897412657737732,\n",
       "   0.48983335494995117,\n",
       "   0.4899247884750366,\n",
       "   0.49001550674438477,\n",
       "   0.4901055693626404,\n",
       "   0.4901949465274811,\n",
       "   0.49028369784355164,\n",
       "   0.4903717637062073,\n",
       "   0.49045923352241516,\n",
       "   0.49054601788520813,\n",
       "   0.49063220620155334,\n",
       "   0.4907177686691284,\n",
       "   0.49080270528793335,\n",
       "   0.4908870458602905,\n",
       "   0.49097079038619995,\n",
       "   0.49105390906333923,\n",
       "   0.49113646149635315,\n",
       "   0.4912184476852417,\n",
       "   0.4912998378276825,\n",
       "   0.4913806617259979,\n",
       "   0.4914608895778656,\n",
       "   0.4915406107902527,\n",
       "   0.491619735956192,\n",
       "   0.49169832468032837,\n",
       "   0.49177637696266174,\n",
       "   0.4918539226055145,\n",
       "   0.49193090200424194,\n",
       "   0.4920073449611664,\n",
       "   0.4920833110809326,\n",
       "   0.4921587407588959,\n",
       "   0.49223363399505615]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x13483a210>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd282a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d408fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
