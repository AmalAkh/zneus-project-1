{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ce29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.seed(42)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "CLASSES = 2\n",
    "\n",
    "TRAIN_VAL_TEST_SPLIT = [0.9, 0.05, 0.05]\n",
    "\n",
    "EPOCHS = 40\n",
    "LOSS_FUNCTION =nn.BCEWithLogitsLoss()#pos_weight=torch.tensor([10.0])\n",
    "\n",
    "AUGMENT = True\n",
    "SAVE_BEST_MODEL = True\n",
    "IS_MULTICLASS = True if CLASSES > 2 else False\n",
    "NUM_OF_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SteelPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path):\n",
    "        super().__init__()\n",
    "        self.path = dataset_path\n",
    "        self.df = pd.read_csv(self.path)\n",
    "\n",
    "        self.features = self.df.drop([\"Class\", *(\"V28 V29 V30 V31 V32 V33\".split(\" \"))] ,axis= 1).values.tolist()\n",
    "        self.labels = self.df[\"Class\"].to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.tensor(self.features[index]), torch.tensor(self.labels[index])\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset = SteelPlateDataset(\"data/norm_data.csv\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, TRAIN_VAL_TEST_SPLIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae793ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714376cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5652e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8785eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1747\n",
      "Validation dataset size: 97\n",
      "Test dataset size: 97\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Default shuffling for training\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for validation\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No shuffling for test\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4f70375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy, Precision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size,lr=0.001, loss_fn=nn.BCELoss(), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.accuracy = Accuracy(task=\"binary\", num_classes=num_classes)\n",
    "        self.precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.to(device_name)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        eval_loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "\n",
    "                \n",
    "                    \n",
    "                loss = self.loss_fn(output, y)\n",
    " \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "              \n",
    "          \n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        self.train()\n",
    "        return (eval_loss/len(dataloader), self.accuracy.compute(), self.precision.compute())\n",
    "    \n",
    "        \n",
    "    def fit(self, train_dataloader, val_dataloader, epochs=10):\n",
    "        self.train()\n",
    "        best_val_loss = 9999\n",
    "\n",
    "     \n",
    "      \n",
    "        for i in range(0,epochs):\n",
    "           \n",
    "            self.accuracy.reset()\n",
    "            epoch_loss = 0\n",
    "            for batch in train_dataloader:\n",
    "\n",
    "                x = batch[0].to(device_name)\n",
    "                y = batch[1].to(device_name)\n",
    "              \n",
    "\n",
    "                output = self.forward(x)\n",
    "                output = torch.sigmoid(output).squeeze(1)\n",
    "                y = y.float()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = self.loss_fn(output, y)\n",
    "\n",
    "               \n",
    "                self.accuracy(output, y)\n",
    "                self.precision(output, y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss/=len(train_dataloader)\n",
    "           \n",
    "            epoch_acc = self.accuracy.compute()\n",
    "            epoch_precision = self.precision.compute()\n",
    "\n",
    "       \n",
    "\n",
    "            val_loss, val_acc, val_precision = self.evaluate(val_dataloader)\n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.state_dict(), \"best-model-by-loss.pth\")\n",
    "\n",
    "         \n",
    "\n",
    "            self.scheduler.step(val_loss)\n",
    "            #wandb.log({\"epoch\": i, \"Train Loss\": epoch_loss, \"Train Acc\":epoch_acc,\"Train F1\":epoch_f1, \"Val Loss\":val_loss, \"Val Acc\":val_acc,\"Val F1\":val_f1, \"LR\":self.optimizer.param_groups[0]['lr']})\n",
    "            print(f\"Epoch {i+1} Loss:{epoch_loss:.4f} Accuracy:{epoch_acc:.4f} Positive Precision:{epoch_precision[1].item():.4f} Negative Precision:{epoch_precision[0].item():.4f}  Val Loss:{val_loss:.4f} Val Accuracy:{val_acc:.4f} Val Positive Precision:{val_precision[1].item():.4f} Val Negative Precision:{val_precision[0].item():.4f} LR = {self.optimizer.param_groups[0]['lr']}\")\n",
    "        #wandb.finish()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353dcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size=27,num_classes=CLASSES, loss_fn=LOSS_FUNCTION, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8031b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss:1.9800 Accuracy:0.3515 Positive Precision:0.0000 Negative Precision:0.6520  Val Loss:1.8107 Val Accuracy:0.3520 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 2 Loss:1.9460 Accuracy:0.3480 Positive Precision:0.0000 Negative Precision:0.6516  Val Loss:1.8107 Val Accuracy:0.3487 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 3 Loss:1.9474 Accuracy:0.3480 Positive Precision:0.0000 Negative Precision:0.6515  Val Loss:1.8107 Val Accuracy:0.3487 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 4 Loss:1.9308 Accuracy:0.3829 Positive Precision:0.0000 Negative Precision:0.6515  Val Loss:1.7462 Val Accuracy:0.3905 Val Positive Precision:0.0000 Val Negative Precision:0.6513 LR = 0.001\n",
      "Epoch 5 Loss:1.8610 Accuracy:0.5083 Positive Precision:0.4034 Negative Precision:0.6532  Val Loss:1.7267 Val Accuracy:0.5103 Val Positive Precision:0.3987 Val Negative Precision:0.6530 LR = 0.001\n",
      "Epoch 6 Loss:1.8565 Accuracy:0.5112 Positive Precision:0.4125 Negative Precision:0.6582  Val Loss:1.7353 Val Accuracy:0.5119 Val Positive Precision:0.4128 Val Negative Precision:0.6586 LR = 0.001\n",
      "Epoch 7 Loss:1.8539 Accuracy:0.5117 Positive Precision:0.4056 Negative Precision:0.6626  Val Loss:1.7283 Val Accuracy:0.5130 Val Positive Precision:0.4049 Val Negative Precision:0.6627 LR = 0.001\n",
      "Epoch 8 Loss:1.8539 Accuracy:0.5140 Positive Precision:0.4072 Negative Precision:0.6682  Val Loss:1.7291 Val Accuracy:0.5152 Val Positive Precision:0.4074 Val Negative Precision:0.6684 LR = 0.001\n",
      "Epoch 9 Loss:1.8506 Accuracy:0.5135 Positive Precision:0.4065 Negative Precision:0.6727  Val Loss:1.7253 Val Accuracy:0.5152 Val Positive Precision:0.4062 Val Negative Precision:0.6727 LR = 0.001\n",
      "Epoch 10 Loss:1.8496 Accuracy:0.5123 Positive Precision:0.4080 Negative Precision:0.6781  Val Loss:1.7250 Val Accuracy:0.5141 Val Positive Precision:0.4077 Val Negative Precision:0.6781 LR = 0.001\n",
      "Epoch 11 Loss:1.8507 Accuracy:0.5180 Positive Precision:0.4088 Negative Precision:0.6829  Val Loss:1.7244 Val Accuracy:0.5195 Val Positive Precision:0.4085 Val Negative Precision:0.6829 LR = 0.001\n",
      "Epoch 12 Loss:1.8468 Accuracy:0.5192 Positive Precision:0.4096 Negative Precision:0.6872  Val Loss:1.7257 Val Accuracy:0.5206 Val Positive Precision:0.4095 Val Negative Precision:0.6873 LR = 0.001\n",
      "Epoch 13 Loss:1.8479 Accuracy:0.5180 Positive Precision:0.4100 Negative Precision:0.6918  Val Loss:1.7244 Val Accuracy:0.5195 Val Positive Precision:0.4098 Val Negative Precision:0.6917 LR = 0.001\n",
      "Epoch 14 Loss:1.8468 Accuracy:0.5197 Positive Precision:0.4104 Negative Precision:0.6956  Val Loss:1.7253 Val Accuracy:0.5211 Val Positive Precision:0.4103 Val Negative Precision:0.6956 LR = 0.001\n",
      "Epoch 15 Loss:1.8475 Accuracy:0.5203 Positive Precision:0.4111 Negative Precision:0.6996  Val Loss:1.7241 Val Accuracy:0.5217 Val Positive Precision:0.4111 Val Negative Precision:0.6997 LR = 0.001\n",
      "Epoch 16 Loss:1.8442 Accuracy:0.5215 Positive Precision:0.4123 Negative Precision:0.7042  Val Loss:1.7243 Val Accuracy:0.5228 Val Positive Precision:0.4124 Val Negative Precision:0.7044 LR = 0.001\n",
      "Epoch 17 Loss:1.8440 Accuracy:0.5215 Positive Precision:0.4134 Negative Precision:0.7084  Val Loss:1.7237 Val Accuracy:0.5228 Val Positive Precision:0.4134 Val Negative Precision:0.7086 LR = 0.001\n",
      "Epoch 18 Loss:1.8495 Accuracy:0.5169 Positive Precision:0.4146 Negative Precision:0.7131  Val Loss:1.7281 Val Accuracy:0.5179 Val Positive Precision:0.4147 Val Negative Precision:0.7133 LR = 0.001\n",
      "Epoch 19 Loss:1.8455 Accuracy:0.5197 Positive Precision:0.4155 Negative Precision:0.7174  Val Loss:1.7271 Val Accuracy:0.5206 Val Positive Precision:0.4156 Val Negative Precision:0.7176 LR = 0.001\n",
      "Epoch 20 Loss:1.8468 Accuracy:0.5215 Positive Precision:0.4166 Negative Precision:0.7208  Val Loss:1.7269 Val Accuracy:0.5222 Val Positive Precision:0.4167 Val Negative Precision:0.7211 LR = 0.001\n",
      "Epoch 21 Loss:1.8462 Accuracy:0.5186 Positive Precision:0.4177 Negative Precision:0.7252  Val Loss:1.7237 Val Accuracy:0.5201 Val Positive Precision:0.4178 Val Negative Precision:0.7255 LR = 0.001\n",
      "Epoch 22 Loss:1.8469 Accuracy:0.5209 Positive Precision:0.4187 Negative Precision:0.7292  Val Loss:1.7242 Val Accuracy:0.5222 Val Positive Precision:0.4188 Val Negative Precision:0.7294 LR = 0.001\n",
      "Epoch 23 Loss:1.8529 Accuracy:0.5169 Positive Precision:0.4192 Negative Precision:0.7328  Val Loss:1.7280 Val Accuracy:0.5179 Val Positive Precision:0.4193 Val Negative Precision:0.7330 LR = 0.001\n",
      "Epoch 24 Loss:1.8450 Accuracy:0.5197 Positive Precision:0.4198 Negative Precision:0.7365  Val Loss:1.7236 Val Accuracy:0.5211 Val Positive Precision:0.4198 Val Negative Precision:0.7366 LR = 0.001\n",
      "Epoch 25 Loss:1.8443 Accuracy:0.5226 Positive Precision:0.4202 Negative Precision:0.7391  Val Loss:1.7276 Val Accuracy:0.5233 Val Positive Precision:0.4203 Val Negative Precision:0.7392 LR = 0.001\n",
      "Epoch 26 Loss:1.8462 Accuracy:0.5220 Positive Precision:0.4210 Negative Precision:0.7425  Val Loss:1.7253 Val Accuracy:0.5233 Val Positive Precision:0.4211 Val Negative Precision:0.7426 LR = 0.001\n",
      "Epoch 27 Loss:1.8477 Accuracy:0.5197 Positive Precision:0.4216 Negative Precision:0.7458  Val Loss:1.7236 Val Accuracy:0.5211 Val Positive Precision:0.4217 Val Negative Precision:0.7460 LR = 0.001\n",
      "Epoch 28 Loss:1.8431 Accuracy:0.5323 Positive Precision:0.4221 Negative Precision:0.7478  Val Loss:1.7303 Val Accuracy:0.5325 Val Positive Precision:0.4222 Val Negative Precision:0.7480 LR = 0.0005\n",
      "Epoch 29 Loss:1.8505 Accuracy:0.5089 Positive Precision:0.4228 Negative Precision:0.7516  Val Loss:1.7247 Val Accuracy:0.5108 Val Positive Precision:0.4228 Val Negative Precision:0.7518 LR = 0.0005\n",
      "Epoch 30 Loss:1.8388 Accuracy:0.5272 Positive Precision:0.4236 Negative Precision:0.7547  Val Loss:1.7249 Val Accuracy:0.5282 Val Positive Precision:0.4236 Val Negative Precision:0.7548 LR = 0.0005\n",
      "Epoch 31 Loss:1.8399 Accuracy:0.5421 Positive Precision:0.4248 Negative Precision:0.7578  Val Loss:1.7374 Val Accuracy:0.5418 Val Positive Precision:0.4249 Val Negative Precision:0.7580 LR = 0.0005\n",
      "Epoch 32 Loss:1.8335 Accuracy:0.5478 Positive Precision:0.4260 Negative Precision:0.7605  Val Loss:1.7493 Val Accuracy:0.5472 Val Positive Precision:0.4261 Val Negative Precision:0.7606 LR = 0.0005\n",
      "Epoch 33 Loss:1.8278 Accuracy:0.5564 Positive Precision:0.4271 Negative Precision:0.7626  Val Loss:1.7445 Val Accuracy:0.5553 Val Positive Precision:0.4272 Val Negative Precision:0.7628 LR = 0.0005\n",
      "Epoch 34 Loss:1.8331 Accuracy:0.5564 Positive Precision:0.4283 Negative Precision:0.7650  Val Loss:1.7287 Val Accuracy:0.5559 Val Positive Precision:0.4284 Val Negative Precision:0.7652 LR = 0.0005\n",
      "Epoch 35 Loss:1.8269 Accuracy:0.5627 Positive Precision:0.4293 Negative Precision:0.7668  Val Loss:1.7292 Val Accuracy:0.5618 Val Positive Precision:0.4294 Val Negative Precision:0.7670 LR = 0.0005\n",
      "Epoch 36 Loss:1.8251 Accuracy:0.5644 Positive Precision:0.4304 Negative Precision:0.7687  Val Loss:1.7267 Val Accuracy:0.5634 Val Positive Precision:0.4305 Val Negative Precision:0.7689 LR = 0.0005\n",
      "Epoch 37 Loss:1.8235 Accuracy:0.5684 Positive Precision:0.4315 Negative Precision:0.7710  Val Loss:1.7376 Val Accuracy:0.5683 Val Positive Precision:0.4315 Val Negative Precision:0.7710 LR = 0.0005\n",
      "Epoch 38 Loss:1.8204 Accuracy:0.5741 Positive Precision:0.4323 Negative Precision:0.7724  Val Loss:1.7399 Val Accuracy:0.5738 Val Positive Precision:0.4324 Val Negative Precision:0.7724 LR = 0.0005\n",
      "Epoch 39 Loss:1.8222 Accuracy:0.5736 Positive Precision:0.4332 Negative Precision:0.7737  Val Loss:1.7439 Val Accuracy:0.5721 Val Positive Precision:0.4332 Val Negative Precision:0.7738 LR = 0.00025\n",
      "Epoch 40 Loss:1.8181 Accuracy:0.5787 Positive Precision:0.4341 Negative Precision:0.7753  Val Loss:1.7452 Val Accuracy:0.5770 Val Positive Precision:0.4341 Val Negative Precision:0.7753 LR = 0.00025\n",
      "Epoch 41 Loss:1.8161 Accuracy:0.5793 Positive Precision:0.4349 Negative Precision:0.7764  Val Loss:1.7409 Val Accuracy:0.5781 Val Positive Precision:0.4349 Val Negative Precision:0.7764 LR = 0.00025\n",
      "Epoch 42 Loss:1.8122 Accuracy:0.5793 Positive Precision:0.4356 Negative Precision:0.7776  Val Loss:1.7451 Val Accuracy:0.5775 Val Positive Precision:0.4356 Val Negative Precision:0.7776 LR = 0.00025\n",
      "Epoch 43 Loss:1.8141 Accuracy:0.5839 Positive Precision:0.4363 Negative Precision:0.7784  Val Loss:1.7466 Val Accuracy:0.5819 Val Positive Precision:0.4363 Val Negative Precision:0.7784 LR = 0.00025\n",
      "Epoch 44 Loss:1.8128 Accuracy:0.5787 Positive Precision:0.4369 Negative Precision:0.7795  Val Loss:1.7416 Val Accuracy:0.5775 Val Positive Precision:0.4370 Val Negative Precision:0.7795 LR = 0.00025\n",
      "Epoch 45 Loss:1.8124 Accuracy:0.5793 Positive Precision:0.4375 Negative Precision:0.7804  Val Loss:1.7417 Val Accuracy:0.5781 Val Positive Precision:0.4375 Val Negative Precision:0.7804 LR = 0.00025\n",
      "Epoch 46 Loss:1.8139 Accuracy:0.5856 Positive Precision:0.4381 Negative Precision:0.7812  Val Loss:1.7458 Val Accuracy:0.5835 Val Positive Precision:0.4381 Val Negative Precision:0.7813 LR = 0.00025\n",
      "Epoch 47 Loss:1.8106 Accuracy:0.5839 Positive Precision:0.4386 Negative Precision:0.7821  Val Loss:1.7437 Val Accuracy:0.5819 Val Positive Precision:0.4387 Val Negative Precision:0.7822 LR = 0.00025\n",
      "Epoch 48 Loss:1.8120 Accuracy:0.5833 Positive Precision:0.4392 Negative Precision:0.7830  Val Loss:1.7436 Val Accuracy:0.5813 Val Positive Precision:0.4392 Val Negative Precision:0.7831 LR = 0.00025\n",
      "Epoch 49 Loss:1.8106 Accuracy:0.5861 Positive Precision:0.4397 Negative Precision:0.7838  Val Loss:1.7431 Val Accuracy:0.5841 Val Positive Precision:0.4397 Val Negative Precision:0.7838 LR = 0.00025\n",
      "Epoch 50 Loss:1.8118 Accuracy:0.5850 Positive Precision:0.4402 Negative Precision:0.7845  Val Loss:1.7452 Val Accuracy:0.5830 Val Positive Precision:0.4402 Val Negative Precision:0.7845 LR = 0.000125\n",
      "Epoch 51 Loss:1.8078 Accuracy:0.5856 Positive Precision:0.4406 Negative Precision:0.7851  Val Loss:1.7425 Val Accuracy:0.5841 Val Positive Precision:0.4407 Val Negative Precision:0.7852 LR = 0.000125\n",
      "Epoch 52 Loss:1.8062 Accuracy:0.5884 Positive Precision:0.4411 Negative Precision:0.7858  Val Loss:1.7410 Val Accuracy:0.5868 Val Positive Precision:0.4411 Val Negative Precision:0.7858 LR = 0.000125\n",
      "Epoch 53 Loss:1.8115 Accuracy:0.5879 Positive Precision:0.4415 Negative Precision:0.7864  Val Loss:1.7421 Val Accuracy:0.5862 Val Positive Precision:0.4415 Val Negative Precision:0.7864 LR = 0.000125\n",
      "Epoch 54 Loss:1.8036 Accuracy:0.5873 Positive Precision:0.4419 Negative Precision:0.7870  Val Loss:1.7394 Val Accuracy:0.5857 Val Positive Precision:0.4420 Val Negative Precision:0.7870 LR = 0.000125\n",
      "Epoch 55 Loss:1.8080 Accuracy:0.5890 Positive Precision:0.4423 Negative Precision:0.7875  Val Loss:1.7408 Val Accuracy:0.5873 Val Positive Precision:0.4424 Val Negative Precision:0.7875 LR = 0.000125\n",
      "Epoch 56 Loss:1.8053 Accuracy:0.5879 Positive Precision:0.4428 Negative Precision:0.7881  Val Loss:1.7415 Val Accuracy:0.5862 Val Positive Precision:0.4428 Val Negative Precision:0.7881 LR = 0.000125\n",
      "Epoch 57 Loss:1.8081 Accuracy:0.5902 Positive Precision:0.4432 Negative Precision:0.7886  Val Loss:1.7385 Val Accuracy:0.5889 Val Positive Precision:0.4432 Val Negative Precision:0.7886 LR = 0.000125\n",
      "Epoch 58 Loss:1.8064 Accuracy:0.5896 Positive Precision:0.4435 Negative Precision:0.7891  Val Loss:1.7419 Val Accuracy:0.5879 Val Positive Precision:0.4436 Val Negative Precision:0.7892 LR = 0.000125\n",
      "Epoch 59 Loss:1.8074 Accuracy:0.5913 Positive Precision:0.4439 Negative Precision:0.7896  Val Loss:1.7393 Val Accuracy:0.5895 Val Positive Precision:0.4440 Val Negative Precision:0.7897 LR = 0.000125\n",
      "Epoch 60 Loss:1.8083 Accuracy:0.5907 Positive Precision:0.4443 Negative Precision:0.7901  Val Loss:1.7366 Val Accuracy:0.5895 Val Positive Precision:0.4443 Val Negative Precision:0.7901 LR = 0.000125\n",
      "Epoch 61 Loss:1.8061 Accuracy:0.5884 Positive Precision:0.4446 Negative Precision:0.7906  Val Loss:1.7339 Val Accuracy:0.5879 Val Positive Precision:0.4447 Val Negative Precision:0.7906 LR = 6.25e-05\n",
      "Epoch 62 Loss:1.8052 Accuracy:0.5930 Positive Precision:0.4450 Negative Precision:0.7910  Val Loss:1.7379 Val Accuracy:0.5911 Val Positive Precision:0.4450 Val Negative Precision:0.7910 LR = 6.25e-05\n",
      "Epoch 63 Loss:1.8060 Accuracy:0.5919 Positive Precision:0.4453 Negative Precision:0.7914  Val Loss:1.7379 Val Accuracy:0.5900 Val Positive Precision:0.4454 Val Negative Precision:0.7914 LR = 6.25e-05\n",
      "Epoch 64 Loss:1.8074 Accuracy:0.5913 Positive Precision:0.4457 Negative Precision:0.7918  Val Loss:1.7364 Val Accuracy:0.5906 Val Positive Precision:0.4457 Val Negative Precision:0.7918 LR = 6.25e-05\n",
      "Epoch 65 Loss:1.8047 Accuracy:0.5919 Positive Precision:0.4460 Negative Precision:0.7923  Val Loss:1.7356 Val Accuracy:0.5911 Val Positive Precision:0.4460 Val Negative Precision:0.7923 LR = 6.25e-05\n",
      "Epoch 66 Loss:1.8047 Accuracy:0.5919 Positive Precision:0.4463 Negative Precision:0.7926  Val Loss:1.7353 Val Accuracy:0.5911 Val Positive Precision:0.4463 Val Negative Precision:0.7926 LR = 6.25e-05\n",
      "Epoch 67 Loss:1.8034 Accuracy:0.5942 Positive Precision:0.4466 Negative Precision:0.7930  Val Loss:1.7362 Val Accuracy:0.5933 Val Positive Precision:0.4466 Val Negative Precision:0.7930 LR = 6.25e-05\n",
      "Epoch 68 Loss:1.8038 Accuracy:0.5942 Positive Precision:0.4469 Negative Precision:0.7933  Val Loss:1.7355 Val Accuracy:0.5933 Val Positive Precision:0.4469 Val Negative Precision:0.7933 LR = 6.25e-05\n",
      "Epoch 69 Loss:1.8056 Accuracy:0.5936 Positive Precision:0.4471 Negative Precision:0.7936  Val Loss:1.7336 Val Accuracy:0.5927 Val Positive Precision:0.4471 Val Negative Precision:0.7936 LR = 6.25e-05\n",
      "Epoch 70 Loss:1.8033 Accuracy:0.5942 Positive Precision:0.4474 Negative Precision:0.7939  Val Loss:1.7335 Val Accuracy:0.5933 Val Positive Precision:0.4474 Val Negative Precision:0.7939 LR = 6.25e-05\n",
      "Epoch 71 Loss:1.8023 Accuracy:0.5965 Positive Precision:0.4476 Negative Precision:0.7941  Val Loss:1.7361 Val Accuracy:0.5949 Val Positive Precision:0.4476 Val Negative Precision:0.7941 LR = 6.25e-05\n",
      "Epoch 72 Loss:1.8050 Accuracy:0.5942 Positive Precision:0.4479 Negative Precision:0.7944  Val Loss:1.7329 Val Accuracy:0.5933 Val Positive Precision:0.4479 Val Negative Precision:0.7944 LR = 3.125e-05\n",
      "Epoch 73 Loss:1.8027 Accuracy:0.5965 Positive Precision:0.4481 Negative Precision:0.7946  Val Loss:1.7334 Val Accuracy:0.5954 Val Positive Precision:0.4481 Val Negative Precision:0.7946 LR = 3.125e-05\n",
      "Epoch 74 Loss:1.8008 Accuracy:0.5959 Positive Precision:0.4483 Negative Precision:0.7948  Val Loss:1.7317 Val Accuracy:0.5949 Val Positive Precision:0.4483 Val Negative Precision:0.7948 LR = 3.125e-05\n",
      "Epoch 75 Loss:1.8068 Accuracy:0.5959 Positive Precision:0.4485 Negative Precision:0.7951  Val Loss:1.7320 Val Accuracy:0.5949 Val Positive Precision:0.4485 Val Negative Precision:0.7951 LR = 3.125e-05\n",
      "Epoch 76 Loss:1.8010 Accuracy:0.5965 Positive Precision:0.4487 Negative Precision:0.7953  Val Loss:1.7331 Val Accuracy:0.5954 Val Positive Precision:0.4487 Val Negative Precision:0.7952 LR = 3.125e-05\n",
      "Epoch 77 Loss:1.8049 Accuracy:0.5965 Positive Precision:0.4489 Negative Precision:0.7955  Val Loss:1.7322 Val Accuracy:0.5954 Val Positive Precision:0.4489 Val Negative Precision:0.7954 LR = 3.125e-05\n",
      "Epoch 78 Loss:1.8004 Accuracy:0.5959 Positive Precision:0.4491 Negative Precision:0.7956  Val Loss:1.7315 Val Accuracy:0.5949 Val Positive Precision:0.4491 Val Negative Precision:0.7956 LR = 3.125e-05\n",
      "Epoch 79 Loss:1.8035 Accuracy:0.5965 Positive Precision:0.4493 Negative Precision:0.7958  Val Loss:1.7303 Val Accuracy:0.5954 Val Positive Precision:0.4493 Val Negative Precision:0.7958 LR = 3.125e-05\n",
      "Epoch 80 Loss:1.8010 Accuracy:0.5976 Positive Precision:0.4494 Negative Precision:0.7959  Val Loss:1.7295 Val Accuracy:0.5965 Val Positive Precision:0.4495 Val Negative Precision:0.7959 LR = 3.125e-05\n",
      "Epoch 81 Loss:1.8023 Accuracy:0.5976 Positive Precision:0.4496 Negative Precision:0.7961  Val Loss:1.7284 Val Accuracy:0.5965 Val Positive Precision:0.4496 Val Negative Precision:0.7961 LR = 3.125e-05\n",
      "Epoch 82 Loss:1.8045 Accuracy:0.5982 Positive Precision:0.4498 Negative Precision:0.7962  Val Loss:1.7291 Val Accuracy:0.5971 Val Positive Precision:0.4498 Val Negative Precision:0.7962 LR = 3.125e-05\n",
      "Epoch 83 Loss:1.8026 Accuracy:0.5993 Positive Precision:0.4499 Negative Precision:0.7963  Val Loss:1.7290 Val Accuracy:0.5982 Val Positive Precision:0.4499 Val Negative Precision:0.7963 LR = 1.5625e-05\n",
      "Epoch 84 Loss:1.8032 Accuracy:0.5982 Positive Precision:0.4501 Negative Precision:0.7965  Val Loss:1.7294 Val Accuracy:0.5971 Val Positive Precision:0.4501 Val Negative Precision:0.7964 LR = 1.5625e-05\n",
      "Epoch 85 Loss:1.8002 Accuracy:0.5982 Positive Precision:0.4502 Negative Precision:0.7966  Val Loss:1.7292 Val Accuracy:0.5971 Val Positive Precision:0.4502 Val Negative Precision:0.7966 LR = 1.5625e-05\n",
      "Epoch 86 Loss:1.8013 Accuracy:0.5987 Positive Precision:0.4504 Negative Precision:0.7967  Val Loss:1.7278 Val Accuracy:0.5987 Val Positive Precision:0.4504 Val Negative Precision:0.7967 LR = 1.5625e-05\n",
      "Epoch 87 Loss:1.7999 Accuracy:0.5993 Positive Precision:0.4505 Negative Precision:0.7968  Val Loss:1.7276 Val Accuracy:0.5992 Val Positive Precision:0.4505 Val Negative Precision:0.7968 LR = 1.5625e-05\n",
      "Epoch 88 Loss:1.8026 Accuracy:0.5999 Positive Precision:0.4506 Negative Precision:0.7969  Val Loss:1.7281 Val Accuracy:0.5998 Val Positive Precision:0.4506 Val Negative Precision:0.7969 LR = 1.5625e-05\n",
      "Epoch 89 Loss:1.7993 Accuracy:0.6005 Positive Precision:0.4507 Negative Precision:0.7970  Val Loss:1.7281 Val Accuracy:0.6003 Val Positive Precision:0.4508 Val Negative Precision:0.7970 LR = 1.5625e-05\n",
      "Epoch 90 Loss:1.8013 Accuracy:0.6010 Positive Precision:0.4509 Negative Precision:0.7971  Val Loss:1.7272 Val Accuracy:0.6009 Val Positive Precision:0.4509 Val Negative Precision:0.7971 LR = 1.5625e-05\n",
      "Epoch 91 Loss:1.8024 Accuracy:0.6010 Positive Precision:0.4510 Negative Precision:0.7972  Val Loss:1.7281 Val Accuracy:0.6009 Val Positive Precision:0.4510 Val Negative Precision:0.7972 LR = 1.5625e-05\n",
      "Epoch 92 Loss:1.7990 Accuracy:0.6005 Positive Precision:0.4511 Negative Precision:0.7972  Val Loss:1.7267 Val Accuracy:0.6003 Val Positive Precision:0.4511 Val Negative Precision:0.7972 LR = 1.5625e-05\n",
      "Epoch 93 Loss:1.7992 Accuracy:0.6010 Positive Precision:0.4512 Negative Precision:0.7973  Val Loss:1.7265 Val Accuracy:0.6009 Val Positive Precision:0.4512 Val Negative Precision:0.7973 LR = 1.5625e-05\n",
      "Epoch 94 Loss:1.8033 Accuracy:0.6010 Positive Precision:0.4513 Negative Precision:0.7974  Val Loss:1.7263 Val Accuracy:0.6009 Val Positive Precision:0.4513 Val Negative Precision:0.7974 LR = 7.8125e-06\n",
      "Epoch 95 Loss:1.8016 Accuracy:0.6010 Positive Precision:0.4514 Negative Precision:0.7974  Val Loss:1.7259 Val Accuracy:0.6009 Val Positive Precision:0.4514 Val Negative Precision:0.7974 LR = 7.8125e-06\n",
      "Epoch 96 Loss:1.8003 Accuracy:0.6010 Positive Precision:0.4515 Negative Precision:0.7975  Val Loss:1.7264 Val Accuracy:0.6009 Val Positive Precision:0.4515 Val Negative Precision:0.7975 LR = 7.8125e-06\n",
      "Epoch 97 Loss:1.8005 Accuracy:0.6010 Positive Precision:0.4516 Negative Precision:0.7976  Val Loss:1.7262 Val Accuracy:0.6009 Val Positive Precision:0.4516 Val Negative Precision:0.7976 LR = 7.8125e-06\n",
      "Epoch 98 Loss:1.7977 Accuracy:0.6010 Positive Precision:0.4517 Negative Precision:0.7976  Val Loss:1.7257 Val Accuracy:0.6009 Val Positive Precision:0.4517 Val Negative Precision:0.7976 LR = 7.8125e-06\n",
      "Epoch 99 Loss:1.8014 Accuracy:0.6010 Positive Precision:0.4518 Negative Precision:0.7977  Val Loss:1.7264 Val Accuracy:0.6009 Val Positive Precision:0.4518 Val Negative Precision:0.7977 LR = 7.8125e-06\n",
      "Epoch 100 Loss:1.8002 Accuracy:0.6010 Positive Precision:0.4519 Negative Precision:0.7978  Val Loss:1.7264 Val Accuracy:0.6009 Val Positive Precision:0.4519 Val Negative Precision:0.7977 LR = 7.8125e-06\n",
      "Epoch 101 Loss:1.8016 Accuracy:0.6010 Positive Precision:0.4520 Negative Precision:0.7978  Val Loss:1.7261 Val Accuracy:0.6009 Val Positive Precision:0.4520 Val Negative Precision:0.7978 LR = 7.8125e-06\n",
      "Epoch 102 Loss:1.7987 Accuracy:0.6010 Positive Precision:0.4521 Negative Precision:0.7979  Val Loss:1.7259 Val Accuracy:0.6009 Val Positive Precision:0.4521 Val Negative Precision:0.7979 LR = 7.8125e-06\n",
      "Epoch 103 Loss:1.8015 Accuracy:0.6010 Positive Precision:0.4522 Negative Precision:0.7979  Val Loss:1.7258 Val Accuracy:0.6009 Val Positive Precision:0.4522 Val Negative Precision:0.7979 LR = 7.8125e-06\n",
      "Epoch 104 Loss:1.8012 Accuracy:0.6010 Positive Precision:0.4522 Negative Precision:0.7980  Val Loss:1.7258 Val Accuracy:0.6009 Val Positive Precision:0.4523 Val Negative Precision:0.7980 LR = 7.8125e-06\n",
      "Epoch 105 Loss:1.7987 Accuracy:0.6010 Positive Precision:0.4523 Negative Precision:0.7981  Val Loss:1.7259 Val Accuracy:0.6009 Val Positive Precision:0.4523 Val Negative Precision:0.7980 LR = 3.90625e-06\n",
      "Epoch 106 Loss:1.7998 Accuracy:0.6010 Positive Precision:0.4524 Negative Precision:0.7981  Val Loss:1.7255 Val Accuracy:0.6009 Val Positive Precision:0.4524 Val Negative Precision:0.7981 LR = 3.90625e-06\n",
      "Epoch 107 Loss:1.7993 Accuracy:0.6010 Positive Precision:0.4525 Negative Precision:0.7982  Val Loss:1.7258 Val Accuracy:0.6009 Val Positive Precision:0.4525 Val Negative Precision:0.7981 LR = 3.90625e-06\n",
      "Epoch 108 Loss:1.7997 Accuracy:0.6010 Positive Precision:0.4526 Negative Precision:0.7982  Val Loss:1.7255 Val Accuracy:0.6009 Val Positive Precision:0.4526 Val Negative Precision:0.7982 LR = 3.90625e-06\n",
      "Epoch 109 Loss:1.7994 Accuracy:0.6010 Positive Precision:0.4527 Negative Precision:0.7983  Val Loss:1.7256 Val Accuracy:0.6009 Val Positive Precision:0.4527 Val Negative Precision:0.7983 LR = 3.90625e-06\n",
      "Epoch 110 Loss:1.7987 Accuracy:0.6010 Positive Precision:0.4527 Negative Precision:0.7983  Val Loss:1.7255 Val Accuracy:0.6009 Val Positive Precision:0.4528 Val Negative Precision:0.7983 LR = 3.90625e-06\n",
      "Epoch 111 Loss:1.7985 Accuracy:0.6010 Positive Precision:0.4528 Negative Precision:0.7984  Val Loss:1.7256 Val Accuracy:0.6009 Val Positive Precision:0.4528 Val Negative Precision:0.7984 LR = 3.90625e-06\n",
      "Epoch 112 Loss:1.7998 Accuracy:0.6010 Positive Precision:0.4529 Negative Precision:0.7984  Val Loss:1.7255 Val Accuracy:0.6009 Val Positive Precision:0.4529 Val Negative Precision:0.7984 LR = 3.90625e-06\n",
      "Epoch 113 Loss:1.7994 Accuracy:0.6010 Positive Precision:0.4530 Negative Precision:0.7985  Val Loss:1.7254 Val Accuracy:0.6009 Val Positive Precision:0.4530 Val Negative Precision:0.7985 LR = 3.90625e-06\n",
      "Epoch 114 Loss:1.8008 Accuracy:0.6010 Positive Precision:0.4531 Negative Precision:0.7985  Val Loss:1.7253 Val Accuracy:0.6009 Val Positive Precision:0.4531 Val Negative Precision:0.7985 LR = 3.90625e-06\n",
      "Epoch 115 Loss:1.8031 Accuracy:0.6010 Positive Precision:0.4531 Negative Precision:0.7986  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4531 Val Negative Precision:0.7986 LR = 3.90625e-06\n",
      "Epoch 116 Loss:1.8019 Accuracy:0.6010 Positive Precision:0.4532 Negative Precision:0.7986  Val Loss:1.7252 Val Accuracy:0.6009 Val Positive Precision:0.4532 Val Negative Precision:0.7986 LR = 1.953125e-06\n",
      "Epoch 117 Loss:1.7982 Accuracy:0.6010 Positive Precision:0.4533 Negative Precision:0.7987  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4533 Val Negative Precision:0.7987 LR = 1.953125e-06\n",
      "Epoch 118 Loss:1.7988 Accuracy:0.6010 Positive Precision:0.4533 Negative Precision:0.7987  Val Loss:1.7253 Val Accuracy:0.6009 Val Positive Precision:0.4533 Val Negative Precision:0.7987 LR = 1.953125e-06\n",
      "Epoch 119 Loss:1.8000 Accuracy:0.6010 Positive Precision:0.4534 Negative Precision:0.7988  Val Loss:1.7252 Val Accuracy:0.6009 Val Positive Precision:0.4534 Val Negative Precision:0.7988 LR = 1.953125e-06\n",
      "Epoch 120 Loss:1.8017 Accuracy:0.6010 Positive Precision:0.4535 Negative Precision:0.7988  Val Loss:1.7253 Val Accuracy:0.6009 Val Positive Precision:0.4535 Val Negative Precision:0.7988 LR = 1.953125e-06\n",
      "Epoch 121 Loss:1.8005 Accuracy:0.6010 Positive Precision:0.4535 Negative Precision:0.7989  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4535 Val Negative Precision:0.7989 LR = 1.953125e-06\n",
      "Epoch 122 Loss:1.7998 Accuracy:0.6010 Positive Precision:0.4536 Negative Precision:0.7989  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4536 Val Negative Precision:0.7989 LR = 1.953125e-06\n",
      "Epoch 123 Loss:1.7995 Accuracy:0.6010 Positive Precision:0.4537 Negative Precision:0.7990  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4537 Val Negative Precision:0.7990 LR = 1.953125e-06\n",
      "Epoch 124 Loss:1.8014 Accuracy:0.6010 Positive Precision:0.4537 Negative Precision:0.7990  Val Loss:1.7250 Val Accuracy:0.6009 Val Positive Precision:0.4537 Val Negative Precision:0.7990 LR = 1.953125e-06\n",
      "Epoch 125 Loss:1.7999 Accuracy:0.6010 Positive Precision:0.4538 Negative Precision:0.7990  Val Loss:1.7250 Val Accuracy:0.6009 Val Positive Precision:0.4538 Val Negative Precision:0.7990 LR = 1.953125e-06\n",
      "Epoch 126 Loss:1.8024 Accuracy:0.6010 Positive Precision:0.4538 Negative Precision:0.7991  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4538 Val Negative Precision:0.7991 LR = 1.953125e-06\n",
      "Epoch 127 Loss:1.7997 Accuracy:0.6010 Positive Precision:0.4539 Negative Precision:0.7991  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4539 Val Negative Precision:0.7991 LR = 9.765625e-07\n",
      "Epoch 128 Loss:1.8006 Accuracy:0.6010 Positive Precision:0.4539 Negative Precision:0.7992  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4540 Val Negative Precision:0.7992 LR = 9.765625e-07\n",
      "Epoch 129 Loss:1.8050 Accuracy:0.6010 Positive Precision:0.4540 Negative Precision:0.7992  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4540 Val Negative Precision:0.7992 LR = 9.765625e-07\n",
      "Epoch 130 Loss:1.8016 Accuracy:0.6010 Positive Precision:0.4541 Negative Precision:0.7993  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4541 Val Negative Precision:0.7992 LR = 9.765625e-07\n",
      "Epoch 131 Loss:1.8001 Accuracy:0.6010 Positive Precision:0.4541 Negative Precision:0.7993  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4541 Val Negative Precision:0.7993 LR = 9.765625e-07\n",
      "Epoch 132 Loss:1.7975 Accuracy:0.6010 Positive Precision:0.4542 Negative Precision:0.7993  Val Loss:1.7251 Val Accuracy:0.6009 Val Positive Precision:0.4542 Val Negative Precision:0.7993 LR = 9.765625e-07\n",
      "Epoch 133 Loss:1.7992 Accuracy:0.6016 Positive Precision:0.4542 Negative Precision:0.7994  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4542 Val Negative Precision:0.7994 LR = 9.765625e-07\n",
      "Epoch 134 Loss:1.8009 Accuracy:0.6016 Positive Precision:0.4543 Negative Precision:0.7994  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4543 Val Negative Precision:0.7994 LR = 9.765625e-07\n",
      "Epoch 135 Loss:1.8003 Accuracy:0.6016 Positive Precision:0.4543 Negative Precision:0.7994  Val Loss:1.7251 Val Accuracy:0.6014 Val Positive Precision:0.4543 Val Negative Precision:0.7994 LR = 9.765625e-07\n",
      "Epoch 136 Loss:1.8006 Accuracy:0.6016 Positive Precision:0.4544 Negative Precision:0.7995  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4544 Val Negative Precision:0.7995 LR = 9.765625e-07\n",
      "Epoch 137 Loss:1.8008 Accuracy:0.6016 Positive Precision:0.4544 Negative Precision:0.7995  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4544 Val Negative Precision:0.7995 LR = 9.765625e-07\n",
      "Epoch 138 Loss:1.8018 Accuracy:0.6016 Positive Precision:0.4545 Negative Precision:0.7995  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4545 Val Negative Precision:0.7995 LR = 4.8828125e-07\n",
      "Epoch 139 Loss:1.8013 Accuracy:0.6016 Positive Precision:0.4545 Negative Precision:0.7996  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4545 Val Negative Precision:0.7995 LR = 4.8828125e-07\n",
      "Epoch 140 Loss:1.8043 Accuracy:0.6016 Positive Precision:0.4546 Negative Precision:0.7996  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4546 Val Negative Precision:0.7996 LR = 4.8828125e-07\n",
      "Epoch 141 Loss:1.8020 Accuracy:0.6016 Positive Precision:0.4546 Negative Precision:0.7996  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4546 Val Negative Precision:0.7996 LR = 4.8828125e-07\n",
      "Epoch 142 Loss:1.8044 Accuracy:0.6016 Positive Precision:0.4546 Negative Precision:0.7996  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4546 Val Negative Precision:0.7996 LR = 4.8828125e-07\n",
      "Epoch 143 Loss:1.8008 Accuracy:0.6016 Positive Precision:0.4547 Negative Precision:0.7997  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4547 Val Negative Precision:0.7997 LR = 4.8828125e-07\n",
      "Epoch 144 Loss:1.7967 Accuracy:0.6016 Positive Precision:0.4547 Negative Precision:0.7997  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4547 Val Negative Precision:0.7997 LR = 4.8828125e-07\n",
      "Epoch 145 Loss:1.8047 Accuracy:0.6022 Positive Precision:0.4548 Negative Precision:0.7997  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4548 Val Negative Precision:0.7997 LR = 4.8828125e-07\n",
      "Epoch 146 Loss:1.7989 Accuracy:0.6022 Positive Precision:0.4548 Negative Precision:0.7997  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4548 Val Negative Precision:0.7997 LR = 4.8828125e-07\n",
      "Epoch 147 Loss:1.7984 Accuracy:0.6022 Positive Precision:0.4549 Negative Precision:0.7998  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4549 Val Negative Precision:0.7998 LR = 4.8828125e-07\n",
      "Epoch 148 Loss:1.8046 Accuracy:0.6016 Positive Precision:0.4549 Negative Precision:0.7998  Val Loss:1.7250 Val Accuracy:0.6014 Val Positive Precision:0.4549 Val Negative Precision:0.7998 LR = 4.8828125e-07\n",
      "Epoch 149 Loss:1.7998 Accuracy:0.6022 Positive Precision:0.4549 Negative Precision:0.7998  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4549 Val Negative Precision:0.7998 LR = 2.44140625e-07\n",
      "Epoch 150 Loss:1.7994 Accuracy:0.6022 Positive Precision:0.4550 Negative Precision:0.7998  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4550 Val Negative Precision:0.7998 LR = 2.44140625e-07\n",
      "Epoch 151 Loss:1.7978 Accuracy:0.6022 Positive Precision:0.4550 Negative Precision:0.7999  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4550 Val Negative Precision:0.7999 LR = 2.44140625e-07\n",
      "Epoch 152 Loss:1.8013 Accuracy:0.6022 Positive Precision:0.4550 Negative Precision:0.7999  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4551 Val Negative Precision:0.7999 LR = 2.44140625e-07\n",
      "Epoch 153 Loss:1.8029 Accuracy:0.6022 Positive Precision:0.4551 Negative Precision:0.7999  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4551 Val Negative Precision:0.7999 LR = 2.44140625e-07\n",
      "Epoch 154 Loss:1.8017 Accuracy:0.6022 Positive Precision:0.4551 Negative Precision:0.7999  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4551 Val Negative Precision:0.7999 LR = 2.44140625e-07\n",
      "Epoch 155 Loss:1.7991 Accuracy:0.6022 Positive Precision:0.4552 Negative Precision:0.7999  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4552 Val Negative Precision:0.7999 LR = 2.44140625e-07\n",
      "Epoch 156 Loss:1.8022 Accuracy:0.6022 Positive Precision:0.4552 Negative Precision:0.8000  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4552 Val Negative Precision:0.8000 LR = 2.44140625e-07\n",
      "Epoch 157 Loss:1.7976 Accuracy:0.6022 Positive Precision:0.4552 Negative Precision:0.8000  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4552 Val Negative Precision:0.8000 LR = 2.44140625e-07\n",
      "Epoch 158 Loss:1.8028 Accuracy:0.6022 Positive Precision:0.4553 Negative Precision:0.8000  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4553 Val Negative Precision:0.8000 LR = 2.44140625e-07\n",
      "Epoch 159 Loss:1.7987 Accuracy:0.6022 Positive Precision:0.4553 Negative Precision:0.8000  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4553 Val Negative Precision:0.8000 LR = 2.44140625e-07\n",
      "Epoch 160 Loss:1.7995 Accuracy:0.6022 Positive Precision:0.4553 Negative Precision:0.8001  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4553 Val Negative Precision:0.8000 LR = 1.220703125e-07\n",
      "Epoch 161 Loss:1.8003 Accuracy:0.6022 Positive Precision:0.4554 Negative Precision:0.8001  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4554 Val Negative Precision:0.8001 LR = 1.220703125e-07\n",
      "Epoch 162 Loss:1.7991 Accuracy:0.6022 Positive Precision:0.4554 Negative Precision:0.8001  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4554 Val Negative Precision:0.8001 LR = 1.220703125e-07\n",
      "Epoch 163 Loss:1.8035 Accuracy:0.6022 Positive Precision:0.4554 Negative Precision:0.8001  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4554 Val Negative Precision:0.8001 LR = 1.220703125e-07\n",
      "Epoch 164 Loss:1.8027 Accuracy:0.6022 Positive Precision:0.4555 Negative Precision:0.8001  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4555 Val Negative Precision:0.8001 LR = 1.220703125e-07\n",
      "Epoch 165 Loss:1.8003 Accuracy:0.6022 Positive Precision:0.4555 Negative Precision:0.8002  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4555 Val Negative Precision:0.8001 LR = 1.220703125e-07\n",
      "Epoch 166 Loss:1.8041 Accuracy:0.6022 Positive Precision:0.4555 Negative Precision:0.8002  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4555 Val Negative Precision:0.8002 LR = 1.220703125e-07\n",
      "Epoch 167 Loss:1.8004 Accuracy:0.6022 Positive Precision:0.4556 Negative Precision:0.8002  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4556 Val Negative Precision:0.8002 LR = 1.220703125e-07\n",
      "Epoch 168 Loss:1.7992 Accuracy:0.6022 Positive Precision:0.4556 Negative Precision:0.8002  Val Loss:1.7250 Val Accuracy:0.6020 Val Positive Precision:0.4556 Val Negative Precision:0.8002 LR = 1.220703125e-07\n",
      "Epoch 169 Loss:1.8001 Accuracy:0.6022 Positive Precision:0.4556 Negative Precision:0.8002  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4556 Val Negative Precision:0.8002 LR = 1.220703125e-07\n",
      "Epoch 170 Loss:1.8008 Accuracy:0.6022 Positive Precision:0.4557 Negative Precision:0.8002  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4557 Val Negative Precision:0.8002 LR = 1.220703125e-07\n",
      "Epoch 171 Loss:1.7983 Accuracy:0.6022 Positive Precision:0.4557 Negative Precision:0.8003  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4557 Val Negative Precision:0.8003 LR = 6.103515625e-08\n",
      "Epoch 172 Loss:1.8034 Accuracy:0.6022 Positive Precision:0.4557 Negative Precision:0.8003  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4557 Val Negative Precision:0.8003 LR = 6.103515625e-08\n",
      "Epoch 173 Loss:1.7991 Accuracy:0.6022 Positive Precision:0.4557 Negative Precision:0.8003  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4558 Val Negative Precision:0.8003 LR = 6.103515625e-08\n",
      "Epoch 174 Loss:1.7997 Accuracy:0.6022 Positive Precision:0.4558 Negative Precision:0.8003  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4558 Val Negative Precision:0.8003 LR = 6.103515625e-08\n",
      "Epoch 175 Loss:1.7972 Accuracy:0.6022 Positive Precision:0.4558 Negative Precision:0.8003  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4558 Val Negative Precision:0.8003 LR = 6.103515625e-08\n",
      "Epoch 176 Loss:1.7984 Accuracy:0.6022 Positive Precision:0.4558 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4558 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 177 Loss:1.8000 Accuracy:0.6022 Positive Precision:0.4559 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4559 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 178 Loss:1.7972 Accuracy:0.6022 Positive Precision:0.4559 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4559 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 179 Loss:1.7993 Accuracy:0.6022 Positive Precision:0.4559 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4559 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 180 Loss:1.7984 Accuracy:0.6022 Positive Precision:0.4559 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4560 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 181 Loss:1.8044 Accuracy:0.6022 Positive Precision:0.4560 Negative Precision:0.8004  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4560 Val Negative Precision:0.8004 LR = 6.103515625e-08\n",
      "Epoch 182 Loss:1.8010 Accuracy:0.6022 Positive Precision:0.4560 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4560 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 183 Loss:1.8034 Accuracy:0.6022 Positive Precision:0.4560 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4560 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 184 Loss:1.7985 Accuracy:0.6022 Positive Precision:0.4561 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4561 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 185 Loss:1.8020 Accuracy:0.6022 Positive Precision:0.4561 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4561 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 186 Loss:1.8014 Accuracy:0.6022 Positive Precision:0.4561 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4561 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 187 Loss:1.7996 Accuracy:0.6022 Positive Precision:0.4561 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4561 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 188 Loss:1.8003 Accuracy:0.6022 Positive Precision:0.4562 Negative Precision:0.8005  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4562 Val Negative Precision:0.8005 LR = 3.0517578125e-08\n",
      "Epoch 189 Loss:1.7999 Accuracy:0.6022 Positive Precision:0.4562 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4562 Val Negative Precision:0.8006 LR = 3.0517578125e-08\n",
      "Epoch 190 Loss:1.8013 Accuracy:0.6022 Positive Precision:0.4562 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4562 Val Negative Precision:0.8006 LR = 3.0517578125e-08\n",
      "Epoch 191 Loss:1.7984 Accuracy:0.6022 Positive Precision:0.4562 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4562 Val Negative Precision:0.8006 LR = 3.0517578125e-08\n",
      "Epoch 192 Loss:1.7987 Accuracy:0.6022 Positive Precision:0.4563 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4563 Val Negative Precision:0.8006 LR = 3.0517578125e-08\n",
      "Epoch 193 Loss:1.8028 Accuracy:0.6022 Positive Precision:0.4563 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4563 Val Negative Precision:0.8006 LR = 1.52587890625e-08\n",
      "Epoch 194 Loss:1.8015 Accuracy:0.6022 Positive Precision:0.4563 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4563 Val Negative Precision:0.8006 LR = 1.52587890625e-08\n",
      "Epoch 195 Loss:1.8017 Accuracy:0.6022 Positive Precision:0.4563 Negative Precision:0.8006  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4563 Val Negative Precision:0.8006 LR = 1.52587890625e-08\n",
      "Epoch 196 Loss:1.8001 Accuracy:0.6022 Positive Precision:0.4563 Negative Precision:0.8007  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4564 Val Negative Precision:0.8007 LR = 1.52587890625e-08\n",
      "Epoch 197 Loss:1.7990 Accuracy:0.6022 Positive Precision:0.4564 Negative Precision:0.8007  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4564 Val Negative Precision:0.8007 LR = 1.52587890625e-08\n",
      "Epoch 198 Loss:1.8003 Accuracy:0.6022 Positive Precision:0.4564 Negative Precision:0.8007  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4564 Val Negative Precision:0.8007 LR = 1.52587890625e-08\n",
      "Epoch 199 Loss:1.8021 Accuracy:0.6022 Positive Precision:0.4564 Negative Precision:0.8007  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4564 Val Negative Precision:0.8007 LR = 1.52587890625e-08\n",
      "Epoch 200 Loss:1.7977 Accuracy:0.6022 Positive Precision:0.4564 Negative Precision:0.8007  Val Loss:1.7249 Val Accuracy:0.6020 Val Positive Precision:0.4564 Val Negative Precision:0.8007 LR = 1.52587890625e-08\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataloader, val_dataloader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994e8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Example data (each pixel has an integer class label)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "features, ground_truth = next(iter(val_dataloader))\n",
    "\n",
    "images = features.to(device_name)\n",
    "model.eval()\n",
    "predictions = model(images)\n",
    "predictions = (torch.sigmoid(predictions) > 0.5).float()\n",
    "y_true = ground_truth.numpy()\n",
    "y_pred = predictions.detach().cpu().int().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a04f0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 38]\n",
      " [ 1 34]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x137c65010>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent,\n",
    "                          display_labels=[\"Yes\", \"Not\"])\n",
    "\n",
    "disp.plot(cmap='Blues', values_format='.1f')\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a114d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8151441216468811,\n",
       " tensor(0.6907, device='mps:0'),\n",
       " tensor([0.6907, 0.0000], device='mps:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False,  # Use sampler instead of shuffle\n",
    "    num_workers=NUM_OF_WORKERS\n",
    ")\n",
    "\n",
    "model.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3509fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
